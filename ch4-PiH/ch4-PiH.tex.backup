\chapter{Peg in hole}

In Chapter 3, we demonstrated that we could learn a search policy from demonstrations of human teachers for a task consisting of 
locating a wooden object on a table and successfully transferring it to a robot apprentice. The motivation behind our approach is 
that the intuition and knowledge exhibited by the human teachers, during the search, consists of a good balance between 
exploration and exploitation actions which then can be encapsulated in a generative Gaussian Mixture Model (GMM) and subsequently used as a control policy. 
The approach is satisfactory if we are interested in extracting different behaviours present across the human teachers and reproducing 
them. If our objective is however to learn a policy with a unique behaviour which is optimal or at least close to optimal, then using  
the approach detailed in Chapter 3, as it is, will not necessarily result in a efficient policy. For the GMM will model
both good patterns exhibited by the human teachers and their mistakes. If the task is difficult and many possible solutions exist, such 
as was the case in the blindfolded search task of the previous Chapter, many demonstrations will be necessary for search patters to be 
present and encoded in the GMM. Otherwise the GMM would have to be combined with another policy as showed in Chapter 3, for our Hybrid GMM-Greedy 
policy. 

%  This sentence is hard to follow. Break it into two sentences. What do you mean by goal-oriented? 
%  It is not clear which shortcoming you want to highlight. Is the requirement for consistency or for goal-orientedness? 
% Obviously, any task < no? The consistency requirement is also somewhat contradictory with the fact that you were embedding different 
% styles of search across demonstrations.

The demonstrated search task is implicitly encoded in the GMM parameters since no cost or reward function was defined, which is common practice in Planning 
and Reinforcement Learning (RL).
Previously different search strategies were encoded with no consideration of their quality. The drawback is that the GMM policy is learned via Expectation-Maximisation (EM) which does not consider whether the data 
from the demonstrations are bad or good. If the task is difficult and there are mostly bad demonstrations, the GMM search policy will 
reproduce suboptimal behaviour. It also could be that no teachers by themselves are good, but by combining different components of 
the teachers individual demonstrations we can extract a good search strategy.


teachers are not good at the task individually and maybe only a few demonstrations are the best, whilst the majority are bad.

There the task undertaken is implicitly encoded, as there is no cost function which is optimised, in 
the GMM and as a result the taught behaviour has to be goal oriented and consistent, which is not always the case in a blindfolded search task. 

To overcome the above mentioned limitations, in this Chapter we use a binary cost function as means of ranking demonstrations provided 
by the human teachers. We combine our PbD-POMDP approach with an Actor-Critic Reinforcement Learning (RL) framework which is close to 
Fitted-Value Iteration(FVI) and other experience replay methods, which we will refer to as RL-PbD-POMDP. Our objective 
is to avoid noisy explorative rollouts, common to all RL approaches and is their Achille's heel, and only rely on 
the data provided by the human teachers. We want to avoid any autonomous exploration common in RL for three reasons. 
Firstly it is time consuming and is typically only applicable to RL problems in which an exhaustive exploration 
of the entire state or parameter space is feasible, such as in traditional RL problems like the inverted pendulum or mountain cart. 
The universal exploration method, used throughout RL, is state independent (sometimes state dependent) white noise 
which results in an entire exploration of the state space. This
is neither practical or feasible for the type of search problems we are considering. The second reason is that the 
exploration cannot be random as we are using a physical robotic system, and this would be dangerous. The third and most important reason
is that we want to use the same amount of information for our RL-PbD-POMDP as we used for the PbD-POMDP approach. 
This is to strongly highlight the fact that we can obtain an improved policy without the need of any additional information

%We chose a search and Peg-In-Hole (PiH) task
We analyse our RL-PbD-POMDP approach on a power-socket Peg-in-Hole (PiH) search task. In this task, human teachers must demonstrate 
to a robot apprentice how to search for and connect a plug to a power socket. The first component of the task, the search for the 
socket, is similar to the table-wooden block setup in the previous Chapter. 
Here the connection of the plug to the power socket, the PiH component, which requires a higher level of precision to be able to achieve.

% Show figure of the task setup.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\textwidth]{./ch4-PiH/Figures/Fig/ex_setup_no_data.pdf}
     \caption{\textit{Top-right:} The experimental setup. Blindfolded human teachers stand in the orange rectangle always 
   facing the wall. Human teachers are informed of their starting location and told that 
   they would always be facing the wall. \textit{Top-left:} Three different power sockets. The plug is connected
   to a cylinder, to make it easy to hold for the human teachers. An ATI 6-axis force torque sensor (Nano25 Series)
   is between the cylinder and the plug. \textit{Bottom-left} Human teacher performing the PiH search task. 
   The human teachers wear goggles to remove sight and ear defenders to greatly diminish 
   hearing.\textit{Bottom-right:} The KUKA LWR robot equipped with the same force torque sensor and plug 
   used by the human teachers.}
  \label{fig:experiment_setup}
\end{figure}

In Figure \ref{fig:experiment_setup} (\textit{Top-right}), we illustrate the setup of the search task. The diagram shows
the distribution of the initial starting position (in orange) of the human teachers. As was the case in our previous search experiment
in Chapter 3, the teacher is disoriented before each trial but knowns that his initial heading will be the same, facing the wall. 
The human teachers are deprived of visual information (they are blindfolded) and their hearing sense is significantly 
reduced (by wearing ear defenders).  We consider one type of plug, Type J\footnote{http://www.iec.ch/worldplugs/typeJ.htm}, 
and three different power sockets. Power \textit{socket A}, has a ring around its holes, \textit{socket B} has a funnel, which we 
hypothesize should make it easier to connect, and \textit{socket C} has a flat elevated surface. See Figure \ref{fig:experiment_setup}
(\textit{Top-left}) for an illustration. The plug held by the human teachers is attached to a cylindrical handle with 
an ATI 6 axis force torque sensor (Nano25 \footnote{http://www.ati-ia.com/products/ft/sensors.aspx}) fixed 
between the two. On top of the cylinder is a set of markers from which a motion capture system 
(OptiTrack\footnote{http://www.optitrack.com/}) provides both position and orientation, see Figure \ref{fig:experiment_setup} (\textit{Top-left}). In the \textit{Bottom-left} quadrant we
can see an example of a human teacher trying to accomplish the search and connection task, and to the \textit{Bottom-right} the KUKA
LWR robot apprentice is reproducing a search policy learned from the teacher.


We found that by learning a value function over the belief space using approximate dynamic programming (part of FVI) and using 
this as a Critic to update the parameters of our GMM policy (Actor) we were able to achieve an important improvement over our 
previous PbD-POMDP approach. We  performed evaluations both in simulation and on the KUKA LWR robot where we tested 
policies ability to generalise to sockets for which no training data was provided and different socket locations. 
In all our evaluations the RL-PbD-POMDP approach proved to be always better. More importantly we demonstrate that 
the RL-PbD-POMDP approach performs significantly better when we use the training data from the worst teacher, 
which mitigates the \textbf{original assumption} that the teachers have to be consistently efficient at the task.

%\section{Outline}
%\begin{itemize}
%  \item \hyperref[ch4:background]{\ref{ch4:background}   Background}
%  \item \hyperref[ch4:experiment]{\ref{ch4:experiment}   Experiment}
%  \item \hyperref[ch4:formulation]{\ref{ch4:formulation} Formulation}
%  \item \hyperref[ch4:results]{\ref{ch4:results}  	 Results}
%\end{itemize}

\section{Background}\label{ch4:background}

\subsection{Peg-in-hole}

The Peg-in-Hole (PiH) task is one of the most widespread steps in industrial assembly and manipulations processes, with 
examples including the assembly of vehicular transmission components \cite{search_strategies_icra_2001} and 
valves \cite{online_gpr_icra_2014}. To be successful, the estimated
position of the robot's end-effector and workpiece must be precise. Typically, the clearance the between 
and plug and the workpiece's hole is very small leaving little room for error. As a result, variations in the assembly's components 
in combination with position uncertainty can result in either jamming during the insertion process or in failure of 
the plug finding the hole. This created a need for adaptive search and insertion policies for PiH, which has been driving research 
in this area. 

From the literature, we identified the different components in PiH solutions. All approaches
use to some extent a vision system to estimate the position of the workpiece. 
Given an estimate of the workpiece's position, 
a common approach is to follow either a blind increasing spiral Cartesian trajectory or parametrised policies 
which guarantee that all positions on the workpiece have been visited. 
To increase the chances of a connection these approaches use a compliant controller 
which usually includes a hybrid force/position controller.
The second predominant approach (which has been confined to academic circles) follows the data driven Programming by Demonstration (PbD) 
framework. Teleoperated or kinesthetic demonstrations of a human teacher are recorded and a policy is learned and fine-tuned 
so as to reproduce the same (F)orce/(T)orque profile as that demonstrated by the human teacher. The first approach 
does not consider reproducing the F/T profile but rather follows a position trajectory whilst being compliant.

In \cite{peg_personal_icra_2010} a PR2 executes a parameterised policy designed to connect a plug to a power outlet
in order for the PR2 to recharge itself. The plug is equipped with a checkerboard to facilitate pose estimation
of the plug with respect to power outlet whose position is extracted through a vision processing pipeline.
An initial connection is attempted by visual servoing which is successful 10\% of the time. When unsuccessful 
a spiralling outward motion is carried out with 2mm increments. This method achieved an overall success rate of 95\%.
The hybrid control paradigm \cite{hybrid_1992} was used throughout the execution of the task.

% PbD: reproduce the F/T profile irrespective of the learned position trajectory DMP
In \cite{fast_peg_pbd_icmc_2014} the authors learn a PiH policy for the Cranfield benchmark object.
A vision system obtains the pose parameters of the object whilst a human teacher  
demonstrates trajectories, through teleoperation, in the frame of reference of the object. 
A time-dependent policy represented with Dynamic Movement Primitives (DMP) \cite{Schaal04learningmovement} 
encodes the recorded Cartesian end-effector pose. A F/T profile is encoded separately by a regressor parameterised 
by radial basis functions. Successive refinements of the DMP policy are achieved through 
using force feedback to adapt the parameters of an admittance controller. This results in the policy having
similar force profiles to the human teachers. Such an approach was first proposed by \cite{trans_workpiece_icra_2013}
and further applications based on this method have been done \cite{sol_pdg_pbd_2014} with the incorporation of  
a disturbance rejection policy.
Reinforcement learning has also been used in combination with DMP to learn PiH policies. In \cite{learn_force_c_icirs_2011}
an DMP policy is initialised with kinesthetic demonstrations of a door opening and pen pick up task. The recorded Cartesian 
trajectories are encoded in parameterised DMP policy and augmented with a F/T regressor profile. A reward function is designed, 
encoding desirable properties of the F/T profile such as smoothness and continuity, and after 110 trials a policy
was found to be a 100\% successful. In \cite{learn_admittance_icra_1994} a 18 dimensional input (sensed position, previous position and force)  and 6 dimensional 
output (linear and angular velocity) neural network is learned by associative reinforcement learning. 
During the learning process the plug is randomly positioned within the vicinity of the hole. After a 100 executions and 
updates, the policy is successful and was able to generalise across different geometries and clearances. 

The above policies were learned from human demonstrations and encoded by a regressor function and
optimised to reproduce a desired F/T profile. Further approaches to the PiH problem 
are predominantly based on heuristic search mechanisms and compliant controllers.

% Blind search stragegies
In \cite{search_strategies_icra_2001} different blind search policies for the insertion of a spline toothed hub 
into a forward clutch are analysed. The state space is discretised into points so that the distance between two 
neighbours is smaller than the clearance of the hole, which is known as a spray point coverage. Different search 
strategies which ensure that all the points are visited are evaluated. It is found that paths following  
concentric circles gradually spiralling inwards were the most effective method in finding the hole. The concentric circle
search strategy has been applied in many PiH tasks. For instance in \cite{peg_imcssd_2015}, a PiH heuristic 
policy was developed to connect a 5-pin waterproof industrial charger to an electric socket. The authors 
estimated the pose of the socket through a vision system and used a force controller in combination with a 
spiral search policy to achieve a connection and demonstrated their approach to be reliable. 

% Do it the same way that humans do it: Does not use FT sensor
In \cite{intuitive_peg_isr_2013} the authors make the observation that humans lack the precision and sensing 
accuracy of robotic systems. But nevertheless, are more proficient than robots at PiH. The authors observe that when humans try to connect a square plug to a socket, they rub the plug against the socket's 
outlet without looking. It is thought that the inherent compliance in humans' motor control  
is the key to our success at PiH tasks \cite{compliant_manip_icra_2008}. 
The authors introduce an Intuitive Assembly Strategy (IAS) inspired by the above observation which 
does not require the hole to be precisely localised. The IAS search strategy is based on compliant 
spiral motion and the execution of the search trajectory is performed with a hybrid force/position controller.

The spiral strategy is widely used in industrial applications due to its simplicity, 
however, it is a blind search method. Another approach to the assembly process 
consists of fine-tuning parameters of predefined policies. In \cite{online_gpr_icra_2014}
the authors develop an online Gaussian Process policy optimisation of an assembly task. They 
demonstrate that by learning the dynamical model of the task during execution, it can be used to 
learn the parameters of the policy faster than offline methods, such as Design of Experiment (DOE) 
or Genetic Algorithms.

%\subsection{Actor-Critic \& Fitted Reinforcement Learning}
%For learning a POMDP policy for our search task, we consider RL approaches which naturally handles continuous 
%belief-state and action space, such as policy search/gradient methods. We gave an overview in Chapter 2 of the different policy search methods 
%fulfilling this requirement. However, an apparent drawback is that they work well on policies 
%which have very few parameters with respect to the size of the state space, but they will perform poorly if the 
%number of parameters is to large. The reason is that the variance of the policies' gradient
%is large, making the stochastic gradient ascent learning slow. The PbD-POMDP policy we learned in 
%Chapter 3 had over 80 Gaussian functions, each of dimension 7, and we expect the number of parameters of 
%this policy to be in the same order.
%Actor-critic's (AC) \cite[Chap. 6.6]{Sutton00policygradient} are an RL approach in which the policy (actor) and critic (value function) have separate
%parameterization and can be represented by different functions, for instance the value function could be a 
%decision tree and the policy a neural network. The advantage of an AC is that the policy can be chosen such 
%that it is computationally efficient in queering actions and the value function does not necessarily have to have
%the same input space as the policy. In a sense the two can be independent. The parameters of both the actor 
%and critic are learned via gradient decent on the Temporal Different (TD) error. It has been reported and 
%proven \cite{rl_ac_surv_2012} that the variance of the gradient update in AC methods has smaller variance than 
%purely actor methods (policy gradient), which results in faster learning. 

%To learn POMDP policy in partially observable search and PiH task we have to consider a pol
%The policy we learn to accomplish the search and socket connection is based on an Actor-Critic (AC) architecture
% In an AC both the value function (Critic) and the policy (Actor) have 
%they own parameterization. 


% Start by taking about updating the parameters
% What assume that value function has a different functional form 

% Convergence only guartees -> Policy Gradient theorem.

%The advantage of an Actor-critic (AC) over a policy gradient method is that the variance of the
%expected gradient estimate is lower. This means that the policy will converge quicker and there is less risk of divergence.

%Learning Q and deriving a policy from it requires an online optimisation at every time step to find the optimal action.

% The lower variance is traded for a bias at the start of the learning when the estimates are far from accurate.

%Actor-only policy methods, also known as policy search, converges is guaranteed if the estimated
%radients are unbiased (General policy gradient theorem). 

% Small changes in the value function will result in small changes in the policy

% Critic is usally updated via temporal difference error

% Policy gradient theorem: 
% 
% Many AC rely on PGT, unbiased estimate of the gradient can 
% be obtained from experince unsing an approximate value function
% satisfying certaint properties.
%

%\cite{Sutton00policygradient} % Policy Gradient theorm
%\cite{Boyan95generalizationin} % Safely approximiting the value function
%\cite[Chap 6.6]{sutton98a}

% Explain why Actor-Critic is better 
%\cite{rl_ac_surv_2012}

% % Explain why Fitted / Batch reinforcement learning is better 
%\cite{fqi_nips_peter_2009},\cite{batch_synth_traj_2013},\cite{fnac_ca_2008},\cite{Riedmiller2005}, \cite{EGW05}, \cite{rl_gmm_2010}
%\cite{fvi_uav_2010}
%\cite{mnih-dqn-2015}
%\cite{neura_fqi_2005}
%\cite{DRQ_AAAI_2015}
%\cite{approx_rl_overview_2011} % Appoximiate RL overview

\section{Experiment}\label{ch4:experiment}

The sockets are positioned at the center of a fake wall clamped to a table, see Figure \ref{fig:experiment_setup} (\textit{Top-left})
for an illustration of the environment. Each teacher is given the opportunity to familiarise himself with 
the environment. Before each trial the human teacher is placed on a chair and disoriented by the experimenter. Once disoriented,
the teacher is allowed to stand and is signalled to start the search task by a light touch to the shoulder.
Figure \ref{fig:experiment_setup} (\textit{Bottom-left}) illustrates a human teacher performing the task. The disorientation
step is to induce the effect of an uniform prior over the teachers believed location. We make the assumption that the human's 
believed location can be presented by a probability density function, which is assumed to be known. All subsequent 
distributions can be obtained through the application of a Bayesian filter given that both the sensing and motion 
information are provided by the force torque and motion capture sensors.

In Figure \ref{fig:experiment_setup} (\textit{Top-right}) we illustrate the experimental setup. The orange area represents 
the teachers starting location and is assumed prior knowledge. The teachers are told and shown before hand the 
environmental setup and it is made explicitly clear to them that they will always be starting in the orange 
area facing the wall. 

A group of 10 human teachers participated in the plug-socket experiment. Each teacher performed the search and PiH 
for sockets A and B. A teacher of group A (starting with socket A), would start by performing 15 times the search and connection 
with socket A and after a short break, during which socket A was replaced by socket B, would carry on to perform 15 trials 
with socket B.

Before the actual recording of the task, the teachers had a training period to familiarise themselves 
with environment and become comfortable in wearing the sensor deprivation apparatus. After each teacher felt sufficiently
ready to carry out the task to the best of his ability, the experimenter proceeded to disoriented him through 
the usage of a chair as mentioned previously. The teachers were reminded that they were facing the direction of 
the fake wall and that the starting location would be always within the orange rectangular area. 
In Figure \ref{fig:experiment_setup_data} we can see the time taken by the teachers to accomplish the task. 

Both groups A and B took $9\pm10$s to find their socket. This was expected since the sockets 
are at the same location. However after the socket was found it took a further $8\pm7$s on average for group A to connect
socket B and $12\pm10$s on average for group B to connect socket A. As we can see this is not a straight forward task when considering
the sensory deprivation. See Figure \ref{fig:experiment_setup_data} (\textit{Right}) the time taken to connect the plug to the socket.

\begin{figure}
 \centering
   \includegraphics[width=\textwidth]{./ch4-PiH/Figures/Fig/start_position_v2.pdf}
   \caption{\textit{Left}: Black points represent the starting position of the end-effector
   for all the demonstrations. Four trajectories are illustrated. \textit{Right:} 
   Time taken for the teachers to accomplish the PiH once the socket is localised. Group A and B are depicted in red 
   and blue. The asterisk indicates that the group has changed sockets, so Group A\textsuperscript{*} means
   that Group A is now performing the task with socket B and Group B\textsuperscript{*} means that group B is now performing 
   the task with socket A.}
  \label{fig:experiment_setup_data}
\end{figure}

The location belief of the humans was represented by a probability density function. We made the assumption 
that after the disorientation step the human's believed location would be uniform and spread across the 
rectangular starting area. Although the mental state of the human remains 
unobservable, there is sufficient evidence to support our assumption that his belief state
gets updated in a Bayesian fashion [citation].

During each trial we recorded the position and orientation of the plug provided by the motion capture 
system, and the sensed force and torque, given by the ATI sensor. 


\section{Formulation}\label{ch4:formulation}

\subsection{Belief probability density function}
% Before explaining the measurement model we detail what measurement $\tilde{y}_t$ is.
In our setting the belief probability density function is a Point Mass Filter (PMF) \cite[p.87]{Bergman99recursivebayesian},
which is a  Bayesian filter. It is parametrised by a set of grid cells  which contain valid probabilities.
Our choice of a PMF, as means to represent the believed location of the plug, is motivated by the fact that the 
sensing likelihoods are non-gaussian and lead to multi-modal distributions. A PMF is able to capture such non-gaussianity whilst
remaining fully deterministic (which is not the case for a particle filter).
The PMF gives a probability density, $p(x_t|y_{0:t},\dot{x}_{0:t})$, which is recursively updated through the 
application of a \textbf{motion}, $p(x_t|x_{t-1},\dot{x}_t)$ and \textbf{measurement}, $p(y_t|x_t)$ model. 
The motion model updates the position of the probability density function and subsequently increases the uncertainty 
of the position. This step essentially consists of applying a convolution kernel to the PMF where the covariance 
is proportional to the measured velocity.
The measurement model indicates areas of the state space from which a measurement $\tilde{y}_t$ could have originated. 
Both the human teachers and the robot apprentice use the same sensor interface. This is a plug holder
equipped with a 6-axis force torque sensor which provides a sensed wrench, $\phi \in \mathbb{R}^6$, which we
call the \textbf{raw} measurement. We define the \textbf{actual} measurement to be a function of the sensed wrench, 
$\tilde{y}_t = h(\phi_t)$, which is a binary feature vector. The feature vector encodes whether a contact is present 
and the direction in which it occurs, which we discretized to the four cardinalities.
In Figure \ref{fig:PMF} (\textit{Right-bottom}) we illustrated the likelihood when an edge is sensed.

%The measurement model, $p(y_t|x_t)$, to be binary. If the plug enters in contact with the socket's edge 
%and the feature of $\tilde{y}_t$ corresponding to the left edge is active then the likelihood of regions close to the 
%left edge will be $1$, whilst others will be $0$.  %, another approach would to learn the model directly conditional distribution $p(\phi|x)$ from recorded data.

%\begin{figure}
% \centering
%   \includegraphics[width=0.45\textwidth]{Figures/images/measurement_model_2.pdf}
%   \caption{World model. \textit{Top}: The plug is presented by its three plug tips and the wall and sockets are fitted with bounding boxes 
%    The blue and pink lines represent edges and each surface is encoded by the equation of a plane. 
%    \textit{Bottom}: When the plug inters in contact with right edge of the socket the feature of the measurement vector $y_t$
%    corresponding to a contact coming from the right is $1$. As a result all the regions, $x_t$, close the left edge are one (red points)
%    whilst the others are zero (blue points) and areas around the socket's central ring are also set to one. 
%    }
%  \label{fig:world_model}
%\end{figure}

% %We illustrate the likelihood for one type of contact, other contacts have there own likelihood function who's 
%activation depends on the current measurement. We do not consider the effect of noise since the measurements are binary. 
%This is one particular approach to define a measurement likelihood function amongst many others.
%  the feature of the measurement vector $y_t$ corresponding to a contact coming from the right is $1$.
%  the feature of the measurement vector $y_t$ corresponding to a contact coming from the right is $1$.
\begin{figure*}
 \centering
   \includegraphics[width=\textwidth]{./ch4-PiH/Figures/PMF/pmf_likelihood_v2.pdf}
   \caption{\textit{Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting 
   region. Each grid cell represents a hypothetical position of the plug, the orientation is assumed to be known. (2) First contact, the distribution 
   is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge.
   \textit{Right}: \textbf{World model}: The plug is presented by its three plug tips and the wall and sockets are fitted with bounding boxes.
   \textbf{Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take 
   a value of one (red points)  whilst the others have a value zero (blue points) and areas around the socket's central 
   ring have a value of one. }
  \label{fig:PMF}
\end{figure*}

\subsection{Belief compression}

The probability density function $p(x_t|y_{0:t},\dot{x}_{0:t})$ is high dimensional and it is 
impractical to directly learn a statistical policy $\pi_{\theta} : p(x_t|y_{0:t},\dot{x}_{0:t}) \rightarrow \dot{x}_t$; 
therefore some form of compression is necessary. One possibility would be E-PCA [cite] which finds a set of 
representative basis functions (which are probability distributions). Although elegant this method 
requires a discretisation of the belief space which is computationally expensive. Instead we chose to 
compress the pdf to a belief space vector composed of the maximum a posteriori, $\hat{x}^{\mathrm{MAP}}_t = \argmax_{x_t} p(x_t|y_{0:t},\dot{x}_{0:t}) \in \mathbb{R}^3$, and the differentiation entropy, 
$U = H\{p(x_t|y_{0:t},\dot{x}_{0:t})\} \in \mathbb{R}$. All pdfs in our recorded data set $D$ are transformed to 
a belief space feature vector, $b_t = [\hat{x}^{\mathrm{MAP}}_t,U]^{\mathrm{T}}$. 

From the demonstrations we obtained a dataset $D=\{\dot{x}^{[i]}_{1:T},\omega^{[i]}_{1:T},\phi^{[i]}_{1:T},b^{[i]}_{1:T}\}$, 
where the upper index $[i]$ references the ith trajectory and subscript $1:T$ denotes the time steps during the trajectory
from initialisation $t=1$ until the end $t=T$. The data consisted of the plug's linear velocity, $\dot{x} \in \mathbb{R}^3$, 
angular velocity $\omega \in \mathbb{R}^3$, the sensed wrench $\phi \in \mathbb{R}^6$ (force-torque),
and the belief state, $b$, over the plug's location.

%probability distribution $p(x)$ over the believed location of the plug. Before going into details regarding the policy 
%to be learned, we introduce the formulation of probability density function, $p(x)$.
%The demonstrations gathered during the search for the socket entries, a priori to a connection being made, contain a wide spectrum of behaviour, 
%which at times appears to be stochastic and random. Learning directly a policy from this data, as it is often the case in LfD, 
%without a method for taking into account bad and good demonstrations will lead to a policy which will merely reproduce 
%the demonstrated behaviour including all its drawbacks. Our goal is to learn a policy which is optimal 
%in partially observable setting.


\section{Learning Actor and Critic}\label{sec:learning-value-actor}

In our approach we learn two data driven policies. The first policy maps from belief space 
to linear velocity $\pi_{\Param_1} : b_t \mapsto \dot{x}_t$ and the second from 
angular sensed wrench to angular velocity, $ \pi_{\Param_2} : \phi_t \mapsto \omega_t$.
We chose to learn the belief policy $\pi_{\Param_1}$ in a Actor-Critic RL framework 
and the wrench policy $\pi_{\Param_2}$ directly from the demonstrated data as was done 
in [cite], which proved to be efficient in overcoming jamming during the PiH. 
A POMDP solver's objective is to find a policy (Actor), $\pi_{\Param_1}: b \mapsto u$, which maximises 
the value function (Critic) $V^{\pi_{\Param_1}} : b \mapsto \mathbb{R}$ for an initial belief, $b_{0}$. The value function
is the expected reward over an infinite time horizon.
\begin{equation}\label{eq:value_function}
  V^{\pi_{\Param_1}}(b_t) = \mathbb{E}\Bigg\{ \sum_{t=0}^{\infty} \gamma^{t} r_{t+1} | b_0=b,\pi_{\Param_1}\Bigg\}
\end{equation}

In an Actor-Critic setting, the temporal difference error, $\delta_t \in \mathbb{R}$, of the value function (the Critic) is 
used both as a learning signal to update simultaneously itself and the actor (the policy). In our setting we will 
learn two separate policies, one for the linear velocity and the other for the angular velocity,
as the orientation remains the same during most of the search until it is time to connect the plug to the socket. 
When the plug has to be connected it is necessary to control for orientation to avoid jamming. 

\subsection{Actor}
%Two control policies are learned; The first $\pi_{\Param_1}: b \mapsto \U$, maps belief state $b$ to linear velocity, 
%$\U$ and the second, $\pi_{\Param_2}: \phi \mapsto \omega$ , sensed wrench, $\phi$, to angular velocity, $\omega$.
%These actors/policies are parametrised by a Gaussian Mixture Model (GMM), Equation \ref{eq:GMM}.
Both actors/policies are parametrised by a Gaussian Mixture Model (GMM), Equation \ref{eq:GMM}.

\begin{equation}
 \pi_{\Param}(\U,\B) = \sum\limits_{k=1}^{K} \piK	    \cdot  g(\U,\B;\MuK,\SigK) \label{eq:GMM}
\end{equation}
The parameters $\Param = \{w^{[k]},\MuK,\SigK\}_{1,\dots,K}$, are the weights, means and covariances 
of the individual Gaussian functions, $g(.)$,
\begin{center}
$\MuK =  \begin{bmatrix} \MuK_{\U} \\ \MuK_{\B} \end{bmatrix}$, 
$\SigK =  \begin{bmatrix} 
	  \SigK_{\U\U} & \SigK_{\U\B} \\
	  \SigK_{\B\U} & \SigK_{\B\B}
	  \end{bmatrix}$
\end{center}
where $\sum_{k} w^{[k]} = 1$, $\MuK_{\U} \in \mathbb{R}^{3}$ and  $\MuK_{\B} \in \mathbb{R}^{4}$.
%There are three steps to learning and use for control a GMM model.  Firstly a generative model is learned through 
%Expectation-Maximisation (EM) which maximised the likelihood

A generative model of the angular velocity and wrench $\pi_{\Param_2}(\omega,\theta)$ and a generative model 
of the linear velocity and belief state $\pi_{\Param_1}(\dot{x},b)$ are learned. 
In both cases we use the Bayesian Information Criterion to determine the number of Gaussian functions.
In the next section, we will show how the parameters of $\pi_{\Param_1}$ can be adapted by the value function of the Critic.

%First a generative model, $p_{\Param}(\dot{x},b)$, is learned through maximising the likelihood of 
%the dataset by Expectation-Maximisation (EM). 
%Second the conditional is taken $p_{\Param}(\dot{x}|b)$
%giving a distribution over the applicable actions $\dot{x}$, Equation \ref{eq:GMC}

%\begin{equation} 
%  \pi_{\Param}(\U|\B) = \sum\limits_{k=1}^{K} \piK(\B) \cdot  g(\B;\MuK_{\U|\B},\SigK_{\U|\B}) \label{eq:GMC}
%\end{equation}
%\begin{align}
% w^{[k]}(\B)  &= \frac{\piK \cdot g(\B;\MuK_{\B},\SigK_{\B\B})}{\sum_{j=1}^{K} w^{[j]} \cdot g(\B;\Mu{j}_{\B},\Sig{j}_{\B\B}) } \\
% \MuK_{\U|\B}   &= \MuK_{\U} + \SigK_{\U\B} \invSigK_{\B\B}(\B - \MuK_{\B})\\
%  \SigK_{\U|\B} &= \SigK_{\U\U} - \SigK_{\U\B}\invSigK_{\B\B}\SigK_{\B\U}
%\end{align}
%Thirdly a concrete action, to be applied, is derived from the conditional distribution. One approach is
%to take the expectation of the conditional, $\hat{\U} = \mathbb{E}_{p_{\Param}(\U|\B)}\{\U\}$ whilst 
%another is to draw a sample $\U \sim p_{\Param}(\U|\B)$. 

\subsection{Critic}

% Q-learning and TD(0) are based on the dynamic porgramming algorithm known as value iteration.
% 
%\cite{NIPS2008_3501} (Fitted Q-iteration by Advantage Weighted Regression)
%
%We use a combination of function approximation and dynamic programming to learn a value function 
%of the belief state with respect to the given task.

%\cite{Szepesvari:2010}

The Critic (the value function, Eq. \ref{eq:value_function}) evaluates 
the performance of the current policy. It is the expected future reward given the current 
belief state and policy.
In our setting a reward of $r=0$ is received at each time step
until the goal (plug-socket connection) is achieved, where a reward of 100 is given, $r_{T}=100$.
Given the continuous nature and dimensionality of the belief space we use locally weighted regression \cite{Atkeson97locallyweighted}
(LWR) as a function approximator of the value function, $V^{\pi}(b)$. LWR is a memory-based non-parametric function 
approximator. It keeps a set of input-target pairs $\{(\B,r)\}$ as parameters. When a value, $\B$, is 
queried, a set of $p$ neighbouring points are chosen from the input space and are 
weighted according to a distance metric. The predicted output is then the result of a weighted 
least square of the $p$ points. Equation \ref{eq:W} is the distance function used where 
$D$ is a diagonal matrix.

\begin{equation}\label{eq:W}
 W_{i,i}  = \exp\left(-\frac{1}{2}(\B - \B_i)^{\mathrm{T}}\, D^{-1} \, (\B - \B_i) \right)
\end{equation}
A new value is queried according to Equation \ref{eq:lwr_predict},
\begin{equation}\label{eq:lwr_predict}
  V^{\pi}(b) = \B \,(B^{\mathrm{T}} W B)^{-1} B^{\mathrm{T}} W \mathbf{r}
\end{equation}
where $B = (\B_1,\dots,\B_p)^{\mathrm{T}} \in \mathbb{R}^{(D\, \times\, p)}$, $W \in \mathbb{R}^{(p\, \times\, p)}$ is
a diagonal matrix, $\mathbf{r} = (r_1,\cdots,r_p)^{\mathrm{T}} \in \mathbb{R}^{(p\, \times\, 1)}$

%We chose this method because it is a safe regressor to use when estimating a value 
%function \cite{Boyan95generalizationin} and is very flexible since it is non-parameteric.

\subsubsection{Fitted Policy Evaluation}

To learn the value function we apply batch reinforcement learning \cite{EGW05}, also known as experience replay.
This is an offline method which applies multiple sweeps of the Bellman backup operator 
over a dataset of tuples $\{(\Bi_t,\Ui_t,r^{[i]}_{t},\Bi_{t+1})\}_{i=1,\cdots,M}$ until the Bellman residual,
$||V^{\pi}_{k+1}(\B) - V^{\pi}_{k}(\B)||$, converges. 

%\begin{algorithm} 
%\caption{Fitted Policy Evaluation}
%\label{alg:fpe}
%\begin{algorithmic}[1]
%    \Inputs{$K$, $\epsilon$, $\{(\B^{[i]}_t,r^{[i]},\B^{[i]}_{t+1})\}_{i=1,\cdots,M}$}
%    \State{Initialise $\hat{V}^{\pi}_{0} = 0$}
%    \For {$k=0$ in $K$}
%        \State $\hat{V}^{\pi}_{k+1}(\B_t)$ = Regress($\B$, $r_t + \gamma \hat{V}^{\pi}_k(\B_{t+1})$)
%	\If{$||\hat{V}^{\pi}_{k+1}(\B) - \hat{V}^{\pi}_{k}(\B)|| < \epsilon$ } 
%	  \State \textbf{return} $\hat{V}^{\pi}_{k+1}(\B)$
%	 \EndIf
%    \EndFor
%\end{algorithmic}
%\end{algorithm}

A wide spectrum of research has made use of batch RL methods to learn policies. 
Most of them have focused on learning the Q-value function directly (Fitted Q-Iteration) 
\cite{NIPS2008_3501,EGW05,Riedmiller05neuralfitted}. Although learning the 
Q-value function directly solves the control problem it often requires discretisation 
of the action space or assumes quantifiable actions. The reason is that doing 
Q-Bellman backups, $\hat{Q}(\B_t,\U_t) \leftarrow \gamma \max_{\U_{t+1}} \hat{Q}(\U_{t+1},\B_{t+1})$, 
requires an optimisation over the actions space, $\U_{t+1}$, to find the best applicable action. 
Given the dimensionality and continuity of our problem we opted for an on-policy evaluation method
which does not require an optimisation, but does require multiple 
\textit{policy evaluation} and \textit{policy improvements} iterations to achieve an optimal policy.
We applied Algorithm \ref{alg:fpe} on our dataset until convergence.


%Which is not the case for Fitted Value Iteration (FVI) and Fitted Q-Iteration (FQI).
%After we applied the fitted policy evaluation to our dataset get the expected future 
%reward for each of the belief belief states, giving us a new dataset:
%$D = \big\{\U^{[i]}_{1:T},\omega^{[i]}_{1:T},\phi^{[i]}_{1:T},\B^{[i]}_{1:T},v^{[i]}_{1:T}\big\}^{i=1\dots N}$, where $v^{[i]}_t = \hat{V}^{\pi}(\Bi_t)$.

\begin{figure}
 \centering
 \setlength\fboxsep{0pt}
  \setlength\fboxrule{0.25pt}
  \includegraphics[width=\textwidth]{./ch4-PiH/Figures/ValueFunction/value_func_final_v2.pdf}
 \caption{\textit{Left:} LWR approximate value function $\hat{V}^{\pi}(\B)$. \textit{Right:} first five best and worst trajectories in terms of the accumulated value.}
  \label{fig:Figure1}
\end{figure}

\begin{figure}
 \centering
 \includegraphics[width=\textwidth]{./ch4-PiH/Figures/ValueFunction/value_func_final_v3.pdf}
 \caption{} 
\end{figure}


Figure \ref{fig:Figure1} (\textit{Left}) shows the belief space with respect to the value function. As expected, the 
value function is high closest to the socket and low further away. Figure \ref{fig:Figure1}
(\textit{Right}) shows the best and worst trajectories in terms of the accumulated value function. There is a
close relationship between the best trajectories and the time taken to successfully connect the plug to the socket. We can see that the five best trajectories
(red) tend to be aligned with the socket (star position in front of socket), whilst the five worst are towards the edges
of the wall.

\subsection{Actor update}

The Temporal Difference (TD) error, is used to update the actor $\delta^{\pi}_t = r_{t+1} + \gamma V^{\pi}(b_{t+1}) - V^{\pi}(b_t)$, 
given by the critic\cite[Chap. 6]{sutton1998reinforcement}. 
In our offline approach we first computed the value function of the belief state, $V^{\pi}(\B)$, 
until convergence and then used the estimated value function to update the actor. This offline batch 
method has the advantage that no divergence will occur during the learning process.

We proceed to update the Actor given the Critic through a modification of the Maximisation step in  Expectation-Maximisation (EM) 
for Gaussian Mixture Models. We refer to this modification as Q-EM which is strongly related to a Monte-Carlo EM-based policy 
search approach \cite[p.50]{DeisenrothNP2013}. 

The reward of a demonstrated trajectory (one episode) is given by the discounted return, Equation $\ref{eq:disc_return}$.
\begin{equation}\label{eq:disc_return}
 R(\B^{[i]},\U^{[i]}) = \sum_{t=0}^{\mathrm{T^{[i]}}} \gamma^t \, r(\B^{[i]}_t,\U^{[i]}_t)
\end{equation}
All policy gradient approaches seek to find a set of parameters, $\Param$, of the Actor,
which will maximise the expected reward, equivalent to maximising Equation \ref{eq:expected_reward}.
\begin{align}\label{eq:expected_reward}
 J(\Param) &= \mathbb{E}_{\pi_{\Param}(\U,\B)}\{R(\B,\U)\} \nonumber \\
	  &= \sum\limits_{i=1}^{N}   \underbrace{\left( \prod_{t=0}^{T^{[i]}} \pi_{\Param}(\U^{[i]}_t,\B^{[i]}_t) \right)}_{\pi_{\Param}(\U^{[i]},\B^{[i]})} \, R(\B^{[i]},\U^{[i]}) 
\end{align}

To find the parameters which maximise the cost function, $\argmax_{\Param'} J(\Param')$, the derivative is taken and 
set to zero. As this cannot be done directly, we maximise the logarithmic lower bound   
of the cost function. This results in Equation \ref{eq:grad_log_cost},

\begin{equation} \label{eq:grad_log_cost}
    \nabla_{\Param'}\log(J(\Param')) = \sum\limits_{i=1}^{N} \sum\limits_{t=0}^{T^{[i]}} \nabla_{\Param'}\log \pi_{\Param'}(\U^{[i]}_t,\B^{[i]}_t) \, Q^{\pi}(\U^{[i]}_t,\B^{[i]}_t)
\end{equation}

Setting the derivative of Equation \ref{eq:grad_log_cost} to zero and solving for the parameters
$\Param=\{w,\boldsymbol{\mu},\boldsymbol{\Sigma}\}$ leads to a Maximisation update step of EM
which is weighted by $Q^{\pi}$, see Appendix \ref{app:lb} for a more complete derivation.
We use the TD error of the \textit{Critic} as a substitute for $Q^{\pi}$. Assuming that our estimated value function, $\hat{V}^{\pi_{\Param}}$, 
is close to the true value function $V^{\pi_{\Param}}$, the TD error $\delta^{\pi}$ is an unbiased estimate of the advantage function, Equation \ref{eq:advantage_f} 
(see Appendix \ref{app:unbiased_delta}).
\begin{equation}\label{eq:advantage_f}
 A^{\pi}(b_t,u_t) = Q^{\pi}(b_t,u_t) - V^{\pi}(b_t) = \delta^{\pi}_t
\end{equation}
Using the advantage function as means of policy search is popular with examples such as
Natural Actor Critic (NAC) \cite{peter_nac_2008}.

Each data point has an associated weight, $\delta \in \mathbb{R}$, where $\delta^{[m]} \geq 0$ means that the 
state action-pair $\X^{[m]}$ leads to an increase in the value function and $\delta^{[m]} \leq 0$ leads to 
a decrease in the value function. The likelihood is re-weighted accordingly, giving more importance to data points which lead to a gain. Since 
the Q-EM update steps cannot allow negative weights, we rescale the TD error to be between 0 and 1.

%We learn the plug-socket search policy using the Q-EM update rules for a GMM parametrisation with the 
%data set, $D_1 = \big\{\U^{[i]}_{1:T},\B^{[i]}_{1:T},\delta^{[i]}_{1:T}\}^{i=1\dots N}$ and learn a second policy for inserting the 
%final steps of connecting the plug to socket with data set $D_2 = \{\omega^{[i]}_{1:T},\phi^{[i]}_{1:T}\}^{i=1\dots N}$. 
%In Figure \ref{fig:data_flow} we illustrate the data flow and the policies which are learned. Note that we also learn a belief space 
%policy directly from the demonstrations, $\pi_{\Param_3}(\U,\B)$, which we will refer to as the GMM policy and we will use it to compare 
%with the policy corrected by the Critic, which we call the Q-EM policy.
%\begin{figure}[h]
%  \centering
%  \includegraphics{Figures/data_flow_2.pdf}
%  \caption{Data flow and policies learned. The original data consists of five variables which are recorded during the demonstrations given 
%  by the human teachers. The probability density function of the believed location of the end effector is compressed to a belief state, which 
%  is the most likely state and entropy. The value function is learned via fitted policy iteration and each data point has a temporal difference 
%  error. An actor policy is learned via Q-EM, $\pi_{\Param_1}(\U,\B)$ , a regular policy is learned via the standard EM approach, $\pi_{\Param_3}(\U,\B)$
%  and a plug insertion policy is learned $\pi_{\Param_2}(\U,\B)$.}
%  \label{fig:data_flow}
%\end{figure}

\begin{figure}
 \centering
 \setlength\fboxsep{0pt}
  \setlength\fboxrule{0.25pt}
  \includegraphics[width=\textwidth]{./ch4-PiH/Figures/fpe_example.pdf}
 \caption{}
  \label{fig:fpe_example}
\end{figure}


\section{Control architecture}\label{seq:control_architecture}

We learned both a Gaussian Mixture Model for both the linear and angular 
velocity. For most of the search and up until the plug is within the socket's hole, only 
the linear control policy is active. The orientation is kept constant.
The direction to search is given by conditional, Equation \ref{eq:gmm_conditional},

\begin{equation}\label{eq:gmm_conditional}
 \pi_{\Param}(\dot{x}|b) = \sum_{k=1}^{K} w^{[k]}_{\xb} \cdot g(\dot{x};\MuK_{\xb},\SigK_{\xb})
\end{equation}

which is a distribution over the possible normalised velocities. The subscript $\xb$ indicates that the parameters 
are the result of the conditional. The reader is referred to \cite{gesture_calinon_2010},\cite{gmr_2004} for 
a detailed derivation of the conditional of a GMM. In autonomous
dynamical systems control, the velocity to is taken from 
the expectation of Equation \ref{eq:gmm_conditional}. The model 
learned is multi-modal, as different search velocities are possible 
in the same belief state. Taking the expectation, which is weighted 
linear combination of the modes would result in unobserved behaviour or 
no movement if the velocities cancel out. In Figure \ref{fig:policy_vf}
we illustrate the multi-modal vector fields of the conditional, Equation \ref{eq:gmm_conditional}.
As a result we use a modified version of the expectation operator which favours the current
direction, Equation \ref{eq:alpha_eq} - \ref{eq:alpha_expectation}.

\begin{align}
 \alpha(\dot{x}) &= w^{[k]}_{\xb} \cdot \exp(-\cos^{-1}(<\dot{x},\MuK_{\xb}>)) \label{eq:alpha_eq}\\
 \dot{x} &= \mathbb{E}_{\alpha}\{\pi_{\Param}(\dot{x}|b)\} = \sum_{k=1}^K \alpha_k(\dot{x}) \cdot \MuK_{\xb} \label{eq:alpha_expectation}
\end{align}

When a velocity mode being applied is no longer present (because we have moved into a region of belief space where the current applied 
velocity has not been seen) another direction mode is sampled. As an example, when the robot suddenly enters in contact with a feature,
which greatly reduces the uncertainty, the current modes will dramatically change and cause a new search direction to be computed. 

The above steps can control the general behaviour of the search but are insufficient for a successful implementation on a robotic system, 
such as the KUKA LWR.
This search task is haptic and as a result the end-effector of the robot will always be in contact with the environment. To make the robot
compliant with the environment we use an impedance controller in combination with a hybrid position-force controller. Our hybrid controller
targets a sensed force $F_x$, in the $x$-axis, of 3N. The other two velocity components of the direction vector are given by 
Equation \ref{eq:alpha_expectation}.This force by itself in insufficient to reliably surmount the edges of the socket and the robot 
will become stuck at the edges, unable able to surmount the friction as these right angle contacts. To overcome the edges we locally modulated 
the vector field of the GMM in $y$ and $z$-axis, Equation \ref{eq:modulation}.

\begin{equation}
  \dot{x} = R_y(c(F_z) \cdot \pi/2) \cdot R_z(c(F_y) \cdot \pi/2) \cdot \dot{x} \label{eq:modulation}
\end{equation}

$R_y$ and $R_z$ are rotation matrix around the $y$ and $z$-axis, $c(F) \in [-1,1]$ is a truncated scaling function of the sensed 
force.  When a force $F_z$ of 5N is sensed, a rotation of $R_y(\pi/2)$ is applied to the original direction resulting in the robot
getting over the edge. The direction velocity is always normalised up to this point. The amplitude of the velocity is a proportional
controller based on the believed distance to the goal,
\begin{equation}
  \nu = \max(\min(\beta_1,K_p (x_g - \hat{x}),\beta_2)\label{eq:prop_speed}
\end{equation}
where the $\beta$'s are lower and upper amplitude limits, $x_g$ is the position of the
goal, and $K_p$ the proportional gain which was tuned through trials. In Figure \ref{fig:control_flow}
we illustrate the complete control flow.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{./ch4-PiH/Figures/control_flow_final.pdf}
  \caption{Control architecture. The PMF (belief) received a measured velocity, $\dot{\tilde{x}}$, and sensor feature $\tilde{y}$ and gets updated 
  via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller, Equation \ref{eq:prop_speed}.}
  \label{fig:control_flow}
\end{figure}

\begin{figure}
   \includegraphics[width=\textwidth]{./ch4-PiH/Figures/Fig/policy_vf.pdf}
  \caption{Q-EM and GMM policy vector fields. \textit{Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level,
  most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see 
  that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that 
  the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. 
  \textit{Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends 
  to be closer to a sink and there is less variation.}
  \label{fig:policy_vf}
\end{figure}

\FloatBarrier
\section{Results}\label{ch4:results}

We evaluate the following three properties of the policy learned in our Actor-Critic framework:
\begin{enumerate}
 \item \textbf{Distance taken to accomplish the goal} (connect plug to socket). We compare the Q-EM policy with 
 a GMM policy learned through standard EM and a myopic Greedy policy. This highlights the difference between complicated and simplistic  
  search algorithms and as a result gives an appreciation of the problem's difficulty.
 \item \textbf{Importance of data} provided by human teachers. We evaluate whether it is possible to learn 
 an improved GMM policy from Greedy demonstrations. We call this policy Q-Greedy and it is used to test whether 
 human demonstrations are necessary.
 We evaluate whether it is possible to obtain a good policy from the worst two teachers. Not all teachers 
 are necessarily proficient at the task in question and we want to test whether our methodology can be applied in these
 cases. We evaluate if we  are able to obtain an improved policy from the worst two teachers.
 \item \textbf{Generalisation}. We learn a policy to insert a plug into socket A which was located at the center of the wooden 
 wall. We test the generalisation of the policy in finding a new socket location. We further test whether the policy can generalise to two new sockets 
 which were not used during the training phase.
\end{enumerate}

We evaluate the above properties under two separate conditions. In the \textbf{first condition} we consider the period between the start 
of the search until the socket is localised. In the \textbf{second condition} we consider the period from the point the socket is found until
a connection has been established. In the first condition the evaluation is done in simulation whilst 
in the second condition, when the socket is found, we perform the evaluation with a physical robot, the KUKA LWR4.

This choice is motivated by the fact that the two parts of the task require different levels of precision.
Finding the socket requires much less precision than establishing a connection. It is thus
more informative to consider the performance of these two parts separately.
Another aspect is that the search for the socket can be reliably evaluated in simulation since the physics of the interaction
is simple. The connection phase is more complicated and a simulation would be unrealistic.
For the evaluation of the connection of the plug to the socket we consider the search start point already within 
the vicinity of the socket.

\subsection{Distance taken to reach the socket's edge (Qualitative)}

We consider three search experiments which we refer to as \textbf{Experiment 1, 2} and \textbf{3},
in order to evaluate the performance (distance travelled to reach the socket) of three search policies: GMM, Q-EM and Greedy.
In these three experiments the task is considered accomplished when a search policy finds the socket's edge. 

In \textbf{Experiment 1}, three starting locations are chosen: \textit{Center}, \textit{Left} and \textit{Right}, 
see Figure \ref{fig:box_exp_sim}, \textit{Experiment 1}, for an illustration of the initial condition. 
This setup tests the effect of the starting positions. A total of 25 searches are carried out for each of the search policies.

In \textbf{Experiment 2}, two \textit{Cases} are chosen in which the believed state (most likely state of the PMF) and the true position
of the end-effector are relatively far apart. The location of the beliefs are chosen to be symmetric, see the Figure \ref{fig:box_exp_sim}, 
\textit{Experiment 2}. A total of 25 searches are carried for each of the two conditions.

In \textbf{Experiment 3}, Figure \ref{fig:box_exp_sim}, \textit{Experiment 3}, the initial true starting positions 
of the end-effector are taken from a regular grid covering the whole start region, also used as the initial distribution for 
the human demonstrations. A total of a 150 searches are carried out for each of the three policies. 
This experiment compares the search policies with the human teachers.

\begin{figure}
    \centering
    \hspace*{-1.4cm}
    \includegraphics[width=1.2\textwidth]{./ch4-PiH/Figures/Fig/experiment_final_v3.pdf}
    \caption{Three simulated search experiments. \textbf{Experiment 1:} Three start positions are considered: \textit{Left}, \textit{Center} and
     \textit{Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. 
     In the second row of Experiment 1, we illustrate the trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition 
     a total of 25 searches were performed for each search policy. 
     \textbf{Experiment 2:} Two cases are considered: \textit{Case 1} blue, the initial belief state (circle) is fixed facing 
     the left edge of the wall and the true location (diamond) is facing the socket.
     \textit{Case 2} pink, the initial belief state (circle) is fixed to the right facing the edge of the wall and the 
     true location is the left edge of the wall. In the second row, the trajectories are plotted for \textit{Case 1}.
     \textbf{Experiment 3:} A 150 start locations are deterministically generated from a 
     grid in the start area. In the second row, we plot the distribution of the areas visited by the true position during the search.}
    \label{fig:box_exp_sim}
\end{figure}

We evaluate the performance of the three experiments in terms of the trajectories and their distribution in reaching 
the edge of the socket. 

We can see a clear difference between the trajectories generated by the GMM and Q-EM policies in Experiment 1, 
see Figure \ref{fig:box_exp_sim} \textit{Experiment 1, second row}. The orange GMM policy trajectories 
go straight towards the wall, whilst the yellow Q-EM policy trajectories drop in height making them closer 
to the socket. The same effect can be seen in Experiment 2 (\textit{second row}). The Q-EM trajectories 
follow a downward trend towards the location of the socket. The gradient is less due to the initial starting condition being lower than in Experiment 1. 

The trajectories of the Greedy policy depend on the chosen believed location (most likely state of the PMF). In the 
second experiment there is no variance in the Greedy's trajectories until it reaches the edge of the red square, 
where the branching occurs as the believed location is disqualified. This happens as  
no sensation is registered when the believed location reaches the wall as the true location is before the believed location, see 
Figure \ref{fig:box_exp_sim}, \textit{Experiment 2, second row}.

In Figure \ref{fig:box_exp_sim} \textit{Experiment 3, second row}, both Human and GMM distributions of
searched locations are similar. They cover the upper region of the wall and top corners, to 
some extent. These distributions are not identical for two reasons. The first is that the learning of the GMM is a 
local optimisation which is dependent on initialisation and number of parameters. The second reason is that 
the synthesis of trajectories from the GMM is a stochastic process. 

The distribution of the searched locations of the Q-EM policy is centred around the origin of the $z$-axis,
see Figure \ref{fig:box_exp_sim}  \textit{Experiment 3, second row}. 
The uncertainty is predominantly located in the $x$ and $y$-axis. The Q-EM policy takes this uncertainty 
into consideration by restraining the search to the $y$-axis regardless of the starting position. The uncertainty 
is reduced whilst remaining in the vicinity of the socket. 
The Greedy's policy search distribution is multi-modal and centred around the $z$-axis where the modes are above 
and below the socket. This shows that the Greedy policy acts according to the most likely state 
which changes from left to right of the socket, because of motion noise, resulting in left-right 
movements and little displacement. As a result the Greedy policy spends more time at these modes.

In Figure \ref{fig:first_contact} (\textit{Top-left}), we illustrate the distribution of the first contact with the wall 
during Experiment 1 for the \textit{Center} initial conditions. The distribution of the first contact of the Greedy method is uniform across 
the entire $y$-axis of the wall. 
It does not take into account the variance of the uncertainty. In contrast, the GMM policy remains centred with respect to the starting position and the Q-EM is even closer to the socket and 
there is much less variance in the location of the first contact.

\begin{figure}
  \centering
   \includegraphics[width=0.45\textwidth]{./ch4-PiH/Figures/Fig/first_contact_center.pdf}
   \includegraphics[width=0.45\textwidth]{./ch4-PiH/Figures/Fig/first_contact_right.pdf}
   \caption{First contact with the wall, during experiment 1. (a) Contact distribution for initial condition ``Center'' . (b) 
   Contact distribution for initial condition was ``Right''. The ellipses correspond to two standard deviations of a fitted Gaussian 
   function.}
   \label{fig:first_contact}
\end{figure}

\subsection{Distance taken to reach the socket's edge (Quantitative)}

In Figure \ref{fig:three_searches} we illustrate the quantitative results of the distance taken 
to reach the socket for all three experiments. In \textbf{Experiment 1}, for the \textit{Center} initial condition,
the Q-EM policy travels far less than the other search policies. Considering that the initial position of the search is 
0.45 [m] away from the wall, the Q-EM policy finds the socket very quickly once contact has been established with the wall. 
For the \textit{Right} and \textit{Left} starting conditions both the GMM and Q-EM policies travel less distance to reach the socket, with a 
smaller variance when compared with the Greedy search policy.

In \textbf{Experiment 2}, the Q-EM search policy is the most efficient. For \textit{Case 1} of Experiment 2, the initial most 
likely state is fixed to the left and the true position is facing the socket. As the belief is chosen to be 
to the left, upon contact with the wall the policy takes a left action since it is more likely to result in a localisation, given 
that the left edge of the wall is within close proximity. 
This on average results in an exploration in the upper left area of the wall, which explains why \textit{Case 1} does worse than Experiment 1
for the \textit{Center} initial condition. In \textit{Case 2} however, where the true state is facing the left 
edge and the believed position is facing the right edge, less distance is taken to find the socket than it does for Case 1, 
Figure \ref{fig:three_searches} (b), as reason for the improvement over Case 1, is that in \textit{Case 2} the true location of 
the end-effector is close to an edge which is an informative feature and results in a much faster localisation.

From \textbf{Experiment 3}, Figure \ref{fig:three_searches_exp3}, it is clear that the three search policies have 
less variation in the distance travelled to find the socket's edge than the human teachers. 
All search policies are better than the human teachers with the exception of group B\textsuperscript{*}, 
which is performing the task with socket A. The Q-EM policy remains the best. 

\begin{figure}
 \centering
  \includegraphics[width=\textwidth]{./ch4-PiH/Figures/Fig/experiment_1_2.pdf}
  \caption{Distance travelled until the socket's edge is reached. a) Three groups correspond to the initial conditions: Center, Left and Right
   depicted in Figure \ref{fig:box_exp_sim}, \textit{top left}. The Q-EM method is always better than the other methods, in terms of distance. b)
   Results of the two initial conditions depicted in Figure \ref{fig:box_exp_sim}, \textit{top middle}, both the true position and most likely state are
   fixed. The Q-EM method always improves on the GMM. }
   \label{fig:three_searches}
\end{figure}
\begin{figure}
 \centering
  \includegraphics[width=0.6\textwidth]{./ch4-PiH/Figures/Fig/experiment3_plot2.pdf}
  \caption{Distance travelled until the socket's edge is reached. Results corresponding to Experiment 3, Figure \ref{fig:box_exp_sim}, \textit{top right}.
   Again the Q-EM method is better, but at a less significant level.}
   \label{fig:three_searches_exp3}
\end{figure}
 

We have shown that under three different experimental settings the Q-EM algorithm is predominantly the best in terms of distance taken 
to localise the socket. The GMM policy learned solely from the data provided by the human teachers also performs well in comparison to  
the human teachers and Greedy policy. We made, however a critical assumption in order to be able to use our (RL-)PbD-POMDP approach. 
This \textbf{assumption} is that a human teacher is proficient in accomplishing the task. If a teacher is not able to accomplish 
the task in a repetitive and consistent way so that a search patter can be encoded by the GMM, the learned policy will perform poorly.
We next evaluate the validity of this assumption and the importance of the training data provided by the human teachers.
% 
\subsection{Importance of data}

We perform two tests to evaluate the importance of the teachers training data for learning a search policy. Firstly we take the 
worst two teachers in terms of distance taken to find the socket's edge and learn a GMM and Q-EM policy separately from their 
demonstrations. In this way we can evaluate whether it is possible to learn a successful policy given 
a few bad demonstrations (15 training trajectories for each policy). Our second evaluation consists of using a noisy 
explorative Greedy policy as a teacher to gather demonstrations which can then be used to learn a new policy, which we call Q-Greedy. 

Figure \ref{fig:subj_5_traj} illustrates 6 trajectories of teacher \# 5. The human teacher preferred to
localise himself at the top of the wall before either proceeding to a corner or going directly towards the socket. Once localised, the teacher 
would reposition himself in front of the socket and try to achieve an insertion. This behaviour was not expected 
since by losing contact with the wall, the human teacher no longer has sensory feedback which is necessary 
to maintain an accurate position estimate.

\begin{figure}
 \centering
    \includegraphics[width=\textwidth]{./ch4-PiH/Figures/Fig/subject5.pdf}
%    \subfigure[]{\includegraphics[width=0.45\textwidth]{./Figures/Results1/experiment4/value_subj_5.pdf}}
%    \subfigure[]{\includegraphics[width=0.445\textwidth]{./Figures/Results1/experiment4/experiment4.pdf}}
    \caption{Demonstrations of teacher \# 5. The teacher demonstrates a preference
    to go first to the top of the wall. He then leaves contact with the wall to position himself in front of the socket before trying to 
    find it}
    \label{fig:subj_5_traj}
 \end{figure}
 
\begin{figure}
 \centering
 \includegraphics[width=0.8\textwidth]{./ch4-PiH/Figures/Fig/value_subj_5.pdf}
 \caption{Value function learned from the 15 demonstrations of teacher \#5. The value of the belief space trajectories are plotted.}
 \label{fig:value_function_subj_5}
\end{figure}
 
 \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./ch4-PiH/Figures/Fig/gmm_v2c.pdf}
    \caption{Marginalised Gaussian Mixture parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. 
    The illustrated transparency of the Gaussian functions is proportional to their weight.
    \textit{Left column}: The Gaussian functions of the Q-EM have shifted from the left corner to the right. This is a result of the value function 
    being higher in the top right corner region, see Figure \ref{fig:value_function_subj_5}. \textit{Center column}:  The original data of the teacher 
    went quite far back which results in a Gaussian function given a direction which moves away from the wall (green arrow), whilst in the case
    of the Q-EM parameters this effect is reduced and moved closer towards the wall.  We can also see from the two plots of the Q-EM parameters 
    that they then follow the paths encoded by the value function.    
    \textit{Right column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really 
    encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent 
    the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge.}
    \label{fig:gmm_exp4}
\end{figure}
 
Figure \ref{fig:value_function_subj_5} illustrates the value function of the belief state learned from the data of teacher \# 5.
The states with the highest values seem to create a path going from the socket towards the right edge of the wall. 
We proceed as before to learn a GMM policy from the raw data and a Q-EM policy in which the data points are weighted by 
the gradient of the value function. In Figure \ref{fig:gmm_exp4}, we illustrate the 
resulting Marginalised Gaussian Mixture parameters for both the GMM and Q-EM policies and we plot 25 rollouts of each policy starting at 
the \textit{Center} initial condition used in Experiment 1. We note that the trajectories of the GMM 
policy seem to have a lot of variance in contrast to the Q-EM policy, resulting from an access of variance amongst the 15 original demonstrations
given by the teacher. To much variance is not necessarily good, a random policy will have to most variance (uniform) in terms of produced trajectories
and will be extremely poor and achieving the goal. Furthermore there is insufficient data to encode a pattern for the GMM model. In contrast, the Q-EM finds a 
pattern by combining multiple parts of the available data and as a result fewer data points are necessary to achieve a good policy. 
This effect is clear in Figure \ref{fig:experiment4_stats}, showing the performance of the GMM and Q-EM algorithms 
under the same initial conditions as in Experiment 1. For all the conditions and for both teachers \#5 and \#7 the Q-EM policy 
always does better than the GMM.

\begin{figure}
 \centering
 \includegraphics[width=0.8\textwidth]{./ch4-PiH/Figures/Fig/experiment4.pdf}
 \caption{Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.}
 \label{fig:experiment4_stats}
\end{figure}

We also tested whether we could use the Greedy policy as a means of gathering demonstrations in order to learn a value function and 
train a Q-Greedy policy. We used the Q-Greedy algorithm in combination with random perturbations applied to the Greedy velocity, to act as 
a simple exploration technique. We performed a maximum of 150 searches, which terminated once the socket was found and used these demonstrations to 
learn a value function and GMM policy which we refer to as Q-Greedy. Figure \ref{fig:three_searches} illustrates the statistical results 
of the Q-Greedy policy for Experiment 1 and 3, showing that there is no difference between two policies. 
Our exploration method is probably too simplistic to discover meaningful search patterns and we could probably devise better 
search strategies which would result in a good policy. However we have shown that human behaviour does already have a usable trade-off 
between exploration and exploitation which can be used to learn a new policy through our RL-PbD-POMDP framework.

\subsection{Generalisation}

An important aspect of a policy or any machine learning methodology is to be able to generalise. So far we have trained and 
evaluated our policy within the same environment. To test whether our GMM policies can generalise to a new setting we changed 
the location of the socket to the upper right corner of the wall. The GMM was trained in the frame of reference of the socket and
when we translated the socket's location it also translated the policy. 

\begin{figure}
 \centering
    \subfigure[]{\includegraphics[width=\textwidth]{./ch4-PiH/Figures/Fig/traj_experiment5.pdf}}
    \caption{Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a 
    \textit{Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the 
    extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for 
    the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for 
    Experiment 1. }
    \label{fig:experiment5_traj}
\end{figure}

\begin{figure}
 \centering
    \subfigure[]{\includegraphics[width=0.8\textwidth]{./ch4-PiH/Figures/Fig/experiment5.pdf}}
    \caption{Distance taken to reach the socket's edge. For the Fixed setup (see Figure \ref{fig:experiment5_traj}) for 
    the initial condition), both the Q-EM and GMM significantly outperform the Greedy. }
    \label{fig:experiment5_stats}
\end{figure}

To evaluate the generalisation of our learned policy we use the same initial conditions of Experiment 1 with an additional 
new configuration named \textit{Fixed}, in which both the true and believed location are fixed, blue triangle and circle,
see Figure \ref{fig:experiment5_traj}, which illustrates the trajectories 
of the three search policies for the \textit{Fixed} initial condition. The Greedy policy moves in a straight line towards the top
right corner of the table. As the true position is to the right, it takes the Greedy policy longer to find the wall 
in contrast to both the GMM and Q-EM policies. From the statistical results shown in Figure \ref{fig:experiment5_stats} we can see
that for the \textit{Fixed} and \textit{Right} initial condition, which are similar, both GMM and Q-EM are better. However, for 
the \textit{Center} and \textit{Left} initial condition this is no longer the case. 
The Greedy method is better under this condition since the socket is close to informative features (it is located close to the edges of the wall). 
Once the end-effector has entered in contact with the wall the actions of the Greedy policy always result in a decrease of uncertainty, which was not the case when the socket was located in the center of wall. 
Thus in both the \textit{Fixed} and \textit{Right} initial condition the Greedy method does worse because it takes longer
to find the wall.

The GMM based policies are still able to generalise under different socket locations. In general, as the socket's location is moved 
further from the original frame of reference in which it was learned, the more likely will the search quality degrade. We 
chose the upper right corner since it is the furthest point from the origin and the GMM and Q-EM policies were still able to find 
the socket. The policy will always be able to find the socket once it has localised itself. This is can be seen from the vector field 
of the policy, see Figure \ref{fig:policy_vf}, when the entropy is low. In this case the policy acts like a point attractor.

\subsection{Distance taken to connect the plug to the socket}
% Real socket experiment.
% Show initial condition setup with belief. (the experiment)
In this section we evaluate the distance taken for the policies and humans to establish a connection, after the socket 
has been found. We start measuring the distance 
from the point that the plug enters in contact with the socket's edge until the plug is connected to the socket. All the following evaluations are done 
on a KUKA LWR4 robot. The robot's end-effector is equipped with a plug holder on which is attached a force-torque sensor, 
the same holders used during the demonstration of the human teachers. In this way both the teacher and robot apprentice share 
the same sensory interface.

We chose to have the robot's end-effector located to the right of the socket and a belief spread uniformly 
along the z-axis. See Figure \ref{fig:real_pictures} for an illustration of the initial starting condition.
This initial configuration was used to evaluate the search policies for three different sockets, see Figure \ref{fig:real_policy} (a) for 
an illustration of the sockets. We kept the same initial configuration for the evaluation of the three sockets so that 
we can observe the generalisation properties of the policies. As a reminder we only used the training data 
from demonstrations acquired during the search with socket A. Socket B has a funnel which should make it 
easier to connect whilst socket C should be harder as it has no informative features on its surface. 

For each of the sockets we performed 25 searches starting from the same initial condition. In Figure \ref{fig:real_policy} we plot
the trajectories of each of the search methods for socket A. The GMM reproduces some of the behaviour exhibited by humans, such as 
first localising itself at the top of the socket before trying to attempt to make a connection. The Q-EM algorithm exhibits less variation
than the GMM and tends to pass via the bottom of the socket to establish a connection. The Greedy method in contrast is much more  
stochastic since it does not take into consideration the variance of the uncertainty  but instead tries to directly establish a connection.
In Figure \ref{fig:real_statistics} (c) we can see that for socket A both the Greedy and Q-EM are better than the GMM and the Q-EM has less
variance in comparison to the Greedy searches. When compared to the human's performance, all three search methods are vastly superior, 
see Figure \ref{fig:real_statistics}.  In Figure \ref{fig:real_pictures} we illustrate a typical rollout of the GMM search policy for both 
socket A and C. Once a contact is made with the socket's edge the policy tends to stay close to informative features and tends to 
wander vertical up and down motions. Only when the uncertainty has been reduced does the GMM policy try to go towards the socket's connector. 

\begin{figure}
 \centering
    \subfigure[]{\includegraphics[width=\textwidth]{./ch4-PiH/Figures/Results2/socket_connection_A.pdf}}
    \caption{%(a) All three sockets have the same connector interface, but have different structures. Both 
   % socket A and B have an edge around the center whilst the surface of socket C is featureless and more elevated than the other two. 
    25 search trajectories for each of the three search policies for socket A. }
    \label{fig:real_policy}
\end{figure}


\begin{figure*}
 \centering
 \subfigure[]{\includegraphics[width=0.95\textwidth]{./ch4-PiH/Figures/Results2/s1_sequence.pdf}}
 \subfigure[]{\includegraphics[width=0.95\textwidth]{./ch4-PiH/Figures/Results2/s2_sequence.pdf}}
 \caption{KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the 
 right of socket A. The second row are screen captures of the ROS Rviz data visualiser in which we see the Point Mass Filter 
 (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until 
 the top was reached before making a connection. (b) Same initial condition as in (a) but with socket C. The policy leads the plug down to 
 the bottom corner of the socket before going the center of the top edge, localising itself, and then makes a connection.}
 \label{fig:real_pictures}
\end{figure*}

\begin{figure}
 \centering
   \includegraphics[width=\textwidth]{./ch4-PiH/Figures/Results2/real_exp_socketAB.pdf}
  % \subfigure[]{\includegraphics[width=0.3\textwidth]{./ch4-PiH/Figures/Results2/real_exp_socketB.pdf}}
  % \subfigure[]{\includegraphics[width=0.3\textwidth]{./ch4-PiH/Figures/Results2/peg_socket_connection.pdf}}
  \caption{Distance taken to connect plug to socket once the socket is localised. (a) \textbf{Socket A}. The human 
  Group A are the set of teachers who first started with socket A. They had no previous training on another socket beforehand. Group 
  B\textsuperscript{*} first gave demonstrations on Socket B before giving demonstrations on Socket A. Group B\textsuperscript{*}
  is better than Group A at doing the task. This is most likely a training effect. However all policy search methods are far better
  at connecting the plug to the socket. (b) \textbf{Socket B}. Both Groups A\textsuperscript{*} and B are similar in terms 
  of the distance they took to insert the plug into the socket and as was the case for (a), the search policies travel less to accomplish 
  the task.   } 
  \label{fig:real_statistics}
\end{figure}

\begin{figure}
 \centering
   \includegraphics[width=0.8\textwidth]{./ch4-PiH/Figures/Results2/peg_socket_connection_v2.pdf}
   \caption{Distance taken (measured from point of contact of plug with socket edge) to connect the plug to the socket.}
  \label{fig:real_statistics2}
\end{figure}

The GMM and Q-EM policies are able to generalise to both socket B and C, as the geometric shape and connector interface of the 
two sockets are similar to socket A. The local force modulation of the policy's vector field, which isn't learned, allows the 
end-effector to surmount edges and obstacles whilst trying to maintain a constant contact force in the x-axis. This modulation makes it possible for the plug to get on top of socket C.
In Figure \ref{fig:real_statistics} (c) we illustrate the statistics of the distance taken to establish a connection for all three sockets. 
The point of interest is that both the GMM and Q-EM algorithms do better than the Greedy approach for socket C. Socket C has no informative 
features on its surface and as a result myopic policies such as in the Greedy case will perform poorly. However for socket A 
and B, the Greedy policy performs better as both of these sockets have edges around their connector point allowing for easy localisation. 
It can also be seen that most search methods perform better on socket B than A, since the funnel shape connector helps in maintaining the plug 
within the vicinity of the socket's holes. 


The search discrepancy between the performance of the humans and search policies can be attributed to many causes. One plausible reason is 
that the PMF probability density representation of the belief is more accurate than the human teachers. 
The motion noise parameter was fixed to be proportional to the velocity and the robot moves at gentle pace ($\sim1$ cm/s) as 
opposed to some of the human teachers. In actuality, we are far less precise than the KUKA which has sub-millimetre accuracy.

\section{Discussion \& Conclusion}\label{ch4:conclusion}
% Recapulate what we did 
%
% We learned a 
%
%

In this work we learned search policies from demonstrations provided by human teachers for a task
which consisted of first localising a power socket (either socket A, B or C) and then connecting it with a plug. Only haptic information 
was available as the teachers were blindfolded. We made the assumption that the position belief of the human teachers 
was initially uniformly distributed in a fixed rectangular region of which they were 
informed and is considered prior knowledge. All subsequent beliefs were then updated in a Bayesian recursion 
using the measured velocity obtained from a vision tracking system, and wrench acquired from a force torque sensor attached 
to the plug. The filtered probability density function, represented by a Point Mass Filter, was then compressed to the 
most likely state and entropy.

Two Gaussian Mixture Model policies where learned from the data recordedj during the teaching by the human teachers . 
The first policy, called Q-EM, was learned in an Actor-Critic RL framework in which a value function was learned over 
the belief space. This was then used to weigh training datapoints in the M-step update of Expectation-Maximisation (EM). The second 
policy, called GMM, was learned using the standard EM algorithm, considering all training data points equally,
following in the footsteps of our initial approach \cite{Chambrier2014}. Both the Q-EM and GMM policies were trained 
with data solely from the demonstrations of the search with socket A.

We evaluated 4 different aspects of the learned policies. Firstly, we evaluated which of three policies, Q-EM, GMM and a Greedy policy, 
took the least distance to find the socket. We concluded that across three different Experiments the Q-EM algorithm was always 
the best. It was clear that the Q-EM policy was less random and more consistent than the GMM policy as it tried to enter in 
contact with the wall at the same height as the socket thus increasing the chances of finding the socket.

Secondly, we tested the importance of the data provided by the human teachers. We took the worst two teachers and trained an
individual GMM and Q-EM policy for each of them. We found that the performance of the Q-EM was better than the GMM in terms 
of distance travelled to find the socket. When qualitatively evaluating the trajectories of the GMM with respect to the 
Q-EM for the worst teacher, it is clear that the Q-EM policy managed to extract a search pattern, which was not the case 
for the GMM policy. We also tried to learn a Q-EM policy from the data provided by a Greedy policy with explorative noise 
and we found no improvement. From these results we conclude that the exploration and exploitation aspects of the trajectories 
provided by the human teachers is necessary.

Thirdly we tested whether the two policies were able to generalise to a different socket location. Under a specific condition,
which we called \textit{Fixed}, both policies were significantly better than the Greedy policy. However for the \textit{Center}
and \textit{Left} initial conditions the Greedy policy was better. For the initial conditions in which the Greedy policy 
enters in contact with the wall at an early stage, it performs better than the GMM and Q-EM.  The reason for this is that  
the actions taken by the Greedy policy in this setting will always result in a decrease of entropy when the location
of the socket is close to a corner, as opposed to being in the center of the wall.

Fourthly we evaluated the three policies on the KUKA LWR4 robot. First all the policies did better than the human 
teachers. For socket A, on which both the GMM and Q-EM policies were trained, there is no clear distinction between 
the Q-EM and Greedy policy. On socket B, which was novel, the Greedy policy performed better than the statistical controllers, 
which we hypothesize was a result of a funnel which would make it easier for a myopic policy. For socket C, both the 
GMM and Q-EM policies do better than the Greedy, as socket C has no features on its surface, this being a disadvantage 
for a myopic policy.

We concluded by making the observation that by simply adding a binary reward function in combination with 
data provided by human demonstrations, with Fitted reinforcement learning, we can learn a better policy without 
the need of doing expensive exploration-exploitation  rollouts traditionally associated with reinforcement learning and 
designing complicated reward functions. This is especially advantageous when only a few demonstrations are available.

\section{Appendix}
\subsection{EM policy search}\label{app:lb}
Steps taken to make a policy $pi_{\Param}(\U,\B)$ maximise the objective function, $J(\Param)$.
The policy will be maximised with respect to the lower bound of the cost function $J(\Param)$:
\begin{align}
  J(\Param') &= \sum_{i \in \mathbb{T}} \pi_{\Param'}(\Ui,\Bi)\, R(\Ui,\Bi) \nonumber\\ 
	   &= \sum_{i \in \mathbb{T}}  \frac{\pi_{\Param'}(\Ui,\Bi)}{\pi_{\Param}(\Ui,\Bi)} \, \pi_{\Param}(\Ui,\Bi) \, R(\Ui,\Bi)
\end{align}

where $\mathbb{T}$ is the set of all rollouts. Next we take the logarithm and make use of Jensen's inequality and move the logarithm into the 
summation.

\begin{align}
  \log( J(\Param') )  &= \log \sum_{i \in \mathbb{T}} \frac{\pi_{\Param'}(\Ui,\Bi)}{\pi_{\Param}(\Ui,\Bi)} \, \pi_{\Param}(\Ui,\Bi) \, R(\Ui,\Bi) \nonumber \\
		     & \geq \sum_{i \in \mathbb{T}}\log\Bigg( \frac{\pi_{\Param'}(\Ui,\Bi)}{\pi_{\Param}(\Ui,\Bi)}\Bigg) \, \pi_{\Param}(\Ui,\Bi) \, R(\Ui,\Bi) \label{eq:lower_bound}
\end{align}

We take the derivative of the lower bound of $\log(J(\Param'))$, Equation \ref{eq:lower_bound}, with respect to $\Param'$ and set it to 
zero so as to maximise the cost function.

\begin{align}
 &\nabla_{\Param'}  \log( J(\Param') ) = \nonumber\\
 &\sum_{i \in \mathbb{T}} \nabla_{\Param'} \log\big( \pi_{\Param'}(\Ui,\Bi)\big) \, \pi_{\Param}(\Ui,\Bi) \, R(\Ui,\Bi) \nonumber \\
				    & - \underbrace{ \nabla_{\Param'} \log\big( \pi_{\Param}(\Ui,\Bi)\big) \, \pi_{\Param}(\Ui,\Bi) \, R(\Ui,\Bi)}_{\color{red} =0} \nonumber \\
				    &= \sum_{i \in \mathbb{T}} \nabla_{\Param'} \log\big( \pi_{\Param'}(\Ui,\Bi)\big) \, \pi_{\Param}(\Ui,\Bi) \, R(\Ui,\Bi) \nonumber \\
				    &= \mathbb{E}_{\pi_{\Param}(\U,\B)} \Big\{ \nabla_{\Param'} \log\big( \pi_{\Param'}(\Ui,\Bi)\big)\, R(\Ui,\Bi) \Big\}
\end{align}

\begin{align}
 \nabla_{\Param'}  \log( J(\Param') ) &= \mathbb{E}_{\pi_{\Param}(\U,\B)}\Big\{R(\Bi,\Ui)  \sum_{t=0}^{T}\nabla_{\Param'}\log \pi_{\Param'}(\Ui,\Bi)\Big\}\\
				    &= \sum\limits_{i=1}^{N} \sum\limits_{t=0}^{T^{[i]}} \nabla_{\Param'}\log \pi_{\Param'}(\U^{[i]}_t,\B^{[i]}_t) \, Q^{\pi}(\U^{[i]}_t,\B^{[i]}_t) \label{eq:grad_log_cost_2}
\end{align}

The reader is referred to \cite{rl_gradient_survey_2013} for more details regarding Expectation-Maximisation and policy search in reinforcement learning.

\subsection{Q-EM for GMM derivation}\label{app:grad}

Making the substitution $\X = (\U,\B)^{\mathrm{T}}$ (small abuse of the notation) and  insuring a positive Q-function, $Q^{\pi}(\X^{[m]}) \geq 0$
and by setting the derivative of Equation \ref{eq:grad_log_cost_2} to zero and solving for the parameters
$\Param=\{w,\boldsymbol{\mu},\boldsymbol{\Sigma}\}$ we get a new weighted Maximisation update step in EM:

\begin{align}
\nabla_{\MuK} \log J(\Param) =& \sum\limits_{m=1}^{M} \alpha(z_{mk})\, Q(\Xm)\, \invSigK (\Xm - \MuK) = 0 \nonumber \\
			 \MuK_{\textrm{\textbf{new}}} =& \frac{\sum\limits_{m=1}^{M} \alpha(z_{mk})\, Q(\Xm)\, \Xm }{\sum\limits_{j=1}^{M} \alpha(z_{jk})\, Q(\X^{[j]})}
\end{align}

where $\alpha(z_{mk})$ is the responsibility factor, denoting the probability that data point $m$ is a member of the 
Gaussian function $k$.

\begin{equation}
 \alpha(z_{mk}) = \frac{w^{[k]} \cdot g(\Xm;\MuK,\SigK)}{\sum\limits_{j=1}^{K}w^{[j]} \cdot g(\Xm;\boldsymbol{\mu}^{[j]},\boldsymbol{\Sigma}^{[j]})}
\end{equation}

\begin{equation}
   \SigK_{\textrm{\textbf{new}}} = \frac{\sum\limits_{m=1}^{M} Q(\Xm) \alpha(z_{mk}) (\Xm - \MuK)(\Xm - \MuK)^{\mathrm{T}} }{ \sum\limits_{j=1}^{M} Q(\X^{[j]})\, \alpha(z_{jk}) }
\end{equation}

\begin{equation}
  w^{[k]}_{\textrm{\textbf{new}}} = \frac{\sum\limits_{m=1}^{M} Q(\X^{[m]})\, \alpha(mk)}{\sum\limits_{j=1}^{M} Q(\X^{[j]})}
\end{equation}

%\subsection{M-step}\label{app:Q-EM}
%Given a set of points $\X = (\U,\B)^{\mathrm{T}}$ and  a positive Q-function, $Q^{\pi}(\X^{[m]}) \geq 0$
%and by setting the derivative of Equation \ref{eq:grad_log_cost} to zero and solving for the parameters
%$\Param=\{w,\boldsymbol{\mu},\boldsymbol{\Sigma}\}$ we get a new weighted Maximisation updates in EM:
%\begin{align}
% w^{[k]}_{\textrm{\textbf{new}}} &= \frac{\sum\limits_{m=1}^{M} Q^{\pi}(\X^{[m]})\, \alpha(z_{mk})}{\sum\limits_{j=1}^{M} Q^{\pi}(\X^{[j]})} \\
% \MuK_{\textrm{\textbf{new}}}     &= \frac{\sum\limits_{m=1}^{M} \alpha(z_{mk})\, Q^{\pi}(\Xm)\, \Xm }{\sum\limits_{j=1}^{M} \alpha(z_{jk})\, Q^{\pi}(\X^{[j]})} \\
% \SigK_{\textrm{\textbf{new}}}    &= \frac{\sum\limits_{m=1}^{M} Q^{\pi}(\Xm)\, \alpha(z_{mk})\, (\Xm - \MuK)(\Xm - \MuK)^{\mathrm{T}} }{ \sum\limits_{j=1}^{M} Q^{\pi}(\X^{[j]})\, \alpha(z_{jk}) }
%\end{align}
%$\alpha(z_{mk})$ is the responsibility factor, denoting the probability that data point $m$ is a member of the 
%Gaussian function $k$ (see \cite[Chap. 9]{Bishop_2006}).

\subsection{Unbiased estimator}\label{app:unbiased_delta}

%\begin{equation}
%  \delta^{\pi}_t = r_{t+1} + \gamma V^{\pi}(b_{t+1}) - V^{\pi}(b_t) \\
%\end{equation}
The temporal difference error is an unbiased estimate of the advantage function:
\begin{align}
  \displaystyle \mathop \mathbb{E}_{\pi_{\Param}} \{ \delta^{\pi}_t|b_t,u_t\} &=  \mathop\mathbb{E}_{\pi_{\Param}}\{ r_{t+1} + \gamma V^{\pi}(b_{t+1})|b_t,u_t\} - V^{\pi}(b_t) \nonumber \\
									   &= Q^{\pi}(b_t,u_t) - V^{\pi}(b_t) \nonumber \\
									   &= A^{\pi}(b_t,u_t)
\end{align}

\chapter{Non-parametric Bayesian State Space Estimator}

% In this chapter
In both Chapters 3 and 4, we demonstrated that it is feasible to learn a POMDP policy from human teachers. Further,
by adding a simple binary reward function we were able to take into consideration the quality of these demonstrations 
provided by the teachers. With this, we showed that our Reinforcement Learning extension, RL-PbD-POMDP was able to 
yield improved policies even when provided with a limited number of demonstrations taken from the worst teachers.

Both tasks from the previous two chapters (search for wooden block on a table and peg-in-hole) fall into 
the category of goal oriented \textit{active-localisation}. In general, the localisation problem consists of estimating 
position parameters given noisy observations whereas active-localisation refers to a policy which actively takes actions to 
acquire information to decrease the uncertainty of the position estimate. In localisation, the model 
of the world also known as the map is considered prior knowledge. This assumption constrains localisation 
to an environment in which schematics exist and can be used 
as the world model such as in the case of offices and buildings. If the map is not known a priori, then Simultaneous Localisation And Mapping (SLAM) 
algorithms have to be used instead of localisation, known as \textit{active-}SLAM. Typically, the map consists of a set of features also known as landmarks which can be identified by sensors, 
and SLAM algorithms maintain a filtered joint probability distribution over both the agent's and features' position which is updated in accordance with a generic 
Bayesian State Space Filter (BSSF) (see Figure \ref{fig:baysian_filter} on page \pageref{fig:baysian_filter}).

In this chapter, we consider an agent tasked with searching for a set of objects on a \textit{Table} world (see Figure \ref{fig:Figure1}), 
in which exteroceptive feedback is extremely limited. The agent can only sense an object after making physical 
contact with it (bumping into it). The agent's uncertainty of its location and that of the object are encoded by probability distributions $P(\cdot)$, which 
at initialisation are known as the agent's prior beliefs. Figure \ref{fig:Figure1} illustrates a particular instance of the agent's beliefs. The agent is currently located in the 
bottom  table and has only a limited knowledge of its location, somewhere near the right edge of the table. 

\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{./ch5-MLMF/Figures/Figure1_v2.pdf}
  \caption{ \textit{Table Environment} Table World (delimited by the black rectangle), viewed from above, and the agent's beliefs. 
  There are three different probability density functions present on the table. 
  The blue represents the believed location of the agent, the red and green probability distributions are associated with object 1 and 2.
  The white shapes in each figure represent the true location of each associated object or agent.}
  \label{fig:Figure1}
\end{figure}
\vspace*{0.6cm}



%	2) Current draw back of all SLAM methodologies 
As the agent explores the world, it integrates all sensing information at each time step and updates its prior beliefs to posteriors
(the result of the prior belief after integrating motion and sensory information).
In the search task illustrated in Figure \ref{fig:Figure1}, the beliefs and sparse measurement information (haptic) available to the agent are 
the source of the uncertainty which is, the absence of positive object measurements. In this setting as the sensory information is strictly haptic, 
we can confidently assume no measurement noise. 
This is known as \textit{negative information} \citep[p.313]{Thrun_Burgard_Fox_2005} \citep{Thrun02particlefilters,negative_info_markov_localisation}. 
To the authors knoweldge, current SLAM methods are limited to non-negative information in that they consider only uncertainty induced by sensing inaccuracy inherent in 
the sensor and motion models (see the three main paradigms of SLAM \cite[Chap. 10-13]{Thrun_Burgard_Fox_2005}). 
Thus SLAM methodologies which use the \textit{Gaussian error} between the predicted and estimated position of features, such as in the case 
of EKF-SLAM and Graph-SLAM, will not perform well in this setting. 

{\quote \textit{The EKF SLAM algorithm, [...], can only process positive sightings of landmarks. It cannot process negative information
that arises from the absence of landmarks. } -Probabilistic Robotics \cite[p.313]{Thrun_Burgard_Fox_2005}-}\\[0.01cm]

In addition to the negative sensing information, the original beliefs depicted in Figure \ref{fig:Figure1} are \textit{non-Gaussian}
and \textit{multi-modal}. We make \textbf{no assumption} regarding the form of the beliefs. This implies that the joint distribution 
can no longer be parameterised by a Multivariate Gaussian. 
This is a necessary assumption for EKF-SLAM and its derived methods which allows for a closed form solution to the state estimation problem. Without the 
Gaussian assumption EKF-SLAM methods have no closed form solution to the filtering problem. 
Using standard non-parametric methods such as Kernel Density, Gaussian Process, Histogram to represent or estimate the joint distribution becomes
unrealistic after a few dimensions or additional map features. 
FastSLAM could be a potential candidate, however as it parameterises the position uncertainty of the agent by a particle filter and each
particle has its own copy of the map, the memory demands become quickly significant.  For planning purposes we would also want to have a 
single representation of the marginals. Figure \ref{fig:ch5_assmuptions} summarises the desirable attributes and assumptions for our filter.

\begin{figure}
\centering
\begin{tikzpicture}    
\node [white_box] (box){%
\begin{minipage}{0.95\textwidth}
\begin{itemize}
  \item Non-Gaussian joint distribution, no assumptions are made with respect to its form.
  \item Mostly negative information available (absence of positive sightings of the landmarks).
  \item Joint distribution volume grows exponentially with respect to the number of objects and states.
  \item Joint distribution volume is dense, there is high uncertainty.
\end{itemize}
\end{minipage}

};
\node[fancytitle, right=10pt] at (box.north west) {Attributes \& Assumptions};
\end{tikzpicture}%
\caption{Assumptions and attributes which have to be fulfilled by our Bayesian State Space Filter. }
 \label{fig:ch5_assmuptions}
\end{figure}

\textit{The main contribution of our work and the importance to the field of Artificial Intelligence:} 

An accurate estimate of the agent's belief space is a necessary precondition before planning or reasoning can be carried out.
In a wide range of Artificial Intelligence (AI) applications the agent's beliefs are discrete. This non-parametric representation
is the most unconstraining but comes at a cost. The parameterisation of the belief's joint distribution grows at an exponential rate
(see section \ref{ch5:space_time_complexity_MLMF} on page \pageref{ch5:space_time_complexity_MLMF} for a details).
We propose a Bayesian state estimator which delivers the same filtered beliefs as a traditional filter but without explicitly parametrising the 
joint distribution. Through memorising all the parameters of the measurement likelihood functions and by taking advantage of 
their sparsity (only a few states in the joint distribution are changed at any given time), we achieve a filter which grows linearly as opposed to exponentially 
in both time and space complexity. We refer to our novel filter as the Measurement Likelihood Memory Filter (MLMF). 
It keeps track of the history of measurement likelihood functions, referred to as the memory, which 
have been applied on the joint distribution.
The MLMF filter efficiently processes negative information. To the best of the author's knowledge there has not been any research on 
negative information in an \textit{active}-SLAM setting. By contrast there has been work considered with negative information in \textit{active-}localisation (only agents position is unknown, map is known)\citep{NegInfoFurtherStudies}.
The incorporation of negative information is useful in many contexts especially in Bayesian Theory of Mind \citep{Bake_Saxe_Tene_2011}
where the reasoning process of a human is inferred from a Bayesian Network and in our own work \cite{deChambrier2013} where we model the 
search behaviour of an intentionally blinded human. In such a setting much negative information is present and an efficient belief filter is required. 
Our MLMF is thus applicable to the SLAM and AI community in general and to the cognitive community which models human or agent behaviours through 
the usage of Bayesian state estimators.

By using this new representation we implement a set of passive search trajectories through the state 
space and demonstrate, for a discretised state space, that our novel filter is optimal with respect to the Bayesian criteria (the successive
filtered posteriors are exact and not an approximation with respect to Bayes rule). We provide an analysis of the space and time complexity of 
our algorithm and prove its efficiency even in the worst case scenario.
Lastly we consider an Active-SLAM setting and evaluate how constraining the size of the number of memorised likelihood 
functions impacts the decision making process of a greedy one-step look-ahead planner.

\section{Outline}

% \hyperref[ch3:background]{\ref{ch3:background}   Background}

The remaining part of this chapter is structured as follows:


\begin{itemize}
 \item \hyperref[ch5:background]{\ref{ch5:background} Background}: Review of three prominent SLAM algorithms
 and their assumptions and an overview of active-localisation and exploration methods used with SLAM.
 \item \hyperref[ch5:BSSE]{\ref{ch5:BSSE} Bayesian State Space Estimation}:  Introduction to EKF-SLAM and
 its unsuitability when mostly negative information is available. Description 
 of the Histogram-SLAM algorithm and the assumptions which can be exploited.
 \item \hyperref[ch5:MLMF]{\ref{ch5:MLMF} Measurement Likelihood Memory Filter}:
 Mathematical derivation of the MLMF, time and space complexity evaluation and extension to 
 the scalable-MLMF.
 \item \hyperref[ch5:evaluation]{\ref{ch5:evaluation} Evaluation}:
 We numerically evaluate the time complexity of the scalable-MLMF and verify its assumptions.
 We investigate the filter's sensitivity with respect to its parameters in an Active-SLAM setting.
 \item \hyperref[ch5:conclusion]{\ref{ch5:conclusion} Conclusion}
\end{itemize}

\section{Background}\label{ch5:background}

\subsection{SLAM}

% Introduction of SLAM
Estimating the location or state parameters of a mobile agent whilst simultaneously building a map of the environment has been
regarded as one of the most important problems to be solved for agents to achieve true autonomy. It is a necessary precondition for 
any agent to have an estimation of the world at its disposal which accurately encompasses all knowledge and related uncertainties. 
There has been much research surrounding the field of Simultaneous Localisation And Mapping (SLAM) which branches out into a wide variety of sub-fields 
dealing with problems from building accurate noise models of the agent sensors \citep{Plagemann07gaussianbeam} to determining which environmental 
feature caused a particular measurement, also known as the data association problem \citep{DataAssociation2003} and many more. 

% Why does SLAM Work

Although the amount of research might seem overwhelming at first view, all current SLAM methodologies are founded on a single principle; 
the uncertainty of the map is correlated through the agent's measurements. When an agent localises itself (by reducing position uncertainty)
all previously located landmarks have their uncertainty reduced since the uncertainty is correlated with that of the agent's uncertainty.

% The three main pillar of SLAM algorithm and their respective draw backs

There are three main paradigms to solving the SLAM problem. The first is EKF-SLAM (Extendend-Kalman Filter) \cite{SLAM_part1}.
EKF-SLAM models the full state, being the agent's position parameters and environmental features, by a Multivariate Gaussian distribution. 
The uncertainty of each individual feature is parametrised by a mean (expected position of the feature) and covariance 
(the level of uncertainty of the position of the feature).

The second approach is Graph-SLAM, \cite{TutGraphSLAM}. Graph-SLAM estimates the full path of the agent and considers every measurement to 
be a constraint on the agent's path. It is parameterised by the canonical Multivariate Gaussian. At each time step a new row and column 
is added to the precision matrix which encodes landmarks which have been observed as constraints on the robot's position.
At predetermined times, a nonlinear sparse optimisation is solved to minimise all the accumulated constraints on the robot's path.

The third method is FastSLAM, \cite{FastSLAM}. FastSLAM exploits the fact that if we know the agent's position with 
certainty all landmarks become independent. It models the distribution of the agent's position by a particle filter. Each particle
has its own copy of the map and updates all landmarks independently which is the strength of this method. 
However, if many particles are required each must have its own copy of the map. 
It is beyond the scope of this chapter to provide a detailed review of these  three paradigms and the reader is referred 
to \cite{Thrun_Burgard_Fox_2005}, \cite{SLAM_HBR}.

\subsection{Active-SLAM \& Exploration}

Active-SLAM refers to a decision theoretic process of choosing control actions so as to actively 
increase the convergence of the map. It is used in conjunction with exploration of an unknown environment
in a SLAM setting. The two steps of this process are: (i) generate a set of 
candidate destination positions, (ii) evaluate these positions based on a utility function. The utility  
is a trade off between reducing the uncertainty of the map or reducing the uncertainty
of the agent's position.

Most approaches use a two-level representation of the map in an exploration setting. At the lower level
there is the chosen (landmark-based) SLAM filter and at the higher level a coarser representation of the world.
Such representations can be occupancy grids \citep{Thrun_grid_based_1996} which encode either occupied and free space
or a topological representation, \cite{Kollar_2008_Exploration_SLAM}.

Early and current approaches to selecting candidate exploratory locations are based on evaluating 
\textit{Next-best-view} locations, \cite{Navigation_strategires_for_exploring_indoor_environments}. Next-best-view points are 
sampled around \textit{free edges} which are at the horizon of the known map (\textit{frontier} regions). 
In such a setting only target points are generated, not the full trajectory. Probabilistic Road Map (PRM) \citep{PRM_1996}
methods have been used as planners to reach desired target locations, such as in \cite{RRT-SLAM}, where a Rapidly
Exploring Random Trees (RRT) is combined with FastSLAM. In \cite{ActivePosSLAM}, paths to \textit{frontier} regions are computed
via PRM  on a occupancy grid map and at the lower level they use Pose-SLAM (synonym for Graph-SLAM).

An alternative approach taken to generating candidate locations is the specification of high level macro actions, they being either 
\textit{exploratory} or \textit{revisiting} actions as is the case in \cite{stachniss05robotics}. Macro actions
reduce the costly evaluation of actions, especially in the case of FastSLAM, which requires propagating the filter 
forward in time so as to infer the information gain of each action.

The last approach is to solve the planning problem through formulating it as  Partially Observable Markov Decision Process (POMDP) \citep{Ross08onlineplanning}. 
However all methods take an approximation of the POMDP and consider a one time step planning horizon \cite[p.37]{GeorgiosLidoris}.

There are many ways of generating actions or paths, however their utility is nearly all exclusively based on the \textit{information gain}, 
which is the estimated reduction of entropy a particular action or path would achieve. A few utilities use f-measures such as the Kullback-Leibler divergence. 
Evaluation of different utility metrics are presented in \cite{Active_SLAM_Uncertainty_compar}.



\section{Bayesian State Space Estimation}\label{ch5:BSSE}

Bayesian State Space Estimation (BSSE) focuses on incorporating observations to update a prior distribution to a posterior distribution 
over the state space through the application of Bayes probability rules. The agent's random variable, $A$, 
is associated with the uncertainty of its location in the world. The same holds for the object(s') random variable(s), $O$. 
Given a sequence of actions and observations, $\{u_{1:t},Y_{0:t}\}$ (subscript $0:t$ is all the indexed variables from $t=0$ to the current time $t=t$), 
algorithms of the BSSE family incorporate this information to provide an estimate $P(A_t,O|Y_{0:t},u_{1:t})$. This is known as the 
filtering problem where all past information is incorporated to estimate the current state.  

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./ch5-MLMF/Figures/explenation/Figure2.pdf}
\caption{Directed graphical model of dependencies between the agent(A) and object(O)'s estimated location. Each 
object, $O^{(i)}$ is associated with one sensing random variable $Y^{(i)}$. The overall sensing random variable is $Y = \left[Y^{(1)},\dots,Y^{(M-1)}\right]^{\mathrm{T}}$,
where $M$ is the total number of agent and object random variables in the filter. 
For readability we have left out the time index $t$ from $A$ and $Y$. Since the objects are static, they have no temporal process associated with 
them thus they will never have a time subscript. The two models necessary for filtering are the motion model $P(A_t|A_{t-1},u_t)$ (red) and measurement model
$P(Y_t|A_t,O)$ (blue).}
\label{fig:bayesian_sse_dag}
\end{figure}

In Figure \ref{fig:bayesian_sse_dag} we depict the general Bayesian Network (BN) of a BSSE. The BN conveys two types of
information, the dependence and independence relation between the random variables in the graph which can be established
through \textit{d-separation} \cite{BayesBall}. The \textbf{conditional dependence} $A \dependent O | Y$ is key to all BSSE and SLAM algorithms. 
The strength of the dependence between the agent and object random variable is governed by the measurement likelihood $P(Y_t|A_t,O)$. 
If the measurement likelihood does not change the joint distribution, then the agent and object random variables will be independent, $A \independent O$. 
If they are independent, then no information acquired by the agent can be used to infer changes in the object estimates.
    
We next demonstrate the behaviour of the BN joint distribution, Figure \ref{fig:bayesian_sse_dag}, for two different parameterisations 
in the case of the absence of direct sighting of the object by the agent.

\subsubsection{EKF-SLAM}\label{sec:EKF-SLAM}

In EKF-SLAM the joint density $p(A_{t},O|Y_{0:t},u_{1:t}) = g([A_t,O]^{\mathrm{T}};\mu_t,\Sigma_t)$ is parametrised by a single Gaussian 
function $g(\cdot)$ with mean, ${\mu_t = \left[\mu_{A_{t}},\mu_{O^{(1)}},\dots,\mu_{O^{(M-1)}}\right]^{\mathrm{T}}}\in \mathbb{R}^{3 + 2\cdot (M-1)}$  where the 
object random variables are in $\mathbb{R}^2$, and covariance, $\Sigma_t$. The mean value of the agent 
$\mu_a = [x,y,\phi]^{\mathrm{T}} \in \mathbb{R}^3$ and those of the objects are $\mu_{O^{(i)}} = [x,y]^{\mathrm{T}} \in \mathbb{R}^2$.

\begin{equation}
\Sigma_t = \begin{bmatrix}
       \Sigma_a & \Sigma_{ao}  \\[0.3em]
       \Sigma_{oa} & \Sigma_o
     \end{bmatrix}
     \in \mathbb{R}^{(3 + 2\cdot (M-1)) \times (3 + 2\cdot (M-1))}
\end{equation}

The $j$'th object measurement is described by range and bearing  $Y^{(j)}_t = [r,\phi]$ in the frame of reference of the agent.
EKF-SLAM assumes that the measurement is corrupted by Gaussian noise, $\epsilon \sim \mathcal{N}(0,R)$,
resulting in the likelihood function:
\begin{align} 
   p(Y_t|A_t,O_t) &= \frac{1}{|2\pi R|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} \big(Y_t - \hat{Y}_t\big)^{\mathrm{T}}R^{-1}\big(Y_t - \hat{Y}_t\big) \right)\label{eq:lik-measurement}\\
   \hat{Y}_t      &= \exp\left(-\frac{1}{2\sigma^2} ||A_t - O ||^2 \right)\label{eq:measurement_ekf}
\end{align}
where the covariance, $R$, encompasses the uncertainty in the measurement and Equation \ref{eq:measurement_ekf} is the measurement function. The elements of the covariance matrix capture 
the measurement error between the true $Y$ and expected $\hat{Y}$ range and bearing of the object. As the joint distribution 
is parametrised by a single Multivariate Gaussian, a closed form solution to the filtering Equations exists, called the Kalman 
Filter \cite{SLAM_part1}. 

The error between the true and expected measurement $e = (Y_t - \hat{Y}_t)$ is an important part of the application of EKF-SLAM.
In our scenario the agent can only perceive the objects once he enters in direct contact with them. 
This means that the variance of the observation $Y_t$ will always be equal to $\hat{Y}$ until a contact occurs. 
To illustrate the problems which this gives rise to, we give an illustration of a 1D search. Figure \ref{fig:EKF-SLAM} shows the 
resulting updates of the beliefs for 4 chosen time segments.

\begin{figure}
\centering
 \includegraphics[width=\textwidth]{./ch5-MLMF/Figures/explenation/Figure3.pdf}
\caption{\textbf{a)} EKF-SLAM time slice evolutions of the pdfs. 
The temporal ordering is given by the numbers in the top right corner of each plot.
The blue pdf represents the agent's believed location and the circle is the agent's true location. The same holds 
for the red distribution which represents the agent's belief of the location of an object.
\textbf{b)} Evolution of the covariance components of $\Sigma$ over time and true $Y_t$ and expected measurements,  $\hat{Y}_t$. 
$\Sigma_a$ and $\Sigma_o$ are the variances of the agent and object positions and $\Sigma_{ao}$ is the cross-covariance 
term.}
\label{fig:EKF-SLAM}
\end{figure}

As expected we do not get the desired behaviour, that the beliefs start updating as soon as they are overlapping, 
see 2nd-3rd temporal snapshot in the Figure. 
Even when most of the belief mass of the agent's location pdf overlaps that of the object pdf, no belief update occurs. 
The multivariate Gaussian parameterisation only guarantees a dependency between the agent and object random variables 
when there is a positive sighting of the landmarks.  This can been seen in Figure \ref{fig:EKF-SLAM} (b),
where the component $\Sigma_{ao}$ is 0 most of the time which implies that $A \independent O | Y$ which is undesirable. 

\subsubsection{Histogram-SLAM}\label{sec:Discrete}

In Histogram-SLAM, the joint distribution is discretized and each bin has a parameter, 
${P(A_t=i,O=j|Y_{0:t},u_{1:t};\boldsymbol{\theta}) = \boldsymbol{\theta}^{(ij)}}$, which sums to one, ${\sum_{ij} \boldsymbol{\theta}^{(ij)} = 1}$. 
For shorthand notation we will write $P(A_t,O|Y_{0:t},u_{1:t})$ instead of $P(A_t=i,O=j|Y_{0:t},u_{1:t};\boldsymbol{\theta})$.
The probability distribution of the agent's position is given by marginalising the object random variable:
\begin{equation}
 P(A_t|Y_{0:t},u_{1:t};\ThA)    = \sum\limits_{j=1}^{|O|} P(A_t,O=j|Y_{0:t},u_{1:t};\boldsymbol{\theta}) \label{eq:agent_marginal}
\end{equation}

% For ease of notation we use the shorthand $P(A_t)$ for $P(A_t|Y_{0:t},u_{1:t};\ThA)$. 
The converse holds true for the object's marginal, that is the summation would be over 
the agents variable. Figure \ref{fig:histogram_joint} (\textit{Top}) illustrates the joint distribution of both the agent and the object random variable. 
The 1D world of the agent and object is discretised to 10 states, producing a joint distribution with 100 parameters!
For a state space of $N$ bins, $s=1...N$, and there is a total of $M$ random variables (one agent and $M-1$ objects)
and the joint distribution has $N^{M}$ parameters. This exponential increase renders Histogram-SLAM intractable
with this parameterisation.

\begin{figure}
 \centering
 \includegraphics[width=\linewidth]{./ch5-MLMF/Figures/explenation/Figure4.pdf}
 \caption{\textbf{Top}: \textit{Left:} Initialisation of the agent and object joint distribution. 
 \textit{Right:} Marginals of the agent and object parameterised by $\ThA$ and $\ThO$, giving the probability of their location. The marginal of each 
 random variable is obtained from Equation \ref{eq:agent_marginal}. The probability of
 the agent and object being in state $s=6$ is given by summing the blue and red highlighted parameters in the joint distribution. 
 \textbf{Bottom}: 1D world Likelihood $P(Y_t|A_t,O)$, the white regions $A \cap O$ will leave the joint distribution unchanged whilst
 the black regions will evaluate the joint distribution to zero. \textit{Left:} No contact detected with the object, the current measurement 
 is $Y_t = \textcolor{red}0$, both the agent and object cannot be in the same state. \textit{Right:} The agent 
 entered into contact with the object and received a haptic feedback $Y_t = \textcolor{red}1$. The agent receives 
 only two measurement possibilities, contact or no contact.
 }
 \label{fig:histogram_joint}
\end{figure}

In the tasks we consider, an observation occurs only if the agent enters in contact with the object, which implies that both
occupy the same discrete state. The likelihood function $P(Y_t|A_t,O)$ is:

\begin{equation} \label{eq:discrete_likelihoood}
P(Y_t=1|A_t,O) =
  \begin{cases}
    1       & \quad \text{if } A_t = O     \\
    0  	    & \quad \text{if } A_t \not= O \\
  \end{cases}
\end{equation}

Figure \ref{fig:histogram_joint} (\textit{Bottom left}), illustrates the likelihood function, Equation \ref{eq:discrete_likelihoood}, 
in the case of a no contact measurement $Y_t=0$. When there is no measurement all the parameters of the 
joint distribution which are in the black regions become zero, which we refer to as the \textbf{dependent states} $A \cap O$ of the joint 
distribution. The white states are the \textbf{independent states} $A \ominus O$, they are not changed by the likelihood function 
and the values of the joint distribution in those states, $P_{\cap}(A_t,O|Y_{0:t},u_{1:t})$, will be unchanged by the likelihood function
$P_{\ominus}(A_t,O|Y_{\mathbf{0:t}},u_{1:t}) \propto P_{\ominus}(A_t,O|Y_ {\mathbf{0:t-1}},u_{1:t})$. 
When the object is detected (\textit{Bottom right}) the likelihood constrains all non-zero values of the joint 
distribution to be in states $i = j$, which in the case of a 2-dimensional joint 
distribution is a line. The \textbf{sparsity} of the likelihood function  will be key to the development of the MLMF filter.
Two models are needed to perform the recursion, namely the motion model $P(A_t|A_{t-1},u_t)$ and the measurement model
$P(Y_t|A_t,O)$, which we already detailed. Both models applied consecutively to the initial joint distribution results in a posterior
distribution. Both Equation \ref{eq:disc_motion}-\ref{eq:disc_measurement} are part of the Histogram Bayesian filter 
update:
\begin{center}
\begin{tikzpicture}    
\node [white_box] (box){%
\begin{minipage}{0.8\textwidth}
\vspace*{-1cm}
\begin{align}
 &\mathrm{\textbf{intialisation}}\nonumber\\
 &P(A_0,O;\boldsymbol{\theta}) = P(A_0;\ThA)\, P(O;\ThO) = \ThA \times \ThO \label{eq:ch5:disc_prod_AO}\\
 &\mathrm{\textbf{motion}}\nonumber\\
 &P(A_t,O|Y_{0:t-1},u_{1:t}) = \sum_{A_{t-1}} P(A_t|A_{t-1},u_t)\, P(A_{t-1},O_t|Y_{0:t-1},u_{1:t-1} \label{eq:disc_motion})\\
 &\mathrm{\textbf{measurement}}\nonumber\\
 & P(A_t,O|Y_{0:t},u_{1:t}) = \frac{P(Y_t|A_t,O)\, P(A_t,O|Y_{0:t-1},u_{1:t}) }{P(Y_t|Y_{0:t-1},u_{1:t})} \label{eq:disc_measurement} 
\end{align}
\end{minipage}
};
\node[fancytitle, right=10pt] at (box.north west) {Histogram Bayesian recursion};
\end{tikzpicture}%
\end{center}

Figure \ref{fig:discrete_example} illustrates the evolution of the joint distribution in a 1D example. 
The agent and object's true positions (unobservable) are in state 6 and 1. The agent moves three steps towards state 10. At each time 
step, as the agent fails to sense the object, the likelihood function  $P(Y_t=0|A_t,O)$ (Figure \ref{fig:histogram_joint}, \textit{Bottom left})
is applied. As the agent moves towards the right, the motion model shifts the joint distribution towards state 10 along the agent's 
dimension, $i$ (note that state 1 and 10 are wrapped).

\begin{figure}
 \centering
  \includegraphics[width=\linewidth]{./ch5-MLMF/Figures/explenation/Figure5.pdf}
  \caption{Histogram-SLAM, 3 time steps. \textbf{1} Application of likelihood $P(Y_0=0|A_0,O)$ and the agent remains stationary, all states along the green line become zero.
  \textbf{2} The agent moves to the right $u_1=1$, the motion $P(A_1|A_0,u_1)$, and likelihood models are applied consecutively. The right motion results in a shift (black arrow on the left) in the joint probability 
  distribution towards the state $i=10$. All parameters on the pink line are zero. \textbf{3} Same as two. At each time step a new likelihood function (pink line) is applied to the joint distribution.}
  \label{fig:discrete_example}
\end{figure}

As the agent moves to the right more joint distribution parameters become zero. The re-normalisation by the \textbf{evidence} ($P(Y_t|Y_{0:t-1},u_{1:t})$, denominator of Equation \ref{eq:disc_measurement}), 
which increases the value of the remaining parameters, is equal to the sum of the probability mass which was set to zero by the likelihood function.
Thus the values of the parameters of the joint distribution which fall on the pink line in Figure \ref{fig:discrete_example} 
(green line also, but only for first time slice) become zero and their values are redistributed to the remaining non-zero parameters. 

The \textbf{inconvenience} with Histogram-SLAM is that its time and space complexity is exponential as the joint distribution is discretised and 
parametrised by $\boldsymbol{\theta}^{(ij)}$. Instead we propose a new filter, MLMF, which we formally introduce in the next section. This filter
achieves the same result as the Histogram filter but without having to parameterise the values of the joint distribution, thus avoiding the exponential growth cost. 

The \textbf{key idea} behind the mechanism of the MLMF filter is to evaluate only the joint distribution $P_{\cap}(A_t,O|Y_{0:t},u_{1:t})$ in
dependent states and updates directly the marginals without marginalising the entire joint state space.
The MLMF filter parametrises \textbf{explicitly} the marginals ${P(A_t|Y_{0:t},u_{1:t};\ThA)}$, ${P(O|Y_{0:t},u_{1:t};\ThO)}$. 
This contrasts the Histogram filter where the marginals are derived from the joint distribution by marginalisation over the entire joint state space. 


\FloatBarrier
\section{Measurement Likelihood Memory Filter}\label{ch5:MLMF}

MLMF keeps a  \textbf{function parameterisation} of the joint distribution instead of a \textbf{value parameterisation} as it is the case 
for Histogram-SLAM. At initialisation the joint distribution is represented by the product of marginals, Equation \ref{eq:ch5:prod_AO}, which 
would result in the joint distribution illustrated in Figure \ref{fig:histogram_joint}, if it were to be evaluated at all states $(i,j)$
as it is done for Histogram-SLAM. MLMF will only evaluate this product, when necessary, at specific states. 
At each time step the motion and measurement update are applied, Equation \ref{eq:ch5:mlmf_motion_update}-\ref{eq:ch5:mlmf_measurement_update}.
An important distinction is that these updates are performed on the \textbf{un-normalised} joint distribution ${P(A_t,O,Y_{0:t}|u_{1:t})}$, which is not the case in Histogram-SLAM where 
the updates are done on the conditional ${P(A_t,O|Y_{0:t},u_{1:t}})$. After applying multiple 
motion and measurement updates the resulting joint distribution is given by Equation \ref{eq:ch5:mlmf_filter_conditional}, see Appendix \ref{appendix:recursion_example}
for a step-by-step derivation. 

\begin{center}
\begin{tikzpicture}    
\node [white_box] (box){%
\begin{minipage}{\textwidth}
\vspace*{-1cm}
\begin{align}
 &\mathrm{\textbf{joint marginals (initial)}}\nonumber\\
 &P(A_0,O) = P(A_0;\mathcolor{blue}{\ThAs})\, P(O;\mathcolor{red}{\ThOs}) \label{eq:ch5:prod_AO}\\
 &\mathrm{\textbf{motion}}\nonumber\\
 &P(A_t,O,Y_{0:t-1}|u_{1:t}) = \sum_{A_{t-1}} P(A_t|A_{t-1},u_t)\, P(A_{t-1},O,Y_{0:t-1}|u_{1:t-1})  \label{eq:ch5:mlmf_motion_update} \\
 &\mathrm{\textbf{measurement}}\nonumber\\
 &P(A_t,O,Y_{0:t}|u_{1:t};\mathcolor{red}{\ThOs},\mathcolor{blue}{\ThAs},\mathcolor{dark-green}{\boldsymbol{\Psi_{0:t}}}) = \nonumber \\[0.2cm]
 &\hspace*{1cm} P(Y_t|A_t,O)\,P(O;\mathcolor{red}{\ThOs})\, P(A_t|u_{1:t};\mathcolor{blue}{\ThAs})\, P(Y_{0:t}|A_t,O,u_{1:t};\mathcolor{dark-green}{\boldsymbol{\bar{\Psi}_{0:t}}})  \label{eq:ch5:mlmf_measurement_update} \\
 &\mathrm{\textbf{joint}}\nonumber\\
 &P(A_t,O|Y_{0:t},u_{1:t};\mathcolor{red}{\ThOs},\mathcolor{blue}{\ThAs},\mathcolor{dark-green}{\boldsymbol{\Psi_{0:t}}},\mathcolor{dark-blue}{\BAlph}) = \frac{P(A_t,O,Y_{0:t}|u_{1:t};\mathcolor{red}{\ThOs},\mathcolor{blue}{\ThAs},\mathcolor{dark-green}{\boldsymbol{\Psi_{0:t}}})}{P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph}) } \label{eq:ch5:mlmf_filter_conditional}  \\ 
 &\mathrm{\textbf{filtered marginal}}\nonumber\\
 &P(A_t|Y_{0:\mathbf{t}};\mathcolor{blue}{\ThA}) = P(A_t|Y_{0:\mathbf{t-1}};\mathcolor{blue}{\ThA}) - \Big(P_{\cap}(A_t|Y_{0:t-1}) -  P_{\cap}(A_t|Y_{0:t})  \Big)   \label{eq:marignal_mrf}  \\
 &P(O|Y_{0:\mathbf{t}};\mathcolor{red}{\ThO}) = P(O|Y_{0:\mathbf{t-1}};\mathcolor{red}{\ThO}) - \Big(P_{\cap}(A_t|Y_{0:t-1}) -  P_{\cap}(A_t|Y_{0:t})  \Big)    
 \end{align}
\end{minipage}
};
\node[fancytitle, right=10pt] at (box.north west) {MLMF Bayesian filter};
\end{tikzpicture}%
\end{center}

The MLFM filter is parameterised by the agent and object 
\textbf{joint marginals} $P(A_t|u_{1_:t};\mathcolor{blue}{\ThAs})$, $P(O;\mathcolor{red}{\ThOs})$, the \textbf{filtered marginals}  
$P(A_t|Y_{0:t},u_{1:t};\mathcolor{blue}{\ThA})$ ($u_{1:t}$ not shown in the above box), $P(O|Y_{0:\mathbf{t}};\mathcolor{red}{\ThO})$, 
the evidence $P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph})$ and the history of likelihood functions, $P(Y_{0:t}|A_t,O,u_{1:t};\mathcolor{dark-green}{\BPsi})$ Equation \ref{eq:memory}, which is 
the product of all the likelihood functions since $t=0$ until $t$ and we will refer to it as the \textbf{memory likelihood function}: 

\begin{equation}
 P(Y_{0:t}|A_t,O,u_{1:t};\mathcolor{dark-green}{\BPsi}) := \prod_{i=0}^t P(\mathcolor{dark-green}{Y_i}|A_t,O,u_{i+1:t};\mathcolor{dark-green}{l_i}) \label{eq:memory}
\end{equation}

\begin{equation} \label{eq:ch5:liklihood_v2}
P(\mathcolor{dark-green}{Y_i}=0|A_t,O,u_{i+1:t};\mathcolor{dark-green}{l_i}) :=
  \begin{cases}
    0       & \quad \text{if } A_t + \mathcolor{dark-green}{l_i} = O     \\
    1  	    & \quad \text{else}  \\
  \end{cases}
\end{equation}
\begin{equation}
  \mathcolor{dark-green}{l_i} := \sum\limits_{j=i+1}^t u_j  \label{eq:ch5:offset}
\end{equation}

The memory likelihood function's parameters $\BPsi = \{(Y_i,l_i)\}_{i=0:t}$ consist of a set of measurements $Y_{0:t}$ and offsets $l_{0:t}$
depicted in green. The measurements $Y_i \in \{0,1\}$ are always binary, whilst the offsets $l_i$, actions $u_t$, 
agent $A_t$ and object $O$ variables' size are equal to the dimension of the state space. The subscript $i$ 
of an offset $l_i$ indicates which likelihood function it belongs to. The offset of a likelihood function is given by the 
summation of all the applied actions from the time the likelihood was added until the current time $t$, Equation \ref{eq:ch5:offset}, which can be computed recursively.
The motion update, Equation \ref{eq:ch5:mlmf_motion_update}, when applied to the joint distribution results in the 
initial marginal $P(A_0;\ThAs)$ and the likelihood functions being moved along the agent's axis. In Algorithm \ref{alg:memory-motion}, we detail how an action $u_t$ and measurement $Y_t$, result in the update of
the memory likelihood's parameters from $\Psi_{0:t-t}$ to $\Psi_{0:t}$; this is an implementation of 
Equations \ref{eq:ch5:mlmf_motion_update}-\ref{eq:ch5:mlmf_measurement_update}.

\begin{center}
\begin{minipage}{.65\linewidth}

\begin{algorithm}[H]
\label{alg:memory-motion}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\Input{$\Psi_{0:t-1}$, $Y_t$, $u_t$}
\Output{$\Psi_{0:t}$}
\BlankLine
\nonl\hrulefill\\
\nonl\textbf{motion update} $\bar{\Psi}_{0:t} \gets \Psi_{0:t-1}$ \label{alg:ch5:motion_memory}\\
\For{$l_i \in \Psi_{0:t-1}$}{$l_i = l_i + u_t$}
\nonl\hrulefill\\
\nonl\textbf{measurement update}\\
$\Psi_{0:t} \gets \{\bar{\Psi}_{0:t}, (Y_t,l_t:=0)\}$ 
\caption{Memory Likelihood update}

\end{algorithm} 
\end{minipage}
\end{center}

Figure \ref{fig:maringal_joint_example_v2} illustrates the evolution of the \textbf{un-normalised} MLMF joint distribution, Equation  \ref{eq:ch5:mlmf_filter_conditional}.
For ease of notation we will omit at times the parameters of the probability functions.
Both $P(A_0;\mathcolor{blue}{\ThAs})$ and $P(O;\mathcolor{red}{\ThOs})$ were initialised as for the Histogram-SLAM example in Figure \ref{fig:discrete_example} on page \pageref{fig:discrete_example}.
Two actions $u_{1:2}=1$ are applied and three measurements $Y_{0:2} = 0$ received which are then integrated into the filter. 
Since initialisation of the joint distribution at $t=0$ until $t=2$ the object's marginal $P(O;\mathcolor{red}{\ThOs})$ remains unchanged and the agent's 
marginal $P(A_2|u_{1:2};\mathcolor{blue}{\ThAs})$ is updated by the two actions according to the motion update, see 
Figure \ref{fig:maringal_joint_example_v2} \textit{Top-right}.
The product of these two marginals (terms of Equation \ref{eq:ch5:mlmf_filter_conditional} before the memory likelihood product) results in the joint
probability distribution $P(A_2,O|u_{1:2};\mathcolor{blue}{\ThAs},\mathcolor{red}{\ThOs})$ illustrated in 
Figure \ref{fig:maringal_joint_example_v2} \textit{Middle-right}. 


\begin{figure}
 \centering
 \includegraphics[width=0.95\textwidth]{./ch5-MLMF/Figures/explenation/example_marginal.pdf}
 \caption{Un-normalised MLMF joint distribution, numerator of Equation \ref{eq:ch5:mlmf_filter_conditional}, at time $t=2$.
 Three measurements (all $Y=0$) and two actions (both $u=1$) have been integrated into the joint distribution, for simplicity we do not consider any motion noise. \textit{Left column:} The first plot
 illustrates the likelihood of the first measurement $Y_0$. We highlight the contour in light-green to indicate that it was the first applied likelihood 
 function (see the correspondence with Figure \ref{fig:discrete_example}). The first likelihood function has been moved by the 2 actions, the 
 second likelihood function has been moved by one action (the last one, $u_2=1$) and the third likelihood has had no action applied to it 
 yet. The last applied likelihood function is highlighted in pink and the product of all the likelihoods since $t=0$ until $t=3$ is depicted at the 
 bottom of the figure which is $P(Y_{0:2}|A_2,O,u_{1:2})$. \textit{Right column:} the top figure illustrates the original marginal of the 
 object $P(O;\ThOs)$, which remains unchanged, and the agent's marginal $P(A_2|u_{1:2};\ThAs)$ which has moved in accordance to the motion update function. 
 Their product would results in the joint distribution $P(A_2,O|u_{1:2};\ThAs,\ThOs)$ illustrated in the middle figure if evaluated at each state $(i,j)$. The bottom figure is the result
 of multiplying $P(A_2,O|u_{1:2};\ThAs,\ThOs)$ with  $P(Y_{0:2}|A_2,O,u_{1:2};\boldsymbol{\Psi_{0:2}})$ giving the filtered joint distribution, Equation \ref{eq:ch5:mlmf_filter_conditional}.
 }
 % Two actions have been applied to the likelihood function which results in a shift
 %of two indices along the i-axis, see Equation \ref{eq:ch5:liklihood_v2}.
 %taken and }
 \label{fig:maringal_joint_example_v2}
\end{figure}
 
In the left column of Figure \ref{fig:maringal_joint_example_v2} we illustrate how the memory likelihood term, Equation \ref{eq:memory}, 
is updated according to Algorithm \ref{alg:memory-motion}. In the \textit{Top-left}, the first likelihood function 
$P(\mathcolor{dark-green}{Y_0}|A_2,O,u_{1:2};\mathcolor{dark-green}{l_0})$ is illustrated. As two actions have been applied, Algorithm \ref{alg:memory-motion} is applied 
twice which results in a $\mathcolor{dark-green}{l_0}=2$ parameter for the first likelihood function. In the figure we highlighted the likelihood in light-green 
to indicate that it was the first added to the memory term making it convenient to compare to Figure \ref{fig:discrete_example} on page \pageref{fig:discrete_example}. As for the second 
likelihood function $P(\mathcolor{dark-green}{Y_1}|A_2,O,u_{2};\mathcolor{dark-green}{l_1})$ only one action has been applied and the third likelihood 
function $P(\mathcolor{dark-green}{Y_2}|A_2,O;\mathcolor{dark-green}{l_2=0})$ has not yet been updated by the next action. 
The parameters of the memory likelihood function, Equation \ref{eq:memory}, are: $\Psi_{0:2} = \{(0,2)_{i=0},(0,1)_{i=1},(0,0)_{i=2}\}$ and the evaluation of memory 
likelihood is depicted in the \textit{Bottom-left} of Figure \ref{fig:maringal_joint_example_v2}. 
  
The reader may have noticed that the amplitude of the values of the joint distribution illustrated in Figure \ref{fig:maringal_joint_example_v2} have not changed
when compared with Figure \ref{fig:discrete_example} on page \pageref{fig:discrete_example}. This is because we have not re-normalised the joint distribution by the evidence $P(Y_{0:t}|u_{1:t};\Alpha)$. 


Our goal is to be able to compute the marginals $P(A_t|Y_{0:t},u_{1:t};\ThA)$, $P(O|Y_{0:t};\ThO)$ of the agent and object random variables and 
evidence $P(Y_{0:t}|u_{1:t};\alpha_{0:t})$ without having to perform an expensive marginalisation over the entire space of the joint distribution 
as was the case for Histogram-SLAM.  The next section describes how to efficiently compute the evidence and the marginals.
For ease of notation we will not always show the conditioned actions $u_{1:t}$.

\subsection{Evidence and marginals}

In order to compute efficiently the marginal likelihood (also known as evidence) $P(Y_{0:t}|u_{1:t};\alpha_{0:t})$ and the filtered  marginals $P(A_t|Y_{0:t},u_{1:t};\ThA)$,
$P(O|Y_{0:t};\ThO)$ we take advantage of the fact that only a very small area 
in the joint distribution space will be affected by the measurement likelihood function at each time step.
Without loss of generality the likelihood function will only make a difference to dependent $A \cap O$ states in the joint distribution, states 
where the likelihood function is less than one. The independent states $A \ominus O$ will not be affected, where the likelihood function 
is equal to one.
Figure \ref{fig:overlap_dependence_independence} shows the relation between the measurement 
function $P(Y_t|A_t,O)$ and the joint distribution $P(A_t,O|Y_{0:t})$ for three different initialisations. 
 
 \begin{figure}
 \centering
  \includegraphics[width=\textwidth]{./ch5-MLMF/Figures/Figure7.pdf}
  \caption{
  \textbf{a)} Likelihood $P(Y_t=0|A_t,O)$, the blue area depicts the regions in which the likelihood is $<1$ 
  and the red area is where the likelihood is $=1$. If the probability mass of the joint distribution 
  is in the blue region, then the parameters of the random variables in these areas are dependent, $A \cap O$.
  Otherwise they are independent, $A \ominus O$.  
   \textbf{b)} The agent and object marginals are not overlapping and are thus completely independent. The joint distribution, 
   $P(A_t,O|Y_{0:t})$ the black rectangle,  is not intersecting with the measurement function. As a result $P_{\cap}(A_t,O|Y_{0:t})$
   is empty.
 \textbf{c)} The marginals overlap resulting in the measurement likelihood function intersecting with the joint distribution.
 The joint distribution is composed of the blue and red areas, Equation \ref{eq:joint_independent_dependent}.
 The probability mass at the intersection is removed and re-normalised to other regions which is the result of applying Bayes integration. 
 \textbf{d)} The marginals of $A$ and $O$ are completely overlapping, however only a small fraction of the probability mass 
 in the joint distribution is within the measurement function's tube.}
  \label{fig:overlap_dependence_independence}
\end{figure}

As illustrated and explained in Figure \ref{fig:overlap_dependence_independence}, the joint distribution can be decomposed in a 
dependent and independent term (Equation \ref{eq:joint_independent_dependent}). 

\begin{equation}\label{eq:joint_independent_dependent}
 P(A_t,O|Y_{0:t}) = P_{\cap}(A_t,O|Y_{0:t}) + P_{\ominus}(A_t,O|Y_{0:t})
\end{equation}

The probability mass covered by the dependent term is located within the measurement function's tube and the independent probability mass 
is located outside. This formulation will lead to large computational gain 
as the independent term is not influenced by the measurement function: 
${P_{\ominus}(A_t,O,\mathbf{Y_{0:t}}) = P_{\ominus}(A_t,O,\mathbf{Y_{0:t-1}})}$ and ${P_{\ominus}(A_t,O|\mathbf{Y_{0:t}}) \propto P_{\ominus}(A_t,O|\mathbf{Y_{0:t-1}})}$.

\subsubsection{Evidence}
The evidence of the measurement $P(Y_{0:t}|u_{1:t};\alpha_{0:t})$ is the normalisation coefficient of the joint distribution Equation \ref{eq:ch5:mlmf_filter_conditional}.
It is the amount of probability mass re-normalised to the other parameters as a result of the consecutive application of the likelihood function.
At time step $t$, the normalising factor to be added to the evidence is the difference between the probability mass located 
inside $A\cap O$ before and after the application of the measurement function $P(Y_t|A_t,O)$, 
see Equation \ref{eq:I_v1}-\ref{eq:I_v2} (see Appendix \ref{appendix:evidence} for the full derivation).

\begin{align}
 \alpha_t 			 	&= \sum\limits_{A_t}\sum\limits_{O} \Big(P(Y_t|A_t,O) - 1\Big)P_{\cap}(A_t,O,Y_{0:t-1}|u_{1:t}) \label{eq:I_v1} \\
 P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph})        &= 1 + \underbrace{\alpha_{0:t-1} + \alpha_t}_{\alpha_{0:t}} \label{eq:I_v2}
\end{align}

The advantage of Equation \ref{eq:I_v1} is that the summation is only over the states which are in the dependent area $\cap$ of the joint 
distribution. This is generally always much smaller than the full space itself.
Until an object is sensed, the likelihood will always be zero $P(Y_t|A_t,O) = 0$ and $\alpha_t$ will correspond to the probability 
mass which falls within the region of the joint distribution in which the likelihood function is zero. In Figure 
\ref{fig:overlap_dependence_independence} b) \& d), the sum of the probability mass in the blue 
regions is equal to $\alpha_t$.
The point of interest is that as we perform the filtering process we will never re-normalise the whole joint distribution, but only keep 
track of how much it should have been normalised. To this end the marginals  $P(A_t|u_{1:t};\ThAs)$ and $P(O;\ThOs)$  are never re-normalised but are used
at each step to compute how much of the probability mass $\alpha_t$ should go to the normalisation factor $P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph})$. 
The normalisation factor in question will never be negative, as the joint distribution's sum is one and each $\alpha_t$ represents some of the mass removed from the joint distribution. Since we 
keep track of the history of applied  measurement likelihood functions the same amount of probability mass is never removed twice
from the joint distribution.

\subsubsection{Marginals}

There are two different sets of marginals used in the MLMF filter. The first set are the \textbf{joint marginals} of the joint distribution, Equation \ref{eq:ch5:mlmf_filter_conditional}
parameterised by $\mathcolor{blue}{\ThAs}$ and $\mathcolor{red}{\ThOs}$. The second set of marginals are the \textbf{filtered marginals} which are updated by evaluating the joint distribution in dependent states and 
are parameterised by $\mathcolor{blue}{\ThA}$ and $\mathcolor{red}{\ThO}$. At initialisation before the first action or observation is made 
the parameters of the filtered marginal are set equal to those of the joint distribution.

In Histogram-SLAM both the agent and object marginals are obtained, at each time step, by marginalising the joint distribution.
This requires storing and summing over all the parameters of the joint distribution which is expensive.
Instead the MLMF takes advantage of the sparsity of the likelihood function which results in only the dependent elements of the marginal being affected, 
Equation \ref{eq:marignal_mrf} (see Appendix \ref{appendix:marginal} for the full derivation of Equation \ref{eq:marignal_mrf}). 

\begin{equation}
 P(O|Y_{0:\mathbf{t}};\mathcolor{red}{\ThO}) = P(O|Y_{0:\mathbf{t-1}};\mathcolor{red}{\ThO}) - \Big(P_{\cap}(O|Y_{0:t-1}) -  \mathbf{P_{\cap}(O|Y_{0:t})}  \Big)   \label{eq:marignal_mrf}  
\end{equation}
\begin{align}
 &\mathbf{P_{\cap}(O|Y_{0:t}};\mathcolor{blue}{\ThAs},\mathcolor{red}{\ThOs},\mathcolor{dark-green}{\BPsi},\mathcolor{dark-blue}{\BAlph})  = \sum\limits_{A_t} \mathbf{P_{\cap}(A_t,O|Y_{0:t},u_{1:t}};\mathcolor{blue}{\ThAs},\mathcolor{red}{\ThOs},\mathcolor{dark-green}{\BPsi},\mathcolor{dark-blue}{\BAlph}) \nonumber \\[0.5cm]
 &\hspace*{2cm} =\frac{ \sum\limits_{A_t} P_{\cap}(O;\mathcolor{red}{\ThOs}) P_{\cap}(A_t|u_{1:t};\mathcolor{blue}{\ThAs})  P(Y_{0:t}|A_t,O,u_{1:t};\mathcolor{dark-green}{\BPsi})}{P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph})} \label{eq:marignal_mrf_2} 
\end{align}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./ch5-MLMF/Figures/explenation/marginal_cal_example_v2.pdf}
\caption{Filtered marginals. Illustration of the agent and object marginal update, Equation \ref{eq:marignal_mrf}. The joint 
distribution parameters which are independent $A \ominus O$ are pale and the dependent areas $A \cap O$, where $P(Y_t<1|A_t,O)$, are bright. MLMF only
evaluates the joint distribution in dependent states. For each state $s$ of the marginals $1,\dots,10$ the difference 
of the marginals inside the dependent area, before and after the measurement likelihood is applied, is evaluated and removed from the marginals 
$P(A_t|Y_{0:t-1},u_{1:t};\ThA)$, $P(O|Y_{0:t-1};\ThO)$ leading to $P(A_t|Y_{0:t},u_{1:t};\ThA)$, $P(O|Y_{0:t};\ThO)$ (we did not show $u_{1:t}$ in the figure for ease of notation). 
\textit{Bottom-left}: joint marginals $P(A_t|u_{1:t};\ThAs)$ and $P(O;\ThOs)$ remain unchanged by measurements.}
\label{fig:ch5:marginal_update}
\end{figure}

Equation \ref{eq:marignal_mrf} is recursive, $P(O|Y_{0:\mathbf{t}};\mathcolor{red}{\ThO})$ is computed in terms of $P(O|Y_{0:\mathbf{t-1}};\mathcolor{red}{\ThO})$. 
Figure \ref{fig:ch5:marginal_update} illustrates a measurement update of the MLMF.  The illustrated marginals (\textit{Bottom row}) are 
(on the \textit{left}) the ``joint marginals'' $P(A_t|u_{1:t};\mathcolor{blue}{\ThAs})$, $P(O;\mathcolor{red}{\ThOs})$ and (on the \textit{right}) the ``filtered marginals''
$P(A_t|Y_{0:t},u_{1:t};\mathcolor{blue}{\ThA})$, $P(O|Y_{0:t};\mathcolor{red}{\ThO})$. 


The shape of the joint marginals remain unchanged by measurements during the filtering process, they are the parameters of the joint distribution used to update the filtered marginals. 
Table \ref{tab:mlmf_parameters} summarises the functions and parameters of the MLMF for two random variables, an agent and object.

\begin{table}[h]
\centering
\fbox{
\begin{tabular}{lcll}
    functions             	&   & parameters                      & description \\\hline
  $P(A_t|Y_{0:t},u_{1:t})$      & : & $\boldsymbol{\theta}_a$	      & filtered marginals\\
  $P(O|Y_{0:t})$          	& : & $\boldsymbol{\theta}_o$	      &  \\
  $P(A_t|u_{1:t})$       	& : & $\boldsymbol{\theta}^*_a$       & joint marginals\\
  $P(O)$                  	& : & $\boldsymbol{\theta}^*_o$       & \\
  $P(Y_{0:t}|u_{1:t})$      	& : & $\alpha_{0:t} \in \mathbb{R}$         & evidence\\
  $P(Y_{0:t}|A_t,O,u_{1:t})$  	& : & $ \Psi_{0:t} = \{(Y_i,l_i)\}_{i=0:t}$ & likelihood history
\end{tabular}
}
\caption{MLMF functions with associated parameters. The marginal parameters are the discretisation of the 
state space $\boldsymbol{\theta} \in \mathbb{R}^N$, $\boldsymbol{\theta}^{(s)}$ correspond to the probability being in state $s$.}
\label{tab:mlmf_parameters}
\end{table}

We evaluated the MLMF with Histogram-SLAM in the case of the 1D filtering scenario
illustrated in Figure \ref{fig:discrete_example} on page \pageref{fig:discrete_example} and we found them to be identical. Having respected the formulation of Bayes rule, we
assert that the MLMF filtering steps (see  Algorithm \ref{alg:mrf-slam}, Appendix \ref{appendix:ch5:mlmf_update} for a more detailed application of motion-measurement update steps) are 
Bayesian Optimal Filter\footnote{An optimal Bayesian solution is an exact solution to the recursive problem of calculating the exact posterior density \cite{PF_tutorial_2002}}. 
Next we evaluate both space and time complexity of the MLMF filter.

\FloatBarrier
\subsection{Space \& time complexity}\label{ch5:space_time_complexity_MLMF}

For discussion purposes we consider the case of three beliefs, namely that of the agent and two other objects $O^{(1)}$ and $O^{(2)}$, we
subsequently generalise. As stated previously $M$ stands for the number of filtered random variables including the agent and
$N$ is the number of discrete states in the world. In the following section, we compare the space and time complexity 
of MLMF-SLAM with Histogram-SLAM.


\subsubsection{Space complexity}

Figure \ref{fig:3bel_lik_profile} \textit{Left} illustrates the volume occupied by the joint distribution
for a space with $N$ states. Histogram-SLAM would require $N^3$ parameters for the joint distribution 
$P(A,O^{(1)},O^{(2)};\boldsymbol{\theta})$ and $3\,N$ parameters to store the marginals. In general 
for $M$ random variables $N^{M} + M\, N$ parameters are necessary, give a space complexity of $\BigO(N^M)$. 

For MLMF-SLAM, each random variable requires two sets of parameters, $\boldsymbol{\theta}$ and $\boldsymbol{\theta}^*$ 
(see Table \ref{tab:mlmf_parameters}). Given
$M$ random variables, the initial number of parameters is $M (2 N)$.
At every time step the likelihood memory function increments by one measurement and offset, $(Y_t,l=0)$ (Algorithm \ref{alg:ch5:motion_memory}).
Given a state space of size $N$, there can be no more than $N$ different measurement functions (one for each state). In
the worst case scenario the number of memory likelihood function parameters $\Psi_{0:t}$, Equation \ref{eq:memory}, will be $N$.
The total number of parameters is $M (2 N) + N$ which gives a final worst case space complexity which is linear in the number of 
random variables, $\BigO(N\,M)$. 

\begin{figure}
 \centering
  \includegraphics[width=\textwidth]{./ch5-MLMF/Figures/Figure8_v2.pdf}
  \caption{\textit{Left:} Joint distribution $P(A,O^{(1)},O^{(2)})$ of the agent and two objects. Each measurement likelihood function, $P(Y|A,O^{(1)})$, 
  $P(Y|A,O^{(2)})$ corresponds to a hyperplane in the joint distribution
  The state space is discretised to $N$ bins giving the total number of parameters in the joint distribution of $N^3$. 
  \textit{Right:} \textbf{Scalable-MLMF} Each agent-object joint distribution pair is modelled independently. For clarity we have left 
  out the action random variable $u$ which is linked to every agent node.
  Two joint distributions $P(A^{(1)},O^{(1)}|Y^{(1)}_{0:t})$   and $P(A^{(2)},O^{(2)}|Y^{(2)}_{0:t})$ parametrise the graphical model. 
  The dashed undirected lines represent a wanted dependency, if present $O^{(1)}$ and $O^{(2)}$ are to be dependent through $A$. In
  the standard setting there will be no exchange of information between the individual joint distributions. However we demonstrate later on how
  we perform a one time transfer of information when one of the objects is sensed.}
  %\textit{Right:} Joint distribution of the agent and one object $P(A_t,O|Y_{0:t})$.
  %Three measurement 
  %functions have been added to the memory term. At every time step when an action is taken all measurement functions are updated according to 
  %the action applied $u_t$. This means that the first function to be added to the memory will have had all actions applied to it. The number 
  %of the equations and their associated lines in the plot indicate the order in which they have been added to the memory. 
  %The three points are candidates at which we want to evaluate the joint distribution.  The cost of evaluating the 
  %joint distribution at the yellow point is $\BigO(1)$ since we only have to check the first element in the memory. 
  %For the green and cyan points the cost is $\BigO(\log(n))$, where $n$ is the number of likelihood functions in memory.}
  \label{fig:3bel_lik_profile}
\end{figure}

\subsubsection{Time complexity}

For Histogram-SLAM, the computational cost is equivalent to that of the space complexity, $\BigO(N^M)$,
since every state in the joint distribution has to be summed to obtain all the marginals.

For MLMF-SLAM, every state in the joint distribution's state space which has been changed by the likelihood function 
has to be summed, see Figure \ref{fig:ch5:marginal_update} on page \pageref{fig:ch5:marginal_update}. As a result the computational complexity is directly 
related to the number of dependent states $|A \cap O|$. In Figure \ref{fig:ch5:marginal_update}, this corresponds to states where $i = j$ and there are $N$ out 
of a total $N^2$ states for that joint distribution. Figure \ref{fig:3bel_lik_profile} (\textit{Left})
illustrates a joint distribution with $N^3$ states. The dependent states $|A \cap O^{(1)} \cap O^{(2)}|$ are those which 
are within the blue and red planes (where the likelihood evaluates to zero) and comprise $N^2$ states each, 
giving a total of $2\,N^2 - N$ dependent states (negative is to remove the states we count twice at the intersection of the blue and red plane). 

The likelihood term $P(Y_t|A_t,O^{(1)})$ evaluates states to zero which satisfy ${(i=j,\forall k)}$, as 
the measurement of object $O^{(1)}$ is independent of object $O^{(2)}$. With 3 objects, the joint distribution would be
${P(A_t=i,O^{(1)}=j,O^{(2)}=k,O^{(l)}=l)}$ then the likelihood $P(Y_t|A_t,O^{(1)})$  evaluated to 
zero for ${(i=j,\forall k,\forall l)}$ which would mean $N^3$ dependent states.
In general, for $M$ random variables the computational cost is $(M-1) N^{M-1}$ which gives $\BigO(N^{M-1})$ as opposed to the Histogram-SLAM's $\BigO(N^M)$. 
The computation complexity in this setup is still exponential but to the order $M-1$ as opposed to $M$ which nevertheless 
quickly limits the scalability as more objects are added. 

Computing the value of a dependent state ${(i,j,k)}$ in the joint distribution required evaluating Equation \ref{eq:ch5:mlmf_filter_conditional} which
contains a product of $N$ likelihood functions, in the worst case scenario. However the likelihood functions are not overlapping and binary. As a result the complete product
does not have to be evaluated since only one likelihood function will effect the state ${(i,j,k)}$. Thus evaluating Equation \ref{eq:ch5:mlmf_filter_conditional}
yields a cost of $\BigO(1)$ and not $\BigO(N)$.

\subsection{Scalable extension to multiple objects}\label{subsec:scalabe_extension}


To make the MLMF filter scalable we introduce an \textbf{independence assumption} between the objects and model 
the joint distribution (Equation \ref{eq:pair_wise_joint}) as a product of agent-object joint distributions:

\begin{equation}\label{eq:pair_wise_joint}
 P(A_t,O^{(1)},\cdots,O^{(M-1)}|Y_{0:t},u_{1:t}) = \prod\limits_{i=1}^{M-1} P(A^{(i)}_t,O^{(i)}|Y^{(i)}_{0:t},u_{1:t})
\end{equation}

The measurement variable $Y_t$, is the vector of all agent-object 
measurements, $Y_t = \left[Y^{(1)}_t,\dots,Y^{(M-1)}_t\right]^{\mathrm{T}}$. Each agent-object joint distribution has its own parametrisation of the agent's marginal,
$A^{(1)}_t,\dots,A^{(M-1)}_t$ which combine to give the overall marginal of the agent $A_t$. The computation of each object marginal $P(O^{(i)}|Y^{(i)}_{0:t})$ is independent of the other objects. This is evident from the marginalisation 
see Equation \ref{eq:marg_indep}-\ref{eq:marg_indep_prod}.

\begin{align}
 P(O^{(i)}|Y^{(i)}_{0:t},u_{1:t}) &= \sum\limits_{A^{(i)}_t} P(A^{(i)}_t,O^{(i)}|Y^{(i)}_{0:t},u_{1:t}) \label{eq:marg_indep} \\
 P(A_t|Y_{0:t},u_{1:t})   &=\prod\limits_{i=1}^{M-1} \ P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t}) \label{eq:marg_indep_prod}  
\end{align}

The independence assumption will create an unwanted effect with respect to agent's marginal $P(A_t|Y_{0:t},u_{1:t})$. 
At initialisation the agent marginals should be equal, $P(A_0|Y_0) = P(A^{(i)}_0|Y^{(i)}_0) \forall_i$, however this is not the case because of 
Equation \ref{eq:marg_indep_prod}. To overcome this we define the marginal, $P(A_t|Y_{0:t},u_{1:t})$, of the agent as being the average of all the individual
pairs $P(A^{(i)}|Y^{(i)}_{0:t},u_{1:t})$.

\begin{equation}
  P(A_t|Y_{0:t},u_{1:t}) := \frac{1}{M-1} \sum\limits_{i=1}^{M-1} \ P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t}) \label{eq:marg_indep_sum}
\end{equation}

Figure \ref{fig:3bel_lik_profile} (\textit{Right}), depicts the graphical model of the scalable formulation. 
As each joint distribution pair has its own parametrisation of the agent's marginal and these do not subsequently get updated by one another,
the information gained by one joint distribution pair is \textbf{not transferred}.
A solution is to transfer information between the marginals $A^{(i)}$ at specific intervals namely when one of the objects is sensed by the agent. 

The exchange of information of one joint distribution to another is achieved through the agent's marginals $A^{(i)}$ according to Algorithm \ref{alg:scalabe-mrf-slam},  Appendix \ref{app:scalable-mlmf}.
The measurement update is the same as previously described in Algorithm \ref{alg:mrf-slam}  in the case of no positive measurements of the objects. If the agent
senses an object, all of the agent marginals of the remaining joint distributions are set to the marginal of the joint distribution pair belonging to the positive 
measurement $Y^{(i)}_t$. 

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{./ch5-MLMF/Figures/pao_prior_post_v2.pdf}
  \caption{\textbf{Transfer of information (joint distributions)} 
  \textit{Top:}  Joint distributions of $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$ and $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$ prior sensing, $Y_t^{(2)}=1$, see Figure \ref{fig:independence_object}
  (\textit{Top right}) for the corresponding marginals. The red and green lines across the joint distributions 
   correspond to the region in which the likelihood functions $P(Y^{(1)}_{t}|A^{(1)}_t,O^{(1)})$ and $P(Y^{(2)}_{t}|A^{(2)}_t,O^{(2)})$ will change the joint distributions.
  The dotted blue lines are to ease the comparison ofthe joint distributions prior and post sensing.
  \textit{Bottom right:}  After the agent has sensed $O^{(2)}$, all the probability mass which was not overlapping the green line becomes an infeasible
  solution to the agent and object locations. At this point the marginals $P(A^{(1)}_t|u_{1:t}) \not= P(A^{(2)}_t|u_{1:t})$ are no longer equal 
  (see the blue marginals \textit{Top}). \textit{Bottom left:} The constraint imposed by the likelihood function of the second object
  (green line) is transferred to the joint distribution of the first object according to Algorithm \ref{alg:scalabe-mrf-slam}.
  This results in a change in the joint distribution  $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$, which satisfies the constraints 
  imposed by the agent's marginal from the joint distribution $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$.}
  \label{fig:transfer_information}
\end{figure}

Figure \ref{fig:transfer_information}, depicts the process of information exchange between object $O^{(1)}$ and $O^{(2)}$ in the event that the agent 
senses $O^{(2)}$. Prior to the positive detection, both marginals $P(A^{(1)}_t|Y^{(1)}_{0:t-1},u_{1:t})$ and $P(A^{(2)}_t|Y^{(2)}_{0:t-1},u_{1:t})$ 
occupy the same region and are identical. When the agent senses $O^{(2)}$ the line defined by the measurement 
likelihood function $P(Y^{(2)}_t|A^{(2)}_t,O^{(2)})$ becomes a hard constraint implying that both the agent and $O^{(2)}$ have to satisfy this constraint.
Figure \ref{fig:independence_object} shows marginals at initialisation, prior contact between the agent and object and the after the measurement 
(post contact) has been integrated into the marginals (resulting from the joint distributions in Figure \ref{fig:transfer_information}).
%The marginals in the \textit{Left} plot are the result after updating the marginals $A^{(i)}$. The \textit{Right} plot shows the result for the case where the objects
%remain independent. 

The result of introducing a dependency between the objects through the agent's marginals in the event of a sensing and treating them
independently gives the same solution as the histogram filter in this particular case. However as each individual marginal $A^{(i)}_t$ diverges 
from the other marginals, the filtered solution will diverge from the histogram's solution. We assume however that the objects are weakly 
dependent and sharing information during positive sensing events is sufficient. In section \ref{subsec:eval_indep_assumptiom} we will 
evaluate the independence assumption with respect to the histogram filter.

Table \ref{tab:time_space_summary} summarises the time and space complexity for the three filters.% In the case of transfer of
%information between marginals the computational complexity is higher, however this is a one time occurrence. 

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{./ch5-MLMF/Figures/Figure11_v3.pdf}
  \caption{\textbf{Transfer of information (marginals)} \textit{Top left:} Initial beliefs of the agent and object's location. The agent moves to the left until it senses object $O^{(2)}$.
  \textit{Top right:} Marginals prior the agent entering in contact with the green object, see Figure \ref{fig:transfer_information} (\textit{Top}) for an illustrate of the joint distributions.
  \textit{Bottom left:} resulting marginals after setting the agent marginals of each 
  joint distribution equal $A^{(1)}_t = A^{(2)}_t$ according to Algorithm \ref{alg:scalabe-mrf-slam}. The object marginal $P(O^{(2)}|Y_{0:t})$ is recomputed. 
  \textit{Bottom Right:} resulting marginals in which the objects have no influence on one another.
  Note that a transfer of information has caused a change in the marginal $O^{(1)}$.}
  \label{fig:independence_object}
\end{figure}

\begin{table}
 \centering
 \begin{tabular}{c|c|c|}
\cline{2-3}
				        &    \textbf{space}   &     \textbf{time} \\ \hline
    \multicolumn{1}{|l}{Histogram}      & \multicolumn{1}{|l}{$\BigO(N^M)$}   &  \multicolumn{1}{|l|}{$\BigO(N^M)$}      \\ \hline
    \multicolumn{1}{|l}{MLMF}           & \multicolumn{1}{|l}{$\BigO(M\,N)$}  &  \multicolumn{1}{|l|}{$\BigO(N^{(M-1)})$} \\ \hline
    \multicolumn{1}{|l}{scalable-MLMF}  & \multicolumn{1}{|l}{$\BigO(M\,N)$}  &  \multicolumn{1}{|l|}{$\BigO(M\,N)$}     \\ \hline
   \end{tabular}
   \caption{\textbf{Time and space complexity summary} For both MLMF and scalabe-MLMF the worst case scenario is reported for the space complexity.}
   \label{tab:time_space_summary}
\end{table}

%To make the MLMF filter scalable we introduce an \textbf{independence assumption} between the objects and model 
%the joint distribution (Equation \ref{eq:pair_wise_joint}) as a product of agent-object joint distributions:
%\begin{equation}\label{eq:pair_wise_joint}
% P(A_t,O^{(1)},\cdots,O^{(M-1)}|Y_{0:t},u_{1:t}) = \prod\limits_{i=1}^{M-1} P(A^{(i)}_t,O^{(i)}|Y^{(i)}_{0:t},u_{1:t})
%\end{equation}
%The measurement variable $Y_t$, is the vector of all agent-object 
%measurements, $Y_t = \left[Y^{(1)}_t,\dots,Y^{(M-1)}_t\right]^{\mathrm{T}}$. Each agent-object joint distribution has its own parametrisation of the agent's marginal,
%$A^{(1)}_t,\dots,A^{(M-1)}_t$ which combine to give the overall marginal of the agent $A_t$. The computation of each object marginal $P(O^{(i)}|Y^{(i)}_{0:t})$ is independent of the other objects. This is evident from the marginalisation 
%see Equation \ref{eq:marg_indep}-\ref{eq:marg_indep_prod}.
%\begin{align}
% P(O^{(i)}|Y^{(i)}_{0:t},u_{1:t}) &= \sum\limits_{A^{(i)}_t} P(A^{(i)}_t,O^{(i)}|Y^{(i)}_{0:t},u_{1:t}) \label{eq:marg_indep} \\
% P(A_t|Y_{0:t},u_{1:t})   &=\prod\limits_{i=1}^{M-1} \ P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t}) \label{eq:marg_indep_prod}  
%\end{align}
%The independence assumption will create an unwanted effect with respect to agent's marginal $P(A_t|Y_{0:t},u_{1:t})$. 
%At initialisation the agent marginals should be equal, $P(A_0|Y_0) = P(A^{(i)}_0|Y^{(i)}_0) \forall_i$, however this is not the case because of 
%Equation \ref{eq:marg_indep_prod}. To overcome this we define the final marginal, $P(A_t|Y_{0:t},u_{1:t})$, of the agent as being the average of all the individual
%pairs $P(A^{(i)}|Y^{(i)}_{0:t},u_{1:t})$.
%\begin{equation}
%  P(A_t|Y_{0:t},u_{1:t}) := \frac{1}{M-1} \sum\limits_{i=1}^{M-1} \ P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t}) \label{eq:marg_indep_sum}
%\end{equation}
%Figure \ref{fig:3bel_lik_profile} (\textit{Right}), depicts the graphical model of the scalable formulation. 
%As each joint distribution pair has its own parametrisation of the agent's marginal and these do not subsequently get updated by one another,
%the information gained by one joint distribution pair is \textbf{not transferred}.
%A solution is to transfer information between the marginals $A^{(i)}$ at specific intervals namely when one of the objects is sensed by the agent. 
%\begin{center}
%\begin{minipage}{\linewidth}
%\begin{algorithm}[H]
%\label{alg:scalabe-mrf-slam}
%\SetKwComment{Comment}{$\triangleright$\ }{}
%\SetKwInOut{Input}{input}
%\SetKwInOut{Output}{output}
%\Input{$P(A^{(i)}_t|u_{1:t})$, $P(A^{(i)}_t|Y^{(i)}_{0:t-1},u_{1:t})$\\$P(O^{(i)})$, $P(O^{(i)}|Y^{(i)}_{0:t-1},u_{1:t})$\\ $Y^{(i)}_t$\\$i=1,\cdots,M$}
%\BlankLine
%\Comment{If object $i$ has been sensed by the agent}
%\eIf{$Y^{(i)}_t == 1$}{
% \Comment*[r]{measurement update Algo. \ref{alg:mrf-slam} }
%$P(O^{(i)}|Y^{(i)}_{0:t})   \gets P(O^{(i)}|Y^{(i)}_{0:t-1})$  \Comment*[r]{measurement update Algo. \ref{alg:mrf-slam}} 
%$P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t}) \gets P(A^{(i)}_t|Y^{(i)}_{0:t-1},u_{1:t})$\\
%\ForAll{$j\in(1,\dots M-1) \setminus i$}
%{
%$P(A^{(j)}_t|Y_{0:t},u_{1:t}) = P(A^{(i)}_t|Y_{0:t},u_{1:t})$ \\
%$P(A^{(j)}_t|u_{1:t}) = P(A^{(i)}_t|u_{1:t})$\\
%$P(O^{(j)}|Y^{(i)}_{0:t}) \leftarrow \sum\limits_{A^{(j)}} P(A^{(j)}_t,O^{(j)}|Y^{(i)}_{0:t})$ \Comment*[r]{object $j$ marginal} 
%}
%
%}{
%\ForAll{$i\in(1,\dots M)$}{
%  measurement update Algo. \ref{alg:mrf-slam}
%}
%}
%\caption{Scalable-MLMF: Measurement Update}
%\end{algorithm} 
%\end{minipage}
%\end{center}

%The exchange of information of one joint distribution to another is achieved through the agent's marginals $A^{(i)}$ according to Algorithm \ref{alg:scalabe-mrf-slam}.
%The measurement update is the same as previously described in Algorithm \ref{alg:mrf-slam} in the case of no positive measurements of the objects. If the agent
%senses an object, all of the agent marginals of the remaining joint distributions are set to the marginal of the joint distribution pair belonging to the positive 
%measurement $Y^{(i)}_t$. 

%\begin{figure}
%  \centering
%  \includegraphics[width=0.9\textwidth]{./ch5-MLMF/Figures/pao_prior_post_v2.pdf}
%  \caption{\textbf{Transfer of information (joint distributions)} 
%  \textit{Top:}  Joint distributions of $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$ and $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$ prior sensing, $Y_t^{(2)}=1$, see Figure \ref{fig:independence_object}
 % (\textit{Top right}) for the corresponding marginals. The red and green lines across the joint distributions 
 %  correspond to the region in which the likelihood functions $P(Y^{(1)}_{t}|A^{(1)}_t,O^{(1)})$ and $P(Y^{(2)}_{t}|A^{(2)}_t,O^{(2)})$ will change the joint distributions.
%  The dotted blue lines are to ease the comparison ofthe joint distributions prior and post sensing.
%  \textit{Bottom right:}  After the agent has sensed $O^{(2)}$, all the probability mass which was not overlapping the green line becomes an infeasible
%  solution to the agent and object locations. At this point the marginals $P(A^{(1)}_t|u_{1:t}) \not= P(A^{(2)}_t|u_{1:t})$ are no longer equal 
%  (see the blue marginals \textit{Top}). \textit{Bottom left:} The constraint imposed by the likelihood function of the second object
%  (green line) is transferred to the joint distribution of the first object according to Algorithm \ref{alg:scalabe-mrf-slam}.
%  This results in a change in the joint distribution  $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$, which satisfies the constraints 
%  imposed by the agent's marginal from the joint distribution $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$.}
%  \label{fig:transfer_information}
%\end{figure}

%Figure \ref{fig:transfer_information}, depicts the process of information exchange between object $O^{(1)}$ and $O^{(2)}$ in the event that the agent 
%senses $O^{(2)}$. Prior to the positive detection, both marginals $P(A^{(1)}_t|Y^{(1)}_{0:t-1},u_{1:t})$ and $P(A^{(2)}_t|Y^{(2)}_{0:t-1},u_{1:t})$ 
%occupy the same region and are identical. When the agent senses $O^{(2)}$ the line defined by the measurement 
%likelihood function $P(Y^{(2)}_t|A^{(2)}_t,O^{(2)})$ becomes a hard constraint implying that both the agent and $O^{(2)}$ have to satisfy this constraint.

%Figure \ref{fig:independence_object} shows marginals resulting from the joint distributions in Figure \ref{fig:transfer_information}. The marginals in
%the \textit{Bottom left} plot are the result after updating the marginals $A^{(i)}$. The \textit{Bottom right} plot shows the result for the case where 
%the objects remain independent. 

%The result of introducing a dependency between the objects through the agent's marginals in the event of a sensing and treating them
%independently gives the same solution as the histogram filter in this particular case. However as each individual marginal $A^{(i)}_t$ diverges 
%from the other marginals, the filtered solution will diverge from the histogram's solution. We assume however that the objects are weakly 
%dependent and sharing information during positive sensing events is sufficient. In section \ref{subsec:eval_indep_assumptiom} we will 
%evaluate the independence assumption with respect to the histogram filter.

%Table \ref{tab:time_space_summary} summarises the time and space complexity for the three filters.% In the case of transfer of
%information between marginals the computational complexity is higher, however this is a one time occurrence. 

%\begin{figure}
%  \centering
 % \includegraphics[width=0.8\textwidth]{./ch5-MLMF/Figures/Figure11_v3.pdf}
 % \caption{\textbf{Transfer of information (marginals)} \textit{Top left:} Initial beliefs of the agent and object's location. The agent moves to the left until it senses object $O^{(2)}$.
 % \textit{Top right:} Marginals prior the agent entering in contact with the green object, 
 % see Figure \ref{fig:transfer_information} (\textit{Top}) for an illustrate of the joint distributions. The black arrow indicates the heading of the agent.
 % \textit{Bottom left:} resulting marginals after setting the agent marginals of each 
 % joint distribution equal $A^{(1)}_t = A^{(2)}_t$ according to Algorithm \ref{alg:scalabe-mrf-slam}. The object marginal $P(O^{(2)}|Y_{0:t})$ is recomputed. 
 % \textit{Bottom right:} resulting marginals in which the objects have no influence on one another.
 % Note that a transfer of information has caused a change in the marginal $O^{(1)}$.}
 % \label{fig:independence_object}
%\end{figure}


%For the full derivation of time and space complexity of the scalable-MLMF filter see Appendix \ref{appendix:space_time_scalable_mlmf}.

%\begin{table}
% \centering
% \begin{tabular}{c|c|c|}
%\cline{2-3}
%				        &    \textbf{space}   &     \textbf{time} \\ \hline
%    \multicolumn{1}{|l}{Histogram}      & \multicolumn{1}{|l}{$\BigO(N^M)$}   &  \multicolumn{1}{|l|}{$\BigO(N^M)$}      \\ \hline
%    \multicolumn{1}{|l}{MLMF}           & \multicolumn{1}{|l}{$\BigO(M\,N)$}  &  \multicolumn{1}{|l|}{$\BigO(N^{(M-1)})$} \\ \hline
%    \multicolumn{1}{|l}{scalable-MLMF}  & \multicolumn{1}{|l}{$\BigO(M\,N)$}  &  \multicolumn{1}{|l|}{$\BigO(M\,N)$}     \\ \hline
%   \end{tabular}
%   \caption{\textbf{Time and space complexity summary} For both MLMF and scalabe-MLMF the worst case scenario is reported for the space complexity.}
%   \label{tab:time_space_summary}
%\end{table}



\section{Evaluation}\label{ch5:evaluation}

We conduct three different types of evaluation to quantify the scalability and correctness of the scalable-MLMF filter. The first experiment
tests the scalability of our filter in terms of processing time taken per motion-measurement update cycle. The second experiment evaluates the independence 
assumption made in the scalable-MLMF filter between the objects. The third and final experiment determines the effect of the 
memory size on a search policy to locate all the objects in the \textit{Table} world.

\subsection{Evaluation of time complexity}

We measured the time taken by the motion-measurement update loop, as a function of the number beliefs and number of states per belief. 
We started with a 100 states per belief and gradually increased it to 10'000'000 over 50 steps. Each of the 50 steps treated 2 to 25  objects. 
Figure \ref{fig:time_complexity} \textit{left} illustrates the computational
cost as a function of number of states and objects. For each state-object pair 100 motion-measurement updates were performed. Most of the trials returned time updates 
below 1 Hz. Figure \ref{fig:time_complexity} \textit{right} shows the computational cost as a function of the number of states plotted for 6 different filter runs with
a different number of objects. As the number of states increases exponentially so does the computational cost. Note the cost increases at the same
rate as the number of states meaning that the computational complexity is linear with respect to the number of states. This result is in agreement with 
the asymptotic time complexity.

\begin{figure}
 \includegraphics[width=\textwidth]{./ch5-MLMF/Figures/Figure12_v2.pdf}
 \caption{\textbf{Time complexity:} \textit{left:} mean time taken for a loop update (motion and measurement) as a function of the number of states in a marginal and the 
 number of objects present. \textit{right:} time taken for a loop update with respect to the number of states in the marginal. The colour coded lines are 
 associated with the number of objects present. The computational cost is plotted on a log scale. As the number of states increases exponentially the
 computational cost matches it.}
 \label{fig:time_complexity}
\end{figure}

% Why did we compare the average with the product of marginals

\subsection{Evaluation of the independence assumption}\label{subsec:eval_indep_assumptiom}

In Section \ref{subsec:scalabe_extension} we made the assumption (for scalability reasons) that the objects' beliefs are independent
of one another. This assumption is validated by comparing the MLMF filter on three random variables, an agent and two objects, with the ground truth
which we obtain from the standard histogram filter. For each of the three beliefs (the agent and two objects), 100 different marginals 
were generated and the true locations (actual position of the agent and objects) were sampled. 
Figure \ref{fig:independence_assumption_test} \textit{Top-left} illustrates one instance of the initialisation of the agent and object marginals
with their associated sampled true position.
The agent carries out  a sweep of the state space for each of the marginals and the policy is saved 
and run with the scalable-MLMF filter. In the first experiment we assumed that the objects are completely independent 
and that there was no transfer of information between the pair-wise joint distributions. In the second and third experiments there 
is an exchange of information as described in Algorithm \ref{alg:scalabe-mrf-slam}. Here we compare the effect of using 
the product of the agent's marginals, Equation \ref{eq:marg_indep_prod}, with the average of the marginals, Equation \ref{eq:marg_indep_sum}.
We expect the average of the the agent's marginal to yield a result closer to the ground truth as the marginal of the 
agent $P(A_t|Y_{0:t},u_{1:t})$ at initialisation is the same as the ground truth (the Histogram-SLAM's). As for the marginal of the 
objects $P(O^{(i)}|Y_{0:t})$ we expect the difference between them to be independent of whether the product or average of the 
agent's marginal is used. This results from Algorithm \ref{alg:scalabe-mrf-slam}. When an object $i$ is sensed all the corresponding 
agent marginals $P(A^{(j)}|u_{1:t})$ are set equal to $P(A^{(i)}|u_{1:t})$ and not to $P(A_t|Y_{0:t},u_{1:t})$. This is a design 
decision of our information transfer heuristic. There are many other possibilities but this is one of the simplest.
For each of the 100 sweeps the ground truth is compared with the scalabe-MLMF using the Hellinger distance (Equation \ref{eq:hellinger})
\begin{equation} \label{eq:hellinger}
 H(P,Q) = \frac{1}{\sqrt{2}}\, \|\sqrt{P} - \sqrt{Q}\|_2  
\end{equation}
which is a metric which measures the distance between two probability distributions. Its value lies strictly between 0 (the two 
distributions are identical) and 1 (no overlap between them). Figure \ref{fig:independence_assumption_test} shows the kernel density 
distribution of the Hellinger distances taken at each time step for all 100 sweeps. In the \textit{Top-left} of the figure, 
for the case when no transfer of information is applied, all the marginals are far from the 
ground truth. This results from the introduction of the independence assumption, necessary to scale the MLMF. 
Figure \ref{fig:independence_assumption_test} \textit{Bottom} shows the results for difference between the product and average of the agents
marginals. As expected there is no difference between the objects' marginals when considering both methods (product and average) with respect
to the ground truth. The predominant difference occurs in the agent's marginal $P(A_t|Y_{0:t},u_{1:t})$. This is also expected and 
prompted the introduction of the average method instead of the product. 

The scalable-MLMF information exchange heuristic will not lead to any of the objects marginals probability mass being falsely  
removed during the information transfer, which is close to a winner-take-all approach in terms of beliefs.
When object $i$ is sensed its associated agent marginal is set to all other agent-object joint pairs, which results in the 
information accumulated in the $j$th agent marginals being replaced by the $i$th.

\begin{figure}
\centering
 \includegraphics[width=\textwidth]{./ch5-MLMF/Figures/Figure13_v2.pdf}
 \caption{\textbf{Comparison of scalable-MLMF and the histogram filter} A deterministic sweep policy was carried out for 100 different initialisations of 
 the agent and object beliefs. \textit{Top left:} One particular Initialisation of the agent and object
 random variables. The true position of the agent and objects were sampled at random. The black arrow indicates the general policy which was 
 followed for each of the 100 sweeps. 
 These were performed for \textbf{1)} scalable-MLMF  with objects considered to be independent at all times (Algorithm \ref{alg:scalabe-mrf-slam}). 
 \textbf{2)} Agent marginal $P(A_t|Y_{0:t},u_{1:t})$ is the product of marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref{eq:marg_indep_prod}. 
 \textbf{3)} marginal $P(A_t|Y_{0:t},u_{1:t})$ is taken to be the average of all marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref{eq:marg_indep_sum}.  For each of these three experiment we report the 
 kernel density estimation over the Hellinger distances taken at every time step between ground truth (from histogram filter) and scalable-MLMF.}
 \label{fig:independence_assumption_test}
\end{figure}

\subsection{Evaluation of memory}

The memory measurement likelihood function $P(Y_{0:t}|A_t,O,u_{1:t};\Psi_{0:t})$ is parameterised by the 
history of all the measurement likelihood functions which have been applied on the joint 
distribution since initialisation. As detailed previously there can be no more than $|\Psi_{0:t}| \leq N$ different measurement likelihood functions added to 
memory. In the case of a very large state space this might be cumbersome. We investigate how restricting the memory size, the number 
of parameters $|\Psi_{0:t}|$, can impact on the decision process in an Active-SLAM setting. Given our set up a breadth-first search in the action 
space is chosen with a one time step horizon, making it a greedy algorithm. The objective function utilised is the information
gain of the beliefs after applying an action, Equation \ref{eq:greedy_algorithm}.

\begin{equation}\label{eq:greedy_algorithm}
 u^*_{t} = \operatorname*{arg\,max}_{u_t} H\{P(A_{t-1},O|Y_{0:t-1},u_{1:t-1})\} - \mathbb{E}_{Y_t}\left[H\{P(A_{t},O|Y_{0:t},u_{1:t})\}\right]
\end{equation}

For each action the filter is run forward in time and all future measurements since we cannot know ahead of time the actual 
measurement. The information gain is the difference between the current entropy (defined 
by $H\{\cdot\}$) and the future entropy after the simulated motion and measurement update. The action $u^*_t$ with the highest information gain 
is subsequently selected. This is repeated at each time step. Figure \ref{fig:exploration_init} illustrates the environment setup for 
a 1D and 2D case. The agent's task is to find the objects in the environment.


\begin{figure}
  \includegraphics[width=\textwidth]{./ch5-MLMF/Figures/exploration_initialisation.pdf}
  \caption{\textbf{Agent's prior beliefs.} Two types of environment, the first is 
  a 2D world where the agent lives in a square surrounded by a wall whilst the second is a 1D
  world. In the 2D figures the agent is illustrated by a circle with a bar to indicate its heading. The true location 
  of the objects are represented by colour coded squares. \textit{Top row} three different initialisations of the agent's location. 
  \textit{Bottom row} d) the agent's prior beliefs with respect to the location of the first object and e) belief of the second object's location.
  \textit{bottom row} f) 1D world with one object.}
  \label{fig:exploration_init}
\end{figure}

For the 2D search we consider three different initialisations (single-Gaussian, four-Gaussian, Uniform) for the agent's belief where there are 
two objects to be found. Ten searches are carried out for each of the three initialisations of the agent's beliefs. 
The agent's true location, for each search, is sampled from its initial belief, and the objects' locations 
(red and green squares in Figure \ref{fig:exploration_init}) are kept fixed throughout all searches. Each search is repeated for 
18 different memory sizes ranging from 1 to $N$ (the number of states). For the 1D search case one object is considered since adding more objects  
makes the search easier and the interest lies in the memory effects of the search and not the search itself. In Figures \ref{fig:time_to_reach_goal_1D}-\ref{fig:time_to_reach_goal_2D} we 
report on the time taken to find all objects with respect to a given memory size which is shown as the percentage of the total number of states. 
In the 1D search case the time variability taken to find the object converges when the memory size is at 60\% of the original state space. 
As for the 2D search with 2 beliefs (agent \& 1 object) the convergence depends on the agent's initial belief. For the 1-Gaussian (green line) 
all searches take approximately the same amount of time after a memory size of 9\%. As for the remaining two initialisations convergence is achieved at  48\%. 
The same holds true for the case of 3 beliefs (agent \& 2 objects).

In the 2D searches, the memory size has a less impact on the time taken to find the objects than in the 1D (which is a special search case). 
Only when the memory size is less than 6\% is there a significant change. We conclude that at least in the case of 
the greedy one step-look ahead planner which is frequently used in the literature, the size of the memory seems not to be a limiting factor in terms of the time taken to accomplish the search.

\begin{figure}	
  \centering
  \includegraphics[width=0.8\textwidth]{./ch5-MLMF/Figures/exper_mem_1d.pdf}
  \caption{\textbf{Memory size vs time to find object in 1D} Results of the effect of the memory size on the decision process
  for the 1D search illustrated in Figure \ref{fig:exploration_init} \textit{f)}.
  The memory size is reported as the percentage of total number of states present in the marginal space. At 100\% the size
  of the memory is equal to that of the state space, $N=100$ in this case. A total sweep of the entire state space would result in a total of 
  200 steps, the dotted grey line in the above figure. When no restrictions are placed on the memory size the policy following the greedy 
  approach takes around 180 steps. This result converges when the number of parameters $|\Psi_{0:t}|$ of the memory likelihood function is 
  greater than 50\% of the original state space. } 
  \label{fig:time_to_reach_goal_1D}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./ch5-MLMF/Figures/exper_mem_all.pdf}
  \caption{\textbf{Memory size vs time to find objects in 2D}. The initial beliefs correspond 
  to those of Figure \ref{fig:exploration_init}, a) for Gaussian (green line), b) 4 Gaussians (red line)
  and c) Uniform (blue line), both objects are initialised according to d) and e).}
   \label{fig:time_to_reach_goal_2D}
\end{figure}



\FloatBarrier
\section{Conclusion}\label{ch5:conclusion}

This work addresses the Active-SLAM filtering problem for scenarios in which sensory information relating to the map is very limited. Current
SLAM algorithms filter the errors originating from sensory measurements and not prior uncertainty. By making the assumption
that the joint distribution of all the random variables is a multivariate Gaussian, inference is tractable. Since the origin of 
the uncertainty does not originate from the measurement noise, no assumption can be made about the structure of the joint distribution.
In this case a suitable filter would be the histogram which makes no assumption about the shape or form taken by the joint distribution. 
However, the space and time complexity are exponential with respect to the number random variables and this is a major 
limiting factor for scalability. 

The main contribution of this work is a formulation of a histogram Bayesian state space estimator in which the computational complexity is 
both linear in time and space. A different approach to other SLAM formulations as been taken in the sense that
the joint distribution is not explicitly parameterised avoiding the exponential increase in parameter space which would otherwise have been the case. 
The MLMF parameters consist of the marginals and the history of measurement functions which have been applied. By solely evaluating the joint 
distribution at the states which are affected by the current measurement function whilst taking into account the 
memory, the MLMF filter obtains the same filtered marginals as the histogram filter. Further, the worst case space complexity is 
linear rather than exponential and the time complexity remains exponential but increases at lower rate than in the histogram filter.
In striving to make the filter scalable we make the assumption that the objects are independent. An individual MLMF
is used for each agent-object pair. We evaluate the difference between the scalable-MLMF with a ground truth provided 
by the histogram filter for 100 different searches with respect to the Hellinger distance. We conclude that 
the divergence is relatively small and thus the scalable-MLMF filter provides a good approximation to the true filtered
marginals. We evaluate the time taken to perform a motion-update loop for different discretisations of the state 
space (100 to 10'000'000 states) and number of objects (2 to 25). In most of the cases we achieve an update cycle rate below 1Hz. 
We evaluate how the increase of the number of states affects the computational cost and find  the relationship to be linear and thus 
in agreement with our analysis of the asymptotic growth rate. We analyse the effect of the memory size 
(the remembered number of measurement likelihood functions) on the decision theoretic process of reducing 
the uncertainty of the map and agent during a search task. 
We conclude that in the 2D case the memory size has much less effect than in the 1D case and that it 
is not necessary to remember every single measurement function.

This implies that the MLMF and scalable-MLMF that we have are a computationally tractable means of 
performing SLAM in a case scenario in which mostly negative information is present and the 
joint distribution cannot be assumed to have any specific structure. Furthermore, the filter can be used at a higher cognitive level than 
the processing of raw sensory information as is often the case in Active-SLAM. MLMF would be well suited for reasoning tasks 
where the robot's field of view is limited.

An interesting future extension could be to make the original MLMF filter scalable without introducing assumptions.
One possibility could be to consider Monte Carlo integration methods for inference. These can scale well to high dimensional 
spaces whilst still providing reliable estimates. A second possibility could be to investigate the use of Gaussian Mixtures as a 
form of parameterisation of the marginals to blend our filter with EKF-SLAM. This would allow the parameters 
to grow quadratically with respect to the dimension of the marginal space as opposed to exponentially as is the case 
with the histogram and MLMF filters.






%\subsection{Space \& time complexity (scalable-MLMF)}\label{appendix:space_time_scalable_mlmf}
%\subsection{Space complexity}
%\paragraph{Scalable-MLMF}
%The initial number of parameters is $\left(M - 1\right) \cdot \left(2 \cdot (2 \cdot N)\right)$. The $(2 \cdot N)$ is the cost per random variable
%as before, the additional factor of $2$ arises because we model pair-wise joint distributions and there are two random variables per joint distribution. The final
%factor $\left(M - 1\right)$ corresponds to the total number of joint distributions, in the case of $M$ random variables. The final cost is then 
%$(M - 1) \cdot N \cdot \left(4 + D\right)$ when taking into account the history.
%\subsection{Time complexity}
%\paragraph{Scalabe-MLMF} The computational cost is $\BigO{2 \cdot \left(M - 1\right) \cdot N }$. For one agent-object joint distribution, $N$
%states lie in the area of influence of the measurement function. Given $M$ random variables, the number of states needed to evaluate in all the joint distributions
%is $M-1$. The final marginal of the agent $P(A_t|Y_t) = \prod\limits_{i=1}^{M-1} P(A^{(i)}_t|Y^{(i)}_t)$ is obtained through $(M-1)\cdot N$ multiplications or additions.
%This explains the factor of $2$ in the final computational cost.
%\paragraph{Memory}\label{appendix:memory_time_complexity}%
%The MLMF filter stores the history of all likelihood functions which have been applied on the joint distribution. This is the 
%memory term $P(Y_{0:t-1}|A_t,O)$ in Equation \ref{eq:joint_filter_memory}. Table \ref{tab:memory_list} shows the memory list after three 
%iterations and the resulting effect in  the joint distribution is illustrated in Figure \ref{fig:3bel_lik_profile} \textit{right}.
%\begin{table}
%\begin{center}
% \begin{tabular}{llc}
% $P(Y_{0 }|A_0,O)  = $  & $P(Y_0|A_0,O)$				                            & (t=0) \\
% $P(Y_{0:1}|A_1,O) = $  & $P(Y_1|A_1,O) \cdot P(Y_0|A_1,O+u_1)$                             & (t=1) \\
% $P(Y_{0:2}|A_2,O) = $  & $P(Y_2|A_2,O) \cdot P(Y_1|A_2,O+u_1) \cdot P(Y_0|A_2,O+u_1+u_2)$  & (t=2)
%\end{tabular}
%\end{center}
%\caption{\textbf{Memory list} After three updates the memory term $P(Y_{0:t}|A_t,O)$ contains three functions. The parametrisation
%of the functions is the same with the exception of actions $u_t$ which are added after each motion update step. As the result the 
%first function added to the list will have all the actions applied to it from the first time step to the last.}
%\label{tab:memory_list}
%\end{table}

%At every time step the current action (causing a displacement) is applied to all elements in the memory before appending the new 
%measurement function $P(Y_t|A_t,O)$. The application of the actions causes a shift of likelihood functions along the agent's axis
%in the joint distribution. As a result, the memory list is always sorted and the first element is always the last measurement function
%to have been applied. 
%During the measurement update Equation \ref{eq:marignal_mrf_2}, only the first entry of the memory function has to be evaluated
%giving a cost of $\BigO{1}$ at every time step when evaluating the dependent states (those which lie on the hyperplane). When
%evaluating a point which is not on the hyperplanes, its offset ($c$) can be evaluated $A=O+c$ and checked whether it is contained
%within the list. We employ a binary search which has a time complexity of $\BigO{\log(n)}$ (where $n$ is the current size of the memory). This is not necessary during most
%of the filtering process since the dependent states to be evaluated in the joint distribution always lie on the line $A=O$.

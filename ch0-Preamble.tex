%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter 0: Preamble
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{figures/ch0/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
\lettrine[lines=2]{D}{ecision} making and planning with partial state information is a problem faced 
by all forms of intelligent entities being either virtual, synthetic or biological. The formulation of 
a problem under partial state information leads to an exorbitant set of choices with associated 
probabilistic outcomes making its resolution difficult when using traditional planning methods. 
Human beings have acquired the ability of acting under uncertainty through education and self-learning. 
Transferring our know-how to artificial agents and robots will make it faster for them to learn and even improve upon us in
tasks in which incomplete knowledge is available, which is the objective of this thesis.

A large body of scientific work has focused on transferring behaviour from humans to robots via Programming by Demonstration frameworks which focus on learning how to imitate human behaviour. Tasks such as ``pick and place'', hitting motions, and bipedal locomotion have been encoded 
through either symbolic, statistical or dynamical system representations. In contrast there has been less focus on transferring higher cognitive behaviour such as
problem solving skills and search strategies from humans to robots.

This thesis aims to model how humans reason with respect to their beliefs and the role uncertainty plays during spatial navigation search tasks. 
We consider for instance tasks such as localising  an object in a room or connection a plug to a power socket in the dark, 
or any such situation with total suppression of visual information, and transfer this reasoning mechanisms to a robot apprentice. 
There are many robotic application domains in which uncertainty resulting from a lack of visual perception is common, such as in underwater 
maintenance, planetary exploration and occluded manipulation tasks. Learning human search models and transferring them to robots is useful
in such domains and learning a search strategy from scratch would prove intractable.

A difficulty in learning humans reasoning mechanisms, in the search scenarios we consider, is that
the humans beliefs and sensations (haptic and tactile) are unobservable and they vary within and
across subjects. We infer the human sensations from either the kinematic relationship between them
and a known geometric description of the environment or the human subjects use a tool equipped with
a sensor (force-torque sensor) whose measurements are used to infer the human sensations. The actual
sensations, which are a function of either the sensor tool or kinematic-environment measurements, are
transformed to a binary feature vector which encodes whether contact are present between features
such as surfaces, edges and corners of the environment.

We model the human's beliefs by a probability density function which we update through recursive Bayesian 
state space estimation using motion estimates, acquired through a tracking system (the human subjects wore markers), 
and the sensation estimates were obtained as described above. We make the assumption that the probability 
density function, representing the human's belief, is updated by a Bayesian recursion and that this process is similar to the way in 
which humans integrate information.

To model the reasoning processes of human subjects performing the search tasks we learn a generative joint distribution over beliefs
and actions (end-effector velocities) which were recorded during the executions of the task. 
The high dimensionality of the belief and its varying complexity  during the searches required that we compress the 
belief to its most likely state and entropy. 

We evaluate this methodology of learning search strategies in a task consisting of finding an object on a table. 
We demonstrate that multiple search strategies are encoded in the joint belief-action distribution and 
we compare this approach with greedy myopic and coastal navigation search algorithms. The results show that the human learned 
search model is the fastest of all methods.

We consider in a second setting a task in which human subjects have to demonstrate how 
to search for and connect a plug to a power socket to a robot apprentice deprived of visual information. 
We take the same approach but incorporate the learning of the policy into a reinforcement learning framework 
and demonstrate that by defining a simple cost function the quality of the final learned policy can be significantly
improved without the need of performing exploratory rollouts which are costly and typically necessary in RL.

Both search tasks above can be considered as active localisation in the sense that uncertainty
originates from the position of the human or robot in the world. We now consider search setting
in which both the position of the robot and and aspects of environment are uncertain. Given the
unstructured nature of the belief a histogram parametrisation of the joint distribution over the
robot and the environment is necessary. However, naively doing so becomes quickly intractable as
the computational cost is exponential in terms of the parametrisation. We demonstrate that by
only parametrising the marginals and by memorising the parameters of the measurement likelihood
functions we can recover the exact same solution as the naive parametrisations at a cost which is
linear in space and time complexity as oppose to exponential.

\mylineskip

\noindent\textsc{\textbf{Keywords:}} Programming by Demonstration, POMDP, Reinforcement Learning, State Space Estimation (SSE)
\end{abstract}

\begin{resume}
\lettrine[lines=2]{R}{ésonner} et prendre des décisions pour résoudre des problèmes ou l'information et partielle est une difficulté  
que doit faire face tous êtres virtuels, synthétiques ou biologiques. La formulation d'un de telles problème ou l'information 
de l'espace d'état et partielle débouche sur à un nombre d'action exorbitants qui ont tous des probabilités variantes de succès. 
Ceci rend la résolution de tel problème difficile quand des méthodes de planning traditionnelles sont employées.  
Les humains ont acquis une habilitées à agir dans des situations ou l'incertitude et omniprésent à travers l'éducation 
et l'auto-apprentissage. Transférer se savoir-faire à des intelligences artificiels ou à des robots augmenteraient leurs habilités 
à résoudre des tâches qui sont partiellement spécifiée et donc l'incertitude reign.

Un grand nombre de travaux scientifiques ont mis l'accent sur le transfert de comportement humain aux robots via la programmation 
par démonstration qui est un cadre permettant au robot d'apprendre à imiter le comportement des humains. Des tâches contenants 
des éléments telles que ramasser et poser des objets, taper des balles ou de la locomotion bipède ont tous été encodés soit par 
des fonctions symboliques, statistiques ou dynamiques.  Par contre, il y a moins d'exemple de transfert de comportement cognitif 
humain de plus haut niveaux aux robots, comme les compétences en résolution de problèmes et les stratégies d'exploration.

L'objectif de ce mémoire est de créer des modèles mathématiques de la façon dont les humains raison à l'égard de l'incertitude 
présente durant des tâches d'exploration dans le domaine de la navigation spatial. Nous considérons des tâches telles que la localisation, 
dans l'obscurité total,  d'un objet dans une pièce ou encore l'établissement d'une connexion  avec une prise électrique.  Notre objective 
et de transférer les mécanismes de raisonnement de l'humain dans de tâche d'exploration spatial, sous une condition de suppression totale 
de l'information visuelle,  à un robot apprenti. Il existe de nombreux domaines d'application robotique où l'incertitude résultant d'une 
absence de perception visuelle est fréquent,  comme l'entretien des fonds marins, l'exploration planétaire et des tâches de manipulation ou 
des occlusions sont fréquents. Créer des modèles mathématiques des stratégies d'explorations d'humain et de les transférer à des robots est 
utile dans ces domaines, sans aide humain rapprendre tout notre a savoir prendrai un temps considérable. 

Une difficulté présente dans l'apprentissage de raisonnement humain, dans les scénarios de recherche que nous considérons, est que 
nos pensée et sensations (haptique et tactile) sont inobservables et varient entre les personnes. Nous déduisons les sensations perçues 
des humaines on observant leurs relations kinématiques avec une description géométrique de l'environnement qui est connue. Ou encore 
les sujets humains utiliser un outil équipé d'un capteur de force dont les mesures sont utilisées pour déduire leurs sensations.
Ces mesures sont ensuite transformées en un vecteur binaire qui est générique pour toutes les tâches que nous considérons.


Nous modélisons les pensées humaines par une fonction de densité de probabilité que nous actualisons via une estimation bayésiens récursives 
à l'aide d'un system de capture de mouvement (les sujets humains portaient des marqueurs ), et les estimations de la sensation 
ont été obtenus comme décrit dans le paragraphe précèdent. Nous faisons l'hypothèse que la fonction de densité de probabilité est actualisé par une récursion bayésien et 
similaire au processus que les humains utilise pour intègrent des informations en continue.

Pour modéliser les processus de raisonnement des sujets humains effectuant les tâches exploratoire nous apprennent une distribution conjointe des pensées et 
actions (vitesses de l'effector) qui ont été enregistrées au cours de l'exécution de la démonstration par les humains. La dimensionnalité élevée de l'état de
la pensé et de sa complexité exige que l'on comprime la compresse à un état le plus simple comme l'état le plus probable et l'entropie.
Nous évaluons cette méthodologie d'apprentissage des stratégies d'exploration dans une tâche consistant en trouver un objet sur une table. 
Nous démontrons que plusieurs stratégies d'exploration sont encodées dans la distribution pensé-action et nous comparons cette approche 
avec des algorithmes d'exploration traditionnelle de navigation. Les résultats montrent que le modèle de recherche humain est le plus 
rapide que toutes autres méthodes.

Nous considérons une deuxième tâche dans laquelle des sujets humains doivent démonter comment rechercher et connecter une fiche à une prise électrique à 
un apprenti robot privé, tous deux privés d'information visuelle. Nous prenons la même approche, mais intègre un système automatique l'apprentissage de renforcement. 
Nous démontrons que, qu'une simple fonction objective des coûts de la qualité peut significativement améliorer les capacités du robot sans la nécessité d'effectuer 
de coûteux explorations autonome qui sont le sujet de méthode traditionnel.

Les deux tâches d'exploration mentionnées ci-dessus peuvent être considérées comme des problèmes de localisation-active 
où l'incertitude et uniquement présent dans la relation entre la position de l'humain vis-à-vis le cadre de référence du monde. 
Nous considérons maintenant un problème d'exploration ou l'incertitude de se trouve à la fois dans la position du robot (ou l'humain) et dans 
des aspects de l'environnement  comme la position d'objets. Etant donné la nature non structurée de l'incertitude un histogramme est choisi 
pour paramétriser la distribution conjointe des positions du robot et le l'environnement. Cependant, cette paramétrisation devient rapidement 
intenable comme le coût de résolution devient exponentiel en termes du nombre de paramètre.  Nous démontrons que seulement quand utilisant 
seulement les paramétrer de marginales et en mémorisant la paramètres des fonctions de mesurement nous pouvons reproduire la même solution que 
la paramétrisations de l'histogram à une complexité linéaire (espace et temps) contre une exponentielle.


\mylineskip

\noindent\textsc{\textbf{Mots-clés:}} Programmation par démonstration, POMDP, Reinforcement Learning, Modèle espace d'états
\end{resume}
%


%\begin{dedication}
%\emph{Dedication\\ here}
%\end{dedication}
%\forcenewpage
%\chapter*{Acknowledgments}
%Acknowledgements here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter 0: Preamble
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{figures/ch0/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
\vspace*{-2cm}
\lettrine[lines=2]{D}{ecision} making and planning with partial state information is a problem faced 
by all forms of intelligent entities. The formulation of a problem under partial state information leads to an 
exorbitant set of choices with associated probabilistic outcomes making its resolution difficult when using traditional planning methods. 
Human beings have acquired the ability of acting under uncertainty through education and self-learning. 
Transferring our know-how to artificial agents and robots will make it faster for them to learn and even improve upon us in
tasks in which incomplete knowledge is available, which is the objective of this thesis.

We model how humans reason with respect to their beliefs and transfer this knowledge in the form of a parameterised policy, following a Programming by Demonstration
framework, to a robot apprentice for two spatial navigation tasks: the first task consists of localising a wooden block on a table and for the second task a power socket must 
be found and connected. In both tasks the human teacher and robot apprentice only rely on haptic and tactile information. 
We model the human and robot's beliefs by a probability density function which we update through recursive Bayesian state space estimation. To model the reasoning processes of human subjects performing the search tasks we learn a generative joint distribution over beliefs and actions 
(end-effector velocities) which were recorded during the executions of the task. For the first search task the direct mapping from belief to actions is learned 
whilst for the second task we incorporate a cost function used to adapt the policy parameters in a Reinforcement Learning framework and show a considerable improvement
over solely learning the behaviour with respect to the distance taken to accomplish the task.

Both search tasks above can be considered as active localisation as the uncertainty
originates only from the position of the agent in the world. We consider searches
in which both the position of the robot and features of the environment are uncertain. Given the
unstructured nature of the belief a histogram parametrisation of the joint distribution of the robots position
and features is necessary. However, naively doing so becomes quickly intractable as
the space and time complexity is exponential. We demonstrate that by only parametrising the marginals and by memorising the parameters of the measurement likelihood
functions we can recover the exact same solution as the naive parametrisations at a cost which is
linear in space and time complexity.

%We make the assumption that the probability density function, representing the human's belief, is updated by a Bayesian recursion and 
%that this process is similar to the way in which humans integrate information.
%There are many robotic application domains in which uncertainty resulting from a lack of visual perception is common, such as underwater maintenance, planetary 
% exploration and occluded manipulation tasks. Learning human search models and transferring them to robots is useful in such domains 
% and learning a search strategy from scratch would prove intractable.

%A difficulty in learning humans reasoning mechanisms, in the search scenarios we consider, is that
%the humans beliefs and sensations (haptic and tactile) are unobservable and that they vary within and
%across subjects. We infer the human sensations from either assuming kinematic relationship between tactile 
%information and known geometric description of the environment or by equipping the human subject with a 
%tool mounted with a force-torque sensor, whose measurements are used to infer the human sensations. The actual
%sensations, which are a function of either the sensor tool or kinematic-environment measurements, are
%transformed to a binary feature vector which encodes whether contact are present between features
%such as surfaces, edges and corners of the environment.

%We evaluate this methodology of learning search strategies in a task consisting of finding an object on a table. 
%We demonstrate that multiple search strategies are encoded in the joint belief-action distribution and 
%we compare this approach with greedy myopic and coastal navigation search algorithms. The results show that the human learned 
%search model is the fastest of all methods.

%We consider in a second setting a task in which human subjects have to demonstrate how 
%to search for and connect a plug to a power socket to a robot apprentice deprived of visual information. 
%We take the same approach but incorporate the learning of the policy into a reinforcement learning framework 
%and demonstrate that by defining a simple cost function the quality of the final learned policy can be significantly
%improved without the need of performing exploratory rollouts which are costly and typically necessary in RL.



\mylineskip

\noindent\textsc{\textbf{Keywords:}} Programming by Demonstration, POMDP, Reinforcement Learning, State Space Estimation (SSE)
\end{abstract}

\begin{resume}
\vspace*{-2cm}
\lettrine[lines=2]{R}{aisonner} et prendre des décisions pour résoudre des problèmes avec une information partielle est une difficulté  à laquelle doit faire face tout être intelligent. 
Les tentatives de résolution de problèmes dont l'information spatiale est partielle débouchent sur un nombre exorbitant d'actions possibles ayant chacune une probabilité de 
réussite propre. Ceci rend la résolution de tels problèmes difficile lors de l'emploi des méthodes de planning traditionnelles.

L'objectif de ce mémoire est de créer des modèles mathématiques correspondant au raisonnement humain à l'égard de l'incertitude présente durant des tâches d'exploration dans le domaine de la navigation spatial 
à l'aide du touché, c'est-à-dire sans information visuelle. Ces modèles de raisonnement sont transférés à un robot apprenti. Cette méthode évite un long apprentissage de notre savoir au robot.


Les deux tâches d'exploration mentionnées ci-dessus peuvent être considérées comme des problèmes de localisation-actif où l'incertitude est uniquement présente dans la relation entre la position de l'humain 
vis-à-vis du cadre de référence, le monde. Nous considérons maintenant un problème d'exploration où l'incertitude se trouve à la fois dans la position du robot (ou l'humain) et dans des aspects de l'environnement  
comme la position d'objets. Étant donné la nature non structurée de l'incertitude un histogramme est choisi pour paramétrer la distribution conjointe des positions du robot et de l'environnement. Cependant, cette 
stratégie devient rapidement intenable; le coût de résolution devenant exponentiel en fonction du grand nombre de paramètres. 
Nous démontrons qu'en appliquant les probabilités marginales aux paramètres des mesures, nous pouvons reproduire la solution identique de l'histogramme avec une complexité 
linéaire au lieu d'exponentielle.

\mylineskip

\noindent\textsc{\textbf{Mots-clés:}} Programmation par démonstration, POMDP, Reinforcement Learning, Modèle espace d'états
\end{resume}
%


\begin{dedication}
\emph{Dedication\\ here}
\end{dedication}
\forcenewpage
\chapter*{Acknowledgments}
Acknowledgements here




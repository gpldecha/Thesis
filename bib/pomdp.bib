% Very early work

@article{Sondik_1973,
author={Richard D. Smallwood, Edward J. Sondik},
 title = {The Optimal Control of Partially Observable Markov Processes over a Finite Horizon},
 journal = {Oper. Res.},
 issue_date = {October 1973},
 volume = {21},
 number = {5},
 month = oct,
 year = {1973},
 issn = {0030-364X},
 pages = {1071--1088},
 numpages = {18},
 url = {http://dx.doi.org/10.1287/opre.21.5.1071},
 doi = {10.1287/opre.21.5.1071},
 acmid = {2744951},
 publisher = {INFORMS},
 address = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA}
}

@article{Kaelbling_1998,
 author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
 title = {Planning and Acting in Partially Observable Stochastic Domains},
 journal = {Artif. Intell.},
 issue_date = {May, 1998},
 volume = {101},
 number = {1-2},
 month = may,
 year = {1998},
 issn = {0004-3702},
 pages = {99--134},
 numpages = {36},
 url = {http://dx.doi.org/10.1016/S0004-3702(98)00023-X},
 doi = {10.1016/S0004-3702(98)00023-X},
 acmid = {1643301},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK},
 keywords = {Partially observable Markov decision processes, Planning, Uncertainty},
} 


@INPROCEEDINGS{ActingUncertainty_1996,
author={A. R. Cassandra and L. P. Kaelbling and J. A. Kurien},
booktitle={Intelligent Robots and Systems '96, IROS 96, Proceedings of the 1996 IEEE/RSJ International Conference on},
title={Acting under uncertainty: discrete Bayesian models for mobile-robot navigation},
year={1996},
volume={2},
pages={963-972 vol.2},
month={Nov}
}

@INPROCEEDINGS{810011,
author={D. Nikovski and I. Nourbakhsh},
booktitle={Computational Intelligence in Robotics and Automation, 1999. CIRA '99. Proceedings. 1999 IEEE International Symposium on},
title={Learning discrete Bayesian models for autonomous agent navigation},
year={1999},
pages={137-143},
keywords={Bayes methods;Markov processes;decision theory;learning (artificial intelligence);mobile robots;navigation;path planning;Baum-Welch algorithm;Markov decision processes;autonomous agent;best-first model;discrete Bayesian models;mobile robots;navigation;path planning;Algorithm design and analysis;Autonomous agents;Bayesian methods;Impedance;Merging;Mobile robots;Navigation;Probability distribution;Robot sensing systems;Strips},
doi={10.1109/CIRA.1999.810011},
month={},}


@ARTICLE{Kaelbling98planningand,
    author = {Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra},
    title = {Planning and acting in partially observable stochastic domains},
    journal = {ARTIFICIAL INTELLIGENCE},
    year = {1998},
    volume = {101},
    pages = {99--134}
}


@INPROCEEDINGS{Roy99coastalnavigation,
    author = {Nicholas Roy and Sebastian Thrun},
    title = {Coastal Navigation with Mobile Robots},
    booktitle = {In Advances in Neural Processing Systems 12},
    year = {1999},
    pages = {1043--1049}
}

% Online belief space planning

@article{Ross08onlineplanning,
 author = {Ross, St{\'e}phane and Pineau, Joelle and Paquet, S{\'e}bastien and Chaib-draa, Brahim},
 title = {Online Planning Algorithms for POMDPs},
 journal = {J. Artif. Int. Res.},
 issue_date = {May 2008},
 volume = {32},
 number = {1},
 month = jul,
 year = {2008},
 issn = {1076-9757},
 pages = {663--704},
 numpages = {42},
 url = {http://dl.acm.org/citation.cfm?id=1622673.1622690},
 acmid = {1622690},
 publisher = {AI Access Foundation},
 address = {USA},
} 


% Belief space compression

@article{belief_compression_2005,
    abstract = {{Human beings take for granted their ability to move around and interact with each other in a wide range of environments.  These same environments are tremendous sources of uncertainty for mobile robots. Not only does navigation introduce positional  uncertainty, but systems that try to interact with human beings are faced with the tremendously noisy and ambiguous  behaviours that humans exhibit. More recent planners have been developed that to handle the non-deterministic and unexpected  outcomes of actions. For example, a number of probabilistic planners model the real world as a Markov process.  However, most such planners assume that the true state of the world can always be determined accurately, an assumption that  is . In an world without enough structure, tracking the current position of a robot is hard; the sensors cannot reliably track the  environment as the robot moves.  The Partially Observable Markov Decision Process (POMDP) constitutes a particular planning methodology th...}},
    author = {Roy, Nicholas},
    citeulike-article-id = {7081914},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.6180},
    journal = {Journal of Artificial Intelligence Research},
    posted-at = {2011-12-18 04:09:06},
    priority = {2},
    title = {{Finding Approximate POMDP solutions Through Belief Compression}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.6180},
    volume = {23},
    year = {2005}
}

@incollection{EPCA_2003,
title = {Exponential Family PCA for Belief Compression in POMDPs},
author = {Nicholas Roy and Gordon, Geoffrey J},
booktitle = {Advances in Neural Information Processing Systems 15},
editor = {S. Becker and S. Thrun and K. Obermayer},
pages = {1667--1674},
year = {2003},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2319-exponential-family-pca-for-belief-compression-in-pomdps.pdf}
}


@article{bs_compression_2010,
    abstract = {{Partially observable Markov decision process (POMDP) is a commonly adopted mathematical framework for solving planning problems in stochastic environments. However, computing the optimal policy of POMDP for large-scale problems is known to be intractable, where the high dimensionality of the underlying belief space is one of the major causes. In this paper, we propose a hybrid approach that integrates two different approaches for reducing the dimensionality of the belief space: 1) belief compression and 2) value-directed compression. In particular, a novel orthogonal nonnegative matrix factorization is derived for the belief compression, which is then integrated in a value-directed framework for computing the policy. In addition, with the conjecture that a properly partitioned belief space can have its per-cluster intrinsic dimension further reduced, we propose to apply a  k -means-like clustering technique to partition the belief space to form a set of sub-POMDPs before applying the dimension reduction techniques to each of them. We have evaluated the proposed belief compression and clustering approaches based on a set of benchmark problems and demonstrated their effectiveness in reducing the cost for computing policies, with the quality of the policies being retained.}},
    author = {Li, Xin and Cheung, William K. and Liu, Jiming},
    citeulike-article-id = {7081915},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tsmcb.2009.2021573},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5184876},
    doi = {10.1109/tsmcb.2009.2021573},
    issn = {1083-4419},
    journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
    keywords = {mdp, pca, statistics},
    month = feb,
    number = {1},
    pages = {125--136},
    posted-at = {2010-04-26 01:17:53},
    priority = {2},
    title = {{Improving POMDP Tractability via Belief Compression and Clustering}},
    url = {http://dx.doi.org/10.1109/tsmcb.2009.2021573},
    volume = {40},
    year = {2010}
}


% Point Based Value Iteration

@inproceedings{PBVI,
   author = "Joelle Pineau and Geoffrey Gordon and Sebastian Thrun",
   title = "Point-based value iteration: An anytime algorithm for POMDPs",
   booktitle = "International Joint Conference on Artificial Intelligence (IJCAI)",
   pages = "1025 - 1032",
   month = "August",
   year = "2003"
} 

@InProceedings{Veiga14aaai,
  author =       {Tiago S. Veiga and Matthijs T. J. Spaan and Pedro
                  U. Lima},
  title =        {Point-based {POMDP} Solving with Factored Value
                  Function Approximation},
  booktitle =    {Proceedings of the Twenty-Eighth AAAI Conference on
                  Artificial Intelligence},
  year =         2014,
  pages =        {2512--2518}
}

@INPROCEEDINGS{HSV,
 author = {Smith, Trey and Simmons, Reid},
 title = {Heuristic Search Value Iteration for POMDPs},
 booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
 series = {UAI '04},
 year = {2004},
 pages = {520--527},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
} 

@proceedings{FSVI,
  editor    = {Manuela M. Veloso},
  title     = {{IJCAI} 2007, Proceedings of the 20th International Joint Conference
               on Artificial Intelligence, Hyderabad, India, January 6-12, 2007},
  year      = {2007},
  timestamp = {Fri, 19 Dec 2008 17:01:54 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/ijcai/2007},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@INPROCEEDINGS{SARSOP,
    author = {Hanna Kurniawati and David Hsu and Wee Sun Lee},
    title = {SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces},
    booktitle = {In Proc. Robotics: Science and Systems},
    year = {2008}
}

@article{HSVI2,
  author    = {Trey Smith and
               Reid G. Simmons},
  title     = {Point-Based {POMDP} Algorithms: Improved Analysis and Implementation},
  journal   = {CoRR},
  volume    = {abs/1207.1412},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.1412},
  timestamp = {Wed, 10 Oct 2012 21:28:54 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1207-1412},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}



@INPROCEEDINGS{POMDP_approach_2010,
AUTHOR = "Y.Z. Du and D. Hsu and H. Kurniawati and W.S. Lee and S.C.W. Ong and S.W. Png",
TITLE = "A POMDP Approach to Robot Motion Planning under Uncertainty",
BOOKTITLE = "Int. Conf. on Automated Planning and Scheduling, Workshop on Solving Real-World POMDP Problems",
URL = "papers/icaps10_pomdpApsInRobotics.pdf",
YEAR = "2010"
}



@ARTICLE{continusou_PBVI,
    author = {Josep M. Porta and Nikos Vlassis and Matthijs T. J. Spaan and Pascal Poupart},
    title = {Point-Based Value Iteration for Continuous POMDPs},
    journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
    year = {2006},
    volume = {7},
    pages = {2329--2367}
}

@article{Perseus_2005,
 author = {Spaan, Matthijs T. J. and Vlassis, Nikos},
 title = {Perseus: Randomized Point-based Value Iteration for POMDPs},
 journal = {J. Artif. Int. Res.},
 issue_date = {July 2005},
 volume = {24},
 number = {1},
 month = aug,
 year = {2005},
 issn = {1076-9757},
 pages = {195--220},
 numpages = {26},
 url = {http://dl.acm.org/citation.cfm?id=1622519.1622525},
 acmid = {1622525},
 publisher = {AI Access Foundation},
 address = {USA},
} 


@article{Macro_uncertainty_2011,
 author = {He, Ruijie and Brunskill, Emma and Roy, Nicholas},
 title = {Efficient Planning Under Uncertainty with Macro-actions},
 journal = {J. Artif. Int. Res.},
 issue_date = {January 2011},
 volume = {40},
 number = {1},
 month = jan,
 year = {2011},
 issn = {1076-9757},
 pages = {523--570},
 numpages = {48},
 url = {http://dl.acm.org/citation.cfm?id=2016945.2016959},
 acmid = {2016959},
 publisher = {AI Access Foundation},
 address = {USA}
} 


@ARTICLE{vf_approx_POMDP_2000,
    author = {Milos Hauskrecht},
    title = {Value-function approximations for partially observable Markov decision processes},
    journal = {Journal of Artificial Intelligence Research},
    year = {2000},
    volume = {13},
    pages = {33--94}
}

@inproceedings{hsu:what,
    author = {Hsu, David and Lee, Wee S. and Rong, Nan},
    booktitle = {NIPS'07},
    citeulike-article-id = {9709602},
    keywords = {pomdp, reinforcement\_learning},
    posted-at = {2011-08-26 10:08:35},
    priority = {2},
    title = {{What makes some POMDP problems easy to approximate?}},
    year = {2007}
}


% Gradient 
%Particle POMDP

@incollection{PF_gradient_pomdp_2008,
    Title = {Particle Filter-based Policy Gradient in POMDPs},
    Url = {http://books.nips.cc/papers/files/nips21/NIPS2008_0628.pdf},
    Booktitle = {Advances in Neural Information Processing Systems 21},
    Author = {Pierre-arnaud Coquelin and Romain Deguest and Rémi Munos},
    Editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
    Year = {2008},
    Pages = {337--344}
} 

@INPROCEEDINGS{Baxter_GPOMDP_2000,
    author = {Jonathan Baxter and Peter L. Bartlett},
    title = {Reinforcement Learning in POMDP's via Direct Gradient Ascent},
    booktitle = {In Proc. 17th International Conf. on Machine Learning},
    year = {2000},
    pages = {41--48},
    publisher = {Morgan Kaufmann}
}



@article{REINFORCE,
year={1992},
issn={0885-6125},
journal={Machine Learning},
volume={8},
number={3-4},
doi={10.1007/BF00992696},
title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
url={http://dx.doi.org/10.1007/BF00992696},
publisher={Kluwer Academic Publishers},
keywords={Reinforcement learning; connectionist networks; gradient descent; mathematical analysis},
author={Williams, RonaldJ.},
pages={229-256},
language={English}
}

@INPROCEEDINGS{Sutton00policygradient,
    author = {Richard S. Sutton and David Mcallester and Satinder Singh and Yishay Mansour},
    title = {Policy gradient methods for reinforcement learning with function approximation},
    booktitle = {In Advances in Neural Information Processing Systems 12},
    year = {2000},
    pages = {1057--1063},
    publisher = {MIT Press}
}


@article{Peters_NN_2008,
  author =		 "Peters, J. and  Schaal, S.",
  year =		 "2008",
  title =		 "Reinforcement learning of motor skills with policy gradients",
  booktitle =		 "Neural Networks",
  key =			 "reinforcement learning, policy gradient methods, natural gradients, natural actor-critic, motor skills, motor primitives",
  URL =			 "http://www-clmc.usc.edu/publications/P/peters-NN2008.pdf",
  number =		 "4",
  pages =		 "682-97",
  ISBN/ISSN =		 "0893-6080 (Print)",
  crossref =		 "p10271"
}

% Next best touch

@INPROCEEDINGS{next_best_touch,
author={Hebert, P. and Howard, T. and Hudson, N. and Ma, J. and Burdick, J.W.},
booktitle={Robotics and Automation (ICRA), 2013 IEEE International Conference on},
title={The next best touch for model-based localization},
year={2013},
month={May},
pages={99-106},
keywords={dexterous manipulators;image sensors;mobile robots;parameter estimation;pose estimation;robot vision;touch (physiological);ARM-S;DARPA autonomous robotic manipulation software robot;autonomous robot;contact method;information gain metric;model class estimation;model parameter estimation;model-based localization;next best touch;next sensing action;object localization;object pose uncertainty;tactile method;Computational modeling;Entropy;Equations;Gain measurement;Mathematical model;Robots;Trajectory},
doi={10.1109/ICRA.2013.6630562},
ISSN={1050-4729}
}

% Probabilistic Robotics (BOOK)

@book{Thrun_2005,
 author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
 title = {Probabilistic Robotics (Intelligent Robotics and Autonomous Agents)},
 year = {2005},
 isbn = {0262201623},
 publisher = {The MIT Press},
} 


% 		Optimal control methods

@INPROCEEDINGS{Platt-RSS-10,
    AUTHOR    = {R. Platt AND R. Tedrake AND L. Kaelbling AND T. Lozano-Perez},
    TITLE     = {Belief space planning assuming maximum likelihood observations},
    BOOKTITLE = {Proceedings of Robotics: Science and Systems},
    YEAR      = {2010},
    ADDRESS   = {Zaragoza, Spain},
    MONTH     = {June}
}


 
%		Belief space planning (no control)

@INPROCEEDINGS{Quadrator_2008,
author={Ruijie He and Prentice, S. and Roy, N.},
booktitle={Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on},
title={Planning in information space for a quadrotor helicopter in a GPS-denied environment},
year={2008},
month={May},
pages={1814-1820},
keywords={Kalman filters;helicopters;path planning;GPS-denied environment;belief roadmap algorithm;global positioning;information space;localization degree;motion planning;probabilistic roadmap algorithm;quadrotor helicopter;unscented Kalman filter;Global Positioning System;Helicopters;Helium;Intelligent sensors;Land vehicles;Motion planning;Remotely operated vehicles;Space vehicles;Trajectory;Unmanned aerial vehicles},
doi={10.1109/ROBOT.2008.4543471},
ISSN={1050-4729}
}

@Article{BelRoadMap_2009,
  author = 	 {S. Prentice and N. Roy},
  title = 	 {The Belief Roadmap: Efficient Planning in Belief Space by Factoring the Covariance},
  journal = 	 {International Journal of Robotics Research},
  year = 	 {2009},
  volume =	 {8},
  number =	 {11-12},
  pages =	 {1448-1465},
  month =	 {December}
}
  



% Policy search

@article{Bayesian_explor_exploit_2009,
year={2009},
issn={0929-5593},
journal={Autonomous Robots},
volume={27},
number={2},
doi={10.1007/s10514-009-9130-2},
title={A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot},
url={http://dx.doi.org/10.1007/s10514-009-9130-2},
publisher={Springer US},
keywords={Bayesian optimization; Online path planning; Sequential experimental design; Attention and gaze planning; Active vision; Dynamic sensor networks; Active learning; Policy search; Active SLAM; Model predictive control; Reinforcement learning},
author={Martinez-Cantin, Ruben and de Freitas, Nando and Brochu, Eric and Castellanos, José and Doucet, Arnaud},
pages={93-103},
language={English}
}


@inproceedings{p_search_pomdp_1998,
 author = {Hansen, Eric A.},
 title = {Solving POMDPs by Searching in Policy Space},
 booktitle = {Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'98},
 year = {1998},
 isbn = {1-55860-555-X},
 location = {Madison, Wisconsin},
 pages = {211--219},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2074094.2074119},
 acmid = {2074119},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 


@inproceedings{Kurniawati2009MotionPU,
  title={Motion Planning under Uncertainty for Robotic Tasks with Long Time Horizons},
  author={Hanna Kurniawati and Yanzhu Du and David Hsu and Wee Sun Lee},
  booktitle={ISRR},
  year={2009}
}


@INPROCEEDINGS{MCVI,
    author = {Haoyu Bai and David Hsu and Wee Sun Lee and Vien A. Ngo},
    title = {Monte Carlo Value Iteration for Continuous-State POMDPs},
    booktitle = {WORKSHOP ON THE ALGORITHMIC FOUNDATIONS OF ROBOTICS},
    year = {2010},
    publisher = {}
}


% Sample based POMDP policy

@inproceedings{Rand_belief_space_replanning,
  added-at = {2011-09-11T00:00:00.000+0200},
  author = {Hauser, Kris},
  booktitle = {WAFR},
  crossref = {conf/wafr/2010},
  editor = {Hsu, David and Isler, Volkan and Latombe, Jean-Claude and Lin, Ming C.},
  ee = {http://dx.doi.org/10.1007/978-3-642-17452-0_12},
  isbn = {978-3-642-17451-3},
  pages = {193-209},
  publisher = {Springer},
  series = {Springer Tracts in Advanced Robotics},
  title = {Randomized Belief-Space Replanning in Partially-Observable Continuous Spaces.},
  volume = 68,
  year = 2010
}


% Pursuit evasion in a multi robot domain

@INPROCEEDINGS{Pursuit_evasion_2004,
    author = {Brian P. Gerkey and Sebastian Thrun and Geoff Gordon},
    title = {Visibility-based pursuit-evasion with limited field of view},
    booktitle = {International Journal of Robotics Research},
    year = {2004},
    pages = {20--27}
}



% Reward functions

@INPROCEEDINGS{Mihaylova02activesensing,
    author = {L. Mihaylova and T. Lefebvre and H. Bruyninckx and K. Gadeyne and J. De Schutter},
    title = {Active Sensing for Robotics - A Survey},
    booktitle = {in Proc. 5 th Int’l Conf. On Numerical Methods and Applications},
    year = {2002},
    pages = {316--324}
}


% deChambrierBillard2014

@article{deChambrierBillard2014,
year={2014},
journal={Robotics and Biomimetics},
volume={1},
number={1},
title={Learning search polices from humans in a partially observable context},
publisher={Springer Berlin Heidelberg},
author={Chambrier, Guillaumede and Billard, Aude}
}

@proceedings{PbD_Sylvain_static_representation,
    author = {Calinon, S. and Guenter, F. and Billard, A.},
    citeulike-article-id = {1222185},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/robot.2006.1642154},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1642154},
    doi = {10.1109/robot.2006.1642154},
    journal = {Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE International Conference on},
    keywords = {algorithms, learning-by-demo},
    pages = {2978--2983},
    posted-at = {2009-03-27 15:55:39},
    priority = {5},
    title = {{On learning the statistical representation of a task and generalizing it to various contexts}},
    url = {http://dx.doi.org/10.1109/robot.2006.1642154},
    year = {2006}
}


% function approximation 


@INPROCEEDINGS{Gordon95stablefunction,
    author = {Geoffrey J. Gordon},
    title = {Stable Function Approximation in Dynamic Programming},
    booktitle = {IN MACHINE LEARNING: PROCEEDINGS OF THE TWELFTH INTERNATIONAL CONFERENCE},
    year = {1995},
    publisher = {Morgan Kaufmann}
}

@book{VonNeumann1944,
    author = "Von Neumann, John and Morgenstern, O.",
    title = "The theory of games and economic behavior",
    year = "1990",
    publisher = "Princeton",
    edition = "3"
}

@article{Bernoulli1954,
  author = {Bernoulli, D.},
  journal = {Econometrica},
  number = {1},
  pages = {23--36},
  title = {{Exposition of a New Theory on the Measurement of Risk (1748)}},
  volume = {22},
  year = {1954}
}

@TechReport{Ellsberg_paradox,
  author={Daniel Ellsberg},
  title={{Risk, Ambiguity and the Savage Axioms}},
  year=2000,
  month=Dec,
  institution={David K. Levine},
  type={Levine's Working Paper Archive},
  url={http://ideas.repec.org/p/cla/levarc/7605.html},
  number={7605},
  abstract={No abstract is available for this item.},
  keywords={},
}


@article{MACHINA01051987,
author = {MACHINA, MARK J.}, 
title = {Decision-Making in the Presence of Risk},
volume = {236}, 
number = {4801}, 
pages = {537-543}, 
year = {1987}, 
doi = {10.1126/science.236.4801.537}, 
abstract ={Proposed in the 18th century by Cramer and Bernoulli and formally axiomatized in the 20th century by von Neumann and Morgenstern and others, the expected utility model has long been the dominant framework for the analysis of decision-making under risk. A growing body of experimental evidence, however, indicates that individuals systematically violate the key behavioral assumption of this model, the so-called independence axiom. This has led to the development and analysis of nonexpected utility models of decision-making. Although recent work in this area has shown that the analytical results of expected utility theory are more robust than previously supposed, other important issues remain unresolved.}, 
URL = {http://www.sciencemag.org/content/236/4801/537.abstract}, 
eprint = {http://www.sciencemag.org/content/236/4801/537.full.pdf}, 
journal = {Science} 
}

@incollection{Billard08chapter,
author="A. Billard and S. Calinon and R. Dillmann and S. Schaal",
title="Robot Programming by Demonstration",
booktitle="Handbook of Robotics",
editor="B. Siciliano and O. Khatib",
publisher="Springer",
address="Secaucus, NJ, USA",
year="2008",
pages="1371--1394"
}

@inproceedings{rai2013learning,
  title={Learning from failed demonstrations in unreliable systems},
  author={Rai, Akshara and De Chambrier, Guillaume and Billard, Aude},
  booktitle={Humanoid Robots (Humanoids), 2013 13th IEEE-RAS International Conference on},
  pages={410--416},
  year={2013},
  organization={IEEE}
}


% Active Exploration and planning 

@INPROCEEDINGS{active_RSS_07,
    AUTHOR    = {R. Martinez-Cantin and N. de Freitas and A. Doucet and J. Castellanos},
    TITLE     = {Active Policy Learning for Robot Planning and Exploration under Uncertainty},
    BOOKTITLE = {Proceedings of Robotics: Science and Systems},
    YEAR      = {2007},
    ADDRESS   = {Atlanta, GA, USA},
    MONTH     = {June},
    DOI       = {10.15607/RSS.2007.III.041} 
}




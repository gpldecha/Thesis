%% Book


% Sutton

@Book{sutton98a,
  author =	 {Richard S. Sutton and Andrew G. Barto},
  title =	 {Reinforcement Learning: An Introduction},
  publisher =	 {{MIT} Press},
  year =	 1998,
  url = 	 {http://www.cs.ualberta.ca/\%7Esutton/book/ebook/the-book.html}
}


@article{Tree_batch_2005,
 author = {Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
 title = {Tree-Based Batch Mode Reinforcement Learning},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2005},
 volume = {6},
 month = dec,
 year = {2005},
 issn = {1532-4435},
 pages = {503--556},
 numpages = {54},
 url = {http://dl.acm.org/citation.cfm?id=1046920.1088690},
 acmid = {1088690},
 publisher = {JMLR.org}
} 

@incollection{heli_2004,
title = {Autonomous Helicopter Flight via Reinforcement Learning},
author = {H. J. Kim and Michael I. Jordan and Shankar Sastry and Andrew Y. Ng},
booktitle = {Advances in Neural Information Processing Systems 16},
editor = {S. Thrun and L. K. Saul and B. Sch\"{o}lkopf},
pages = {799--806},
year = {2004},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2455-autonomous-helicopter-flight-via-reinforcement-learning.pdf}
}

@article{RL_robots_surv_2013,
   author = "J. Kober and J. Andrew (Drew) Bagnell and J. Peters",
   title = "Reinforcement Learning in Robotics: A Survey",
   journal = "International Journal of Robotics Research",
   month = "July",
   year = "2013",
   Notes = "accepted"
} 

% Important concepts

@INPROCEEDINGS{Safe_val_function_1995,
    author = {Justin A. Boyan and Andrew W. Moore},
    title = {Generalization in Reinforcement Learning: Safely Approximating the Value Function},
    booktitle = {Advances in Neural Information Processing Systems 7},
    year = {1995},
    pages = {369--376},
    publisher = {MIT Press}
}


% Gradient policy search

﻿@Article{reinforce_1992,
author="Williams, Ronald J.",
title="Simple statistical gradient-following algorithms for connectionist reinforcement learning",
journal="Machine Learning",
volume="8",
number="3",
pages="229--256",
year="1992",
doi="10.1007/BF00992696",
url="http://dx.doi.org/10.1007/BF00992696"
}

@INPROCEEDINGS{gpomdp_2000,
    author = {Jonathan Baxter and Peter L. Bartlett},
    title = {Reinforcement Learning in POMDP's via Direct Gradient Ascent},
    booktitle = {In Proc. 17th International Conf. on Machine Learning},
    year = {2000},
    pages = {41--48},
    publisher = {Morgan Kaufmann}
}

@INPROCEEDINGS{eNAC_2003,
    author = {Sethu Vijayakumar and Tomohiro Shibata and Stefan Schaal},
    title = {Reinforcement learning for humanoid robotics},
    booktitle = {Autonomous Robot},
    year = {2003},
    pages = {2002}
}

@inproceedings{pancake_2010,
  address = {Taipei, Taiwan},
  author = {Kormushev, P. and Calinon, S. and Caldwell, D. G.},
  booktitle = {Proc. {IEEE/RSJ} Intl Conf. on Intelligent Robots and Systems ({IROS})},
  month = {October},
  pages = {3232--3237},
  title = {Robot Motor Skill Coordination with {EM}-based Reinforcement Learning},
  year = 2010
}

﻿@Article{Wang2016,
author="Wang, Jiexin and Uchibe, Eiji and Doya, Kenji",
title="EM-based policy hyper parameter exploration: application to standing and balancing of a two-wheeled smartphone robot",
journal="Artificial Life and Robotics",
year="2016",
volume="21",
number="1",
pages="125--131",
doi="10.1007/s10015-015-0260-7",
url="http://dx.doi.org/10.1007/s10015-015-0260-7"
}



@INPROCEEDINGS{PoWER_2009,
	author={J. Kober and J. Peters},
	booktitle={Robotics and Automation, 2009. ICRA '09. IEEE International Conference on},
	title={Learning motor primitives for robotics},
	year={2009},
pages={2112-2118},
keywords={Anthropomorphism;Cybernetics;Humans;Intelligent robots;Machine learning;Machine learning algorithms;Robot programming;Robotics and automation;Service robots;Stability},
doi={10.1109/ROBOT.2009.5152577},
ISSN={1050-4729},
month={May}
}

@INPROCEEDINGS{archery_2010,
author={P. Kormushev and S. Calinon and R. Saegusa and G. Metta},
booktitle={Humanoid Robots (Humanoids), 2010 10th IEEE-RAS International Conference on},
title={Learning the skill of archery by a humanoid robot iCub},
year={2010},
pages={417-423},
doi={10.1109/ICHR.2010.5686841},
month={Dec}
}

@article{NAC_2008,
title = "Natural Actor-Critic ",
journal = "Neurocomputing ",
volume = "71",
number = "7–9",
pages = "1180 - 1190",
year = "2008",
note = "Progress in Modeling, Theory, and Application of Computational Intelligenc15th European Symposium on Artificial Neural Networks 200715th European Symposium on Artificial Neural Networks 2007 ",
issn = "0925-2312",
doi = "http://dx.doi.org/10.1016/j.neucom.2007.11.026",
url = "http://www.sciencedirect.com/science/article/pii/S0925231208000532",
author = "Jan Peters and Stefan Schaal",
}


@INPROCEEDINGS{dmp_iros_2011,
author={F. Stulp and E. Theodorou and M. Kalakrishnan and P. Pastor and L. Righetti and S. Schaal},
booktitle={Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on},
title={Learning motion primitive goals for robust manipulation},
year={2011},
pages={325-331},
keywords={Cost function;Grasping;Learning;Robots;Shape;Trajectory;Uncertainty},
doi={10.1109/IROS.2011.6094877},
ISSN={2153-0858},
month={Sept}
}

@INPROCEEDINGS{sigma_hull_iros_2013,
author={A. Lee and Y. Duan and S. Patil and J. Schulman and Z. McCarthy and J. van den Berg and K. Goldberg and P. Abbeel},
booktitle={Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on},
title={Sigma hulls for Gaussian belief space planning for imprecise articulated robots amid obstacles},
year={2013},
pages={5660-5667},
doi={10.1109/IROS.2013.6697176},
ISSN={2153-0858},
month={Nov}
}


% Survey

@article{p_search_surv_2011,
url = {http://dx.doi.org/10.1561/2300000021},
year = {2011},
volume = {2},
journal = {Foundations and Trends® in Robotics},
title = {A Survey on Policy Search for Robotics},
doi = {10.1561/2300000021},
issn = {1935-8253},
number = {1–2},
pages = {1-142},
author = {Marc Peter Deisenroth and Gerhard Neumann and Jan Peters}
}




% Fitted Reinforcement Learning




@inproceedings{fqi_nips_peter_2009,
  title = {Fitted Q-iteration by Advantage Weighted Regression},
  author = {Neumann, G. and Peters, J.},
  journal = {Advances in neural information processing systems 21 : 22nd Annual Conference on Neural Information Processing Systems 2008},
  booktitle = {Advances in neural information processing systems 21},
  pages = {1177-1184},
  editors = {Koller, D. , D. Schuurmans, Y. Bengio, L. Bottou},
  publisher = {Curran},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  address = {Red Hook, NY, USA},
  month = jun,
  year = {2009}
}

@article{batch_synth_traj_2013,
  author    = {Raphael Fonteneau and
               Susan A. Murphy and
               Louis Wehenkel and
               Damien Ernst},
  title     = {Batch mode reinforcement learning based on the synthesis of artificial
               trajectories},
  journal   = {Annals {OR}},
  volume    = {208},
  number    = {1},
  pages     = {383--416},
  year      = {2013},
  url       = {http://dx.doi.org/10.1007/s10479-012-1248-5},
  doi       = {10.1007/s10479-012-1248-5},
  timestamp = {Tue, 27 Aug 2013 01:00:00 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/anor/FonteneauMWE13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@Inbook{fnac_ca_2008,
author="Melo, Francisco S.
and Lopes, Manuel",
editor="Daelemans, Walter
and Goethals, Bart
and Morik, Katharina",
title="Fitted Natural Actor-Critic: A New Algorithm for Continuous State-Action MDPs",
bookTitle="Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2008, Antwerp, Belgium, September 15-19, 2008, Proceedings, Part II",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="66--81",
isbn="978-3-540-87481-2",
doi="10.1007/978-3-540-87481-2_5",
url="http://dx.doi.org/10.1007/978-3-540-87481-2_5"
}

@Inbook{Riedmiller2005,
author="Riedmiller, Martin",
editor="Gama, Jo{\~a}o
and Camacho, Rui
and Brazdil, Pavel B.
and Jorge, Al{\'i}pio M{\'a}rio
and Torgo, Lu{\'i}s",
title="Neural Fitted Q Iteration -- First Experiences with a Data Efficient Neural Reinforcement Learning Method",
bookTitle="Machine Learning: ECML 2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005. Proceedings",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="317--328",
isbn="978-3-540-31692-3",
doi="10.1007/11564096_32",
url="http://dx.doi.org/10.1007/11564096_32"
}

@Article{EGW05,
  author       = "Ernst, Damien and Geurts, Pierre and Wehenkel, Louis",
  title        = "Tree-Based Batch Mode Reinforcement Learning",
  journal      = "Journal of Machine Learning Research",
  volume       = "6",
  pages        = "503-556",
  month        = "April",
  year         = "2005",
  keywords     = "optimisation, machine learning",
 }

@INPROCEEDINGS{rl_gmm_2010,
author={A. Agostini and E. Celaya},
booktitle={The 2010 International Joint Conference on Neural Networks (IJCNN)},
title={Reinforcement Learning with a Gaussian mixture model},
year={2010},
pages={1-8},
keywords={Gaussian processes;function approximation;iterative methods;learning (artificial intelligence);probability;Gaussian mixture model;Gaussian processes;batch iterative process;fitted value iteration algorithms;function approximation;neural fitted Q iteration;probability density estimations;reinforcement learning;Approximation algorithms;Function approximation;Learning;Maximum likelihood estimation;Probability density function},
doi={10.1109/IJCNN.2010.5596306},
ISSN={2161-4393},
month={July},}

@INPROCEEDINGS{fvi_uav_2010,
author={H. Bou-Ammar and H. Voos and W. Ertel},
booktitle={2010 IEEE International Conference on Control Applications},
title={Controller design for quadrotor UAVs using reinforcement learning},
year={2010},
pages={2130-2135},
keywords={aerospace robotics;aircraft control;feedback;learning systems;mobile robots;nonlinear control systems;remotely operated vehicles;stability;unsupervised learning;feedback linearization;fitted value iteration;nonlinear autopilot;nonlinear control design;nonlinear dynamic behavior;quadrotor UAV;reinforcement learning;small unmanned aerial vehicles;stabilizing control;unsupervised learning;Control systems;Input variables;Learning;Mathematical model;Rotors;Unmanned aerial vehicles},
doi={10.1109/CCA.2010.5611206},
ISSN={1085-1992},
month={Sept},}


@ARTICLE{rl_ac_surv_2012,
author={I. Grondman and L. Busoniu and G. A. D. Lopes and R. Babuska},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
title={A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients},
year={2012},
volume={42},
number={6},
pages={1291-1307},
keywords={function approximation;gradient methods;learning (artificial intelligence);RL;actor-critic reinforcement learning;low-variance gradient estimation;natural policy gradients;optimal policies;policy-gradient-based actor-critic algorithms;real-life applications;standard policy gradients;Approximation algorithms;Approximation methods;Convergence;Equations;Optimization;Standards;Actor-critic;natural gradient;policy gradient;reinforcement learning (RL)},
doi={10.1109/TSMCC.2012.2218595},
ISSN={1094-6977},
month={Nov}
}

@ARTICLE{kernel_rl_ormoneit_2002,
author={D. Ormoneit and P. Glynn},
journal={IEEE Transactions on Automatic Control},
title={Kernel-based reinforcement learning in average-cost problems},
year={2002},
volume={47},
number={10},
pages={1624-1636},
keywords={Markov processes;decision theory;dynamic programming;iterative methods;learning (artificial intelligence);probability;Markov decision processes;average-cost problems;grid-based approximations;kernel-based reinforcement learning;local averaging methods;nearest-neighbor regression;optimal controls;trees;unique approximation;Convergence;Dynamic programming;Heuristic algorithms;Kernel;Learning;Neural networks;Optimal control;Stability;Table lookup;Training data},
doi={10.1109/TAC.2002.803530},
ISSN={0018-9286},
month={Oct}}

@article{kernel_BarretoPP14,
  author    = {Andr{\'{e}} da Motta Salles Barreto and
               Doina Precup and
               Joelle Pineau},
  title     = {Practical Kernel-Based Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1407.5358},
  year      = {2014},
  url       = {http://arxiv.org/abs/1407.5358},
  timestamp = {Fri, 01 Aug 2014 13:50:01 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BarretoPP14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@techreport{stable_FA_gordon_1995,
 author = {Gordon, Geoffrey J.},
 title = {Stable Function Approximation in Dynamic Programming},
 year = {1995},
 publisher = {Carnegie Mellon University},
 address = {Pittsburgh, PA, USA}
} 


@CONFERENCE{ACML_variance_2015,
  author = 	 {Zhao, T. and Niu, G. and Xie, N. and Yang, J. and Sugiyama, M.},
  title       = {Regularized Policy Gradients: {D}irect Variance Reduction in Policy Gradient Estimation},
  booktitle   = {Proceedings of the Fourth Asian Conference on Machine Learning (ACML2015)},
  year        = {2015},
  editor      = {},
  series      = {JMLR Workshop and Conference Proceedings},
  month       = {Nov.~20-22},
  address     = {Hong Kong, China},
  volume      = {45},
 pages        = {333--348}
}


%% DEEP LEARNING

@INPROCEEDINGS{Lange_riedmiller_2010,
author={S. Lange and M. Riedmiller},
booktitle={The 2010 International Joint Conference on Neural Networks (IJCNN)},
title={Deep auto-encoder neural networks in reinforcement learning},
year={2010},
pages={1-8},
keywords={learning (artificial intelligence);neural nets;batch-mode RL algorithm;data efficiency;deep autoencoder neural network;feature spaces;task dependent information;visual reinforcement learning;Approximation methods;Feature extraction;Image reconstruction;Noise measurement;Principal component analysis;Training;Visualization},
doi={10.1109/IJCNN.2010.5596468},
ISSN={2161-4393},
month={July}
}

%% BOOK

@book{RL_book_2010,
  author = {Szepesvári, Csaba},
  booktitle = {Algorithms for Reinforcement Learning},
  publisher = {Morgan and Claypool Publishers},
  series = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  title = {Algorithms for Reinforcement Learning},
  url = {http://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009},
  year = 2010
}


@book{RL_state_art_2012,
  author = {Wiering, Marco and van Otterio, Martijn},
  booktitle = {Reinforcement Learning State-of-the-Art},
  publisher = {Springer-Verlag Berlin Heidelberg},
  title = {Reinforcement Learning State-of-the-Art},
  year = 2012
}





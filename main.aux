\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{DARPA_2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}}
\@writefile{brf}{\backcite{DARPA_2015}{{1}{1.1}{section.1.1}}}
\citation{decision_un_2013}
\citation{ActingUncertainty_1996}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of the decision making under uncertainty in both robotics and everyday life situations. (a) European Space Agency (ESA), remote orbital peg in hole task. (b)-(c) ESA, simulated exploration of a cave on Mars in the dark. (d)-(e) MIT DAC team, Atlas robot doing valve task, \url  {http://drc.mit.edu/}. Other pictures include underwater exploration and industrial peg-in-hole assembly.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ch1-example}{{1.1}{2}{Examples of the decision making under uncertainty in both robotics and everyday life situations. (a) European Space Agency (ESA), remote orbital peg in hole task. (b)-(c) ESA, simulated exploration of a cave on Mars in the dark. (d)-(e) MIT DAC team, Atlas robot doing valve task, \url {http://drc.mit.edu/}. Other pictures include underwater exploration and industrial peg-in-hole assembly.\relax }{figure.caption.1}{}}
\@writefile{brf}{\backcite{decision_un_2013}{{2}{1.1}{figure.caption.1}}}
\@writefile{brf}{\backcite{ActingUncertainty_1996}{{2}{1.1}{figure.caption.1}}}
\citation{stankiewicz2006lost}
\citation{Billard08chapter}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{3}{1.1}{figure.caption.1}}}
\@writefile{brf}{\backcite{Billard08chapter}{{3}{1.1}{figure.caption.1}}}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contribution}{4}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Learning to reason with uncertainty as humans}{4}{subsection.1.2.1}}
\newlabel{sub:contr1}{{1.2.1}{4}{Learning to reason with uncertainty as humans}{subsection.1.2.1}{}}
\citation{rai2013learning}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{5}{1.2.1}{subsection.1.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Reinforcement learning in belief space}{5}{subsection.1.2.2}}
\newlabel{sub:contr2}{{1.2.2}{5}{Reinforcement learning in belief space}{subsection.1.2.2}{}}
\@writefile{brf}{\backcite{rai2013learning}{{5}{1.2.2}{subsection.1.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Non-parametric Bayesian state space filter}{6}{subsection.1.2.3}}
\newlabel{sub:contr3}{{1.2.3}{6}{Non-parametric Bayesian state space filter}{subsection.1.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Thesis outline}{6}{section.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Roadmap of the Thesis with key points. \relax }}{7}{figure.caption.2}}
\newlabel{fig:rmap_thesis}{{1.2}{7}{Roadmap of the Thesis with key points. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Chapter outline.\relax }}{9}{figure.caption.3}}
\newlabel{fig:ch2_outline}{{2.1}{9}{Chapter outline.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Decisions under uncertainty}{10}{section.2.1}}
\newlabel{sec:deci_un}{{2.1}{10}{Decisions under uncertainty}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Relation between beliefs, desires and actions and are all considered to be rational.\relax }}{11}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Decision theory}{11}{subsection.2.1.1}}
\newlabel{sec:ch2_DT}{{2.1.1}{11}{Decision theory}{subsection.2.1.1}{}}
\citation{Bernoulli1954}
\citation{VonNeumann1944}
\@writefile{brf}{\backcite{Bernoulli1954}{{12}{2.1.1}{figure.caption.4}}}
\newlabel{eq:exp_utility}{{2.1.1}{12}{Decision theory}{figure.caption.4}{}}
\@writefile{brf}{\backcite{VonNeumann1944}{{12}{2.1.1}{figure.caption.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sequential decision making}{13}{section.2.2}}
\newlabel{sec:sqp}{{2.2}{13}{Sequential decision making}{section.2.2}{}}
\newlabel{eq:joint_state_actions_util}{{2.1}{13}{Sequential decision making}{equation.2.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Definition of common variables used.\relax }}{14}{table.caption.5}}
\newlabel{tab:notation}{{2.1}{14}{Definition of common variables used.\relax }{table.caption.5}{}}
\newlabel{fig:mdp_off}{{2.3(a)}{15}{Subfigure 2 2.3(a)}{subfigure.2.3.1}{}}
\newlabel{sub@fig:mdp_off}{{(a)}{15}{Subfigure 2 2.3(a)\relax }{subfigure.2.3.1}{}}
\newlabel{fig:mdp_on}{{2.3(b)}{15}{Subfigure 2 2.3(b)}{subfigure.2.3.2}{}}
\newlabel{sub@fig:mdp_on}{{(b)}{15}{Subfigure 2 2.3(b)\relax }{subfigure.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf  {(a)} the decision nodes are not considered random variables whilst in \textbf  {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }}{15}{figure.caption.6}}
\newlabel{fig:mdp}{{2.3}{15}{Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf {(a)} the decision nodes are not considered random variables whilst in \textbf {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {off-policy}}}{15}{figure.caption.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {on-policy}}}{15}{figure.caption.6}}
\newlabel{eq:joint_state_actions}{{2.2}{16}{Sequential decision making}{equation.2.2.2}{}}
\newlabel{eq:temporal_expected_utility}{{2.4}{16}{Sequential decision making}{equation.2.2.4}{}}
\newlabel{eq:expansion}{{2.5}{16}{Sequential decision making}{equation.2.2.5}{}}
\newlabel{eq:bellman}{{2.6}{16}{Sequential decision making}{equation.2.2.6}{}}
\newlabel{eq:on_policy_bellman}{{2.7}{17}{Sequential decision making}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}POMDP}{17}{subsection.2.2.1}}
\newlabel{eq:sensor}{{2.8}{18}{POMDP}{equation.2.2.8}{}}
\newlabel{eq:likelihood}{{2.9}{18}{POMDP}{equation.2.2.9}{}}
\newlabel{fig:motion_update}{{2.4(a)}{19}{Subfigure 2 2.4(a)}{subfigure.2.4.1}{}}
\newlabel{sub@fig:motion_update}{{(a)}{19}{Subfigure 2 2.4(a)\relax }{subfigure.2.4.1}{}}
\newlabel{fig:measurement}{{2.4(b)}{19}{Subfigure 2 2.4(b)}{subfigure.2.4.2}{}}
\newlabel{sub@fig:measurement}{{(b)}{19}{Subfigure 2 2.4(b)\relax }{subfigure.2.4.2}{}}
\newlabel{fig:likelihood}{{2.4(c)}{19}{Subfigure 2 2.4(c)}{subfigure.2.4.3}{}}
\newlabel{sub@fig:likelihood}{{(c)}{19}{Subfigure 2 2.4(c)\relax }{subfigure.2.4.3}{}}
\newlabel{fig:measurement_update}{{2.4(d)}{19}{Subfigure 2 2.4(d)}{subfigure.2.4.4}{}}
\newlabel{sub@fig:measurement_update}{{(d)}{19}{Subfigure 2 2.4(d)\relax }{subfigure.2.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \textbf  {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf  {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf  {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf  {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }}{19}{figure.caption.7}}
\newlabel{fig:belief_update_example}{{2.4}{19}{\textbf {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{19}{figure.caption.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{19}{figure.caption.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{19}{figure.caption.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{19}{figure.caption.7}}
\newlabel{eq:motion_update}{{2.10}{20}{POMDP}{equation.2.2.10}{}}
\newlabel{eq:measurement_update}{{2.11}{20}{POMDP}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Bayesian state space filter.\relax }}{20}{figure.caption.8}}
\newlabel{fig:sub_pomdp}{{2.6(a)}{21}{Subfigure 2 2.6(a)}{subfigure.2.6.1}{}}
\newlabel{sub@fig:sub_pomdp}{{(a)}{21}{Subfigure 2 2.6(a)\relax }{subfigure.2.6.1}{}}
\newlabel{fig:sub_bmdp}{{2.6(b)}{21}{Subfigure 2 2.6(b)}{subfigure.2.6.2}{}}
\newlabel{sub@fig:sub_bmdp}{{(b)}{21}{Subfigure 2 2.6(b)\relax }{subfigure.2.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \textbf  {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf  {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }}{21}{figure.caption.9}}
\newlabel{fig:pomdp}{{2.6}{21}{\textbf {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{21}{figure.caption.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{21}{figure.caption.9}}
\newlabel{eq:belief_bellman}{{2.15}{21}{POMDP}{equation.2.2.15}{}}
\citation{Sondik_1973}
\citation{Thrun_2005}
\citation{Kaelbling_1998}
\newlabel{eq:belief_state_transformation}{{2.16}{22}{POMDP}{equation.2.2.16}{}}
\newlabel{eq:max_component}{{2.17}{22}{POMDP}{equation.2.2.17}{}}
\newlabel{eq:final_belief_bellman}{{2.18}{22}{POMDP}{equation.2.2.18}{}}
\@writefile{brf}{\backcite{Sondik_1973}{{22}{2.2.1}{equation.2.2.18}}}
\@writefile{brf}{\backcite{Thrun_2005}{{22}{2.2.1}{equation.2.2.18}}}
\@writefile{brf}{\backcite{Kaelbling_1998}{{22}{2.2.1}{equation.2.2.18}}}
\citation{POMDP_approach_2010}
\citation{Thrun_2005}
\citation{PBVI}
\citation{HSV}
\citation{HSVI2}
\citation{FSVI}
\citation{SARSOP}
\citation{POMDP_approach_2010}
\@writefile{brf}{\backcite{POMDP_approach_2010}{{23}{2.2.1}{equation.2.2.18}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Literature review}{23}{section.2.3}}
\newlabel{sec:lit_rev}{{2.3}{23}{Literature review}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Value Iteration}{23}{subsection.2.3.1}}
\newlabel{lit:VI}{{2.3.1}{23}{Value Iteration}{subsection.2.3.1}{}}
\@writefile{brf}{\backcite{Thrun_2005}{{23}{2.3.1}{subsection.2.3.1}}}
\@writefile{toc}{\contentsline {subsubsection}{Point-base Value Iteration}{23}{section*.11}}
\@writefile{brf}{\backcite{PBVI}{{23}{2.3.1}{section*.11}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Mind-map of AI and robotic methods for acting under uncertainty.\relax }}{24}{figure.caption.10}}
\newlabel{fig:mindmap}{{2.7}{24}{Mind-map of AI and robotic methods for acting under uncertainty.\relax }{figure.caption.10}{}}
\citation{Spaan05icra}
\citation{PBVI_C_2006}
\citation{solving_continous_pomdps_2013}
\@writefile{brf}{\backcite{HSV}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{HSVI2}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{FSVI}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{SARSOP}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{POMDP_approach_2010}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{Spaan05icra}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{PBVI_C_2006}{{25}{2.3.1}{section*.11}}}
\citation{Ross08onlineplanning}
\citation{MC-POMDP}
\citation{Tree_batch_2005}
\@writefile{brf}{\backcite{solving_continous_pomdps_2013}{{26}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{Ross08onlineplanning}{{26}{2.3.1}{section*.11}}}
\@writefile{toc}{\contentsline {subsubsection}{Approximate Value Iteration}{26}{section*.12}}
\@writefile{brf}{\backcite{MC-POMDP}{{26}{2.3.1}{section*.12}}}
\citation{mc_update_ppomdps}
\citation{neura_fqi_2005}
\citation{DRQ_AAAI_2015}
\citation{mnih-dqn-2015}
\citation{Roy99coastalnavigation}
\@writefile{brf}{\backcite{Tree_batch_2005}{{27}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{mc_update_ppomdps}{{27}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{neura_fqi_2005}{{27}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{DRQ_AAAI_2015}{{27}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{mnih-dqn-2015}{{27}{2.3.1}{section*.12}}}
\@writefile{toc}{\contentsline {subsubsection}{Latent Value Iteration}{27}{section*.13}}
\citation{belief_compression_2005}
\citation{EPCA_2003}
\citation{bs_compression_2010}
\@writefile{brf}{\backcite{Roy99coastalnavigation}{{28}{2.3.1}{section*.13}}}
\@writefile{brf}{\backcite{belief_compression_2005}{{28}{2.3.1}{section*.13}}}
\@writefile{brf}{\backcite{EPCA_2003}{{28}{2.3.1}{section*.13}}}
\@writefile{brf}{\backcite{bs_compression_2010}{{28}{2.3.1}{section*.13}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: Value Iteration}{28}{section*.14}}
\citation{gpomdp_2000}
\citation{reinforce_1992}
\citation{gpomdp_2000}
\citation{sis_pomdp_2002}
\citation{Pegasus_2000}
\citation{heli_2004}
\citation{dmp_iros_2011}
\citation{dmp_seq_2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Policy search}{29}{subsection.2.3.2}}
\newlabel{lit:policy_search}{{2.3.2}{29}{Policy search}{subsection.2.3.2}{}}
\@writefile{brf}{\backcite{gpomdp_2000}{{29}{2.3.2}{subsection.2.3.2}}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient: policy search}{29}{section*.15}}
\@writefile{brf}{\backcite{reinforce_1992}{{29}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{gpomdp_2000}{{29}{2.3.2}{section*.15}}}
\citation{PoWER_2009}
\citation{archery_2010}
\citation{pancake_2010}
\citation{Wang2016}
\citation{p_search_surv_2011}
\citation{RL_robots_surv_2013}
\citation{ac_survey_2012}
\citation{eNAC_2003}
\citation{NAC_2008}
\@writefile{brf}{\backcite{sis_pomdp_2002}{{30}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{Pegasus_2000}{{30}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{heli_2004}{{30}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{dmp_iros_2011}{{30}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{dmp_seq_2012}{{30}{2.3.2}{section*.15}}}
\@writefile{toc}{\contentsline {subsubsection}{Expectation-Maximisation: policy search}{30}{section*.16}}
\@writefile{brf}{\backcite{PoWER_2009}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{archery_2010}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{pancake_2010}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{Wang2016}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{p_search_surv_2011}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{RL_robots_surv_2013}{{30}{2.3.2}{section*.16}}}
\@writefile{toc}{\contentsline {subsubsection}{Actor-critic: policy search}{30}{section*.17}}
\@writefile{brf}{\backcite{ac_survey_2012}{{31}{2.3.2}{section*.17}}}
\@writefile{brf}{\backcite{eNAC_2003}{{31}{2.3.2}{section*.17}}}
\@writefile{brf}{\backcite{NAC_2008}{{31}{2.3.2}{section*.17}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: policy search}{31}{section*.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Planning}{31}{subsection.2.3.3}}
\newlabel{lit:Planning}{{2.3.3}{31}{Planning}{subsection.2.3.3}{}}
\citation{BelRoadMap_2009}
\citation{Quadrator_2008}
\citation{FIRM_2011}
\citation{rob_online_bs_icra_2014}
\citation{bsp_rss_2010a}
\citation{LQG_MP_2011}
\citation{Erez10ascalable}
\citation{van_den_Berg_2012}
\citation{Needle_2014}
\@writefile{toc}{\contentsline {subsubsection}{Belief space road maps}{32}{section*.19}}
\@writefile{brf}{\backcite{BelRoadMap_2009}{{32}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{Quadrator_2008}{{32}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{FIRM_2011}{{32}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{rob_online_bs_icra_2014}{{32}{2.3.3}{section*.19}}}
\@writefile{toc}{\contentsline {subsubsection}{Optimal control}{32}{section*.20}}
\@writefile{brf}{\backcite{bsp_rss_2010a}{{32}{2.3.3}{section*.20}}}
\citation{non_gauss_bel_plan_2012}
\citation{seq_traj_replan_iros_2013}
\@writefile{brf}{\backcite{LQG_MP_2011}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{Erez10ascalable}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{van_den_Berg_2012}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{Needle_2014}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{non_gauss_bel_plan_2012}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{seq_traj_replan_iros_2013}{{33}{2.3.3}{section*.20}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: planning}{33}{section*.21}}
\citation{un_water_inspection_icra_2012}
\citation{u_aware_grasp_ICRA_2015}
\citation{Li_2015}
\citation{Littman95}
\citation{RL_book_sa}
\citation{Thrun_2005}
\citation{acting_uncer_1996}
\citation{qmdp_ijcnn_2014}
\citation{where_look_2012}
\citation{Hauser_2011}
\citation{pomdp_iros_tous_2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Heuristics}{34}{subsection.2.3.4}}
\newlabel{lit:heuristics}{{2.3.4}{34}{Heuristics}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Myopic \& Q-MDP}{34}{section*.22}}
\@writefile{brf}{\backcite{un_water_inspection_icra_2012}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{u_aware_grasp_ICRA_2015}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Li_2015}{{34}{2.3.4}{section*.22}}}
\citation{CostalNavigation1999}
\citation{stachniss05robotics}
\citation{dense_entropy_icra_2014}
\@writefile{brf}{\backcite{Littman95}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{RL_book_sa}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Thrun_2005}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{acting_uncer_1996}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{qmdp_ijcnn_2014}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{where_look_2012}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Hauser_2011}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{pomdp_iros_tous_2015}{{35}{2.3.4}{section*.22}}}
\@writefile{toc}{\contentsline {subsubsection}{Information gain}{35}{section*.23}}
\@writefile{brf}{\backcite{CostalNavigation1999}{{35}{2.3.4}{section*.23}}}
\citation{Hsiao_RSS_10}
\citation{Efficient_touch_2012}
\citation{next_best_touch}
\@writefile{brf}{\backcite{stachniss05robotics}{{36}{2.3.4}{section*.23}}}
\@writefile{brf}{\backcite{dense_entropy_icra_2014}{{36}{2.3.4}{section*.23}}}
\@writefile{brf}{\backcite{Hsiao_RSS_10}{{36}{2.3.4}{section*.23}}}
\@writefile{brf}{\backcite{Efficient_touch_2012}{{36}{2.3.4}{section*.23}}}
\@writefile{brf}{\backcite{next_best_touch}{{36}{2.3.4}{section*.23}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: heuristic}{36}{section*.24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Summary: literature}{37}{subsection.2.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }}{38}{figure.caption.25}}
\newlabel{fig:mind_summary}{{2.8}{38}{Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }{figure.caption.25}{}}
\citation{Billard08chapter}
\citation{Billard_schol_2013}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Approach}{39}{section.2.4}}
\newlabel{sec:approach}{{2.4}{39}{Approach}{section.2.4}{}}
\@writefile{brf}{\backcite{Billard08chapter}{{39}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{Billard_schol_2013}{{39}{2.4}{section.2.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }}{40}{figure.caption.26}}
\newlabel{fig:belief-pipeline}{{2.9}{40}{Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }{figure.caption.26}{}}
\citation{Biomechanics_2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \textbf  {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf  {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf  {Learn policy:} A generative policy, $\policy  (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }}{41}{figure.caption.27}}
\newlabel{fig:human_search}{{2.10}{41}{\textbf {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf {Learn policy:} A generative policy, $\policy (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }}{42}{figure.caption.28}}
\newlabel{fig:control_architecture}{{2.11}{42}{Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }{figure.caption.28}{}}
\@writefile{brf}{\backcite{Biomechanics_2009}{{42}{2.4}{figure.caption.27}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning to reason with uncertainty as humans}{43}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf  {Blindfolded search task} \textit  {Left:} Search task, a human demonstrator searching for the green wooden block on the table given that both his hearing and vision senses have been impeded. He starts (hand) at the white spot near position (1). The the red and blue trajectories are examples of possible searches. \textit  {Middle:} Inferred belief the human might have with respect to his position. If the human always starts at (1) and his belief is known, all following beliefs (2) can be inferred from Bayes rule. \textit  {Right:} WAM Robot 7 DOF reproduces the search strategies demonstrated by humans to find the object.\relax }}{44}{figure.caption.29}}
\newlabel{fig:searching}{{3.1}{44}{\textbf {Blindfolded search task} \textit {Left:} Search task, a human demonstrator searching for the green wooden block on the table given that both his hearing and vision senses have been impeded. He starts (hand) at the white spot near position (1). The the red and blue trajectories are examples of possible searches. \textit {Middle:} Inferred belief the human might have with respect to his position. If the human always starts at (1) and his belief is known, all following beliefs (2) can be inferred from Bayes rule. \textit {Right:} WAM Robot 7 DOF reproduces the search strategies demonstrated by humans to find the object.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Outline}{45}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Background}{45}{section.3.2}}
\newlabel{ch3:background}{{3.2}{45}{Background}{section.3.2}{}}
\citation{Wang_2007}
\citation{what_det_our_nav_ability_2010}
\citation{spatial_updating_2008}
\citation{spatial_memory_how_ego_allo_combine_2006}
\citation{updating_egocentric_human_navigation_2000}
\citation{Pasqualotto2013175}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Spatial navigation}{46}{subsection.3.2.1}}
\@writefile{brf}{\backcite{Wang_2007}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{what_det_our_nav_ability_2010}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{spatial_updating_2008}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{spatial_memory_how_ego_allo_combine_2006}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{updating_egocentric_human_navigation_2000}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{Pasqualotto2013175}{{46}{3.2.1}{subsection.3.2.1}}}
\citation{spatial_memory_how_ego_allo_combine_2006}
\citation{cogprints730}
\citation{human_stsm_2015}
\citation{Iachini2014}
\citation{stankiewicz2006lost}
\@writefile{brf}{\backcite{spatial_memory_how_ego_allo_combine_2006}{{47}{3.2.1}{subsection.3.2.1}}}
\@writefile{toc}{\contentsline {subsubsection}{Spatial cognition and memory}{47}{section*.30}}
\@writefile{brf}{\backcite{cogprints730}{{47}{3.2.1}{section*.30}}}
\@writefile{brf}{\backcite{human_stsm_2015}{{47}{3.2.1}{section*.30}}}
\@writefile{brf}{\backcite{Iachini2014}{{47}{3.2.1}{section*.30}}}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{47}{3.2.1}{section*.30}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: spatial cognition}{47}{section*.31}}
\citation{stankiewicz2006lost}
\citation{Towards_a_ToM_2010}
\citation{ToM_humanoid}
\citation{Leslie_TOMM}
\citation{Baron-Cohen}
\citation{MRF_ToM}
\citation{ToM_HRI_2106}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{48}{3.2.1}{section*.31}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Human beliefs}{48}{subsection.3.2.2}}
\@writefile{brf}{\backcite{Towards_a_ToM_2010}{{48}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{ToM_humanoid}{{48}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Leslie_TOMM}{{48}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Baron-Cohen}{{48}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{MRF_ToM}{{48}{3.2.2}{subsection.3.2.2}}}
\citation{Bake_Saxe_Tene_2011}
\citation{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}
\citation{Bake_Tene_Saxe_2006}
\citation{Bake_Saxe_Tene_2011}
\citation{Kasper2001153}
\citation{Hamner_2006_5810}
\citation{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}
\citation{Nicolescu01learningand}
\@writefile{brf}{\backcite{ToM_HRI_2106}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Bake_Tene_Saxe_2006}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Programming by demonstration \& uncertainty}{49}{subsection.3.2.3}}
\@writefile{brf}{\backcite{Kasper2001153}{{49}{3.2.3}{subsection.3.2.3}}}
\@writefile{brf}{\backcite{Hamner_2006_5810}{{49}{3.2.3}{subsection.3.2.3}}}
\citation{GeorgiosLidoris}
\@writefile{brf}{\backcite{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}{{50}{3.2.3}{subsection.3.2.3}}}
\@writefile{brf}{\backcite{Nicolescu01learningand}{{50}{3.2.3}{subsection.3.2.3}}}
\@writefile{brf}{\backcite{GeorgiosLidoris}{{50}{3.2.3}{subsection.3.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Experiment: table search}{50}{section.3.3}}
\newlabel{ch3:experiment}{{3.3}{50}{Experiment: table search}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Table search task. Blindfolded human subjects after a disorientation step are placed in one of the four starting locations. The heading of the subject is always kept the same. The human's objective is to locate the green block on the table. Throughout all experiments the green wooden block is kept in the same location.\relax }}{51}{figure.caption.32}}
\newlabel{fig:tab_search_task}{{3.2}{51}{Table search task. Blindfolded human subjects after a disorientation step are placed in one of the four starting locations. The heading of the subject is always kept the same. The human's objective is to locate the green block on the table. Throughout all experiments the green wooden block is kept in the same location.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Formulation}{51}{section.3.4}}
\newlabel{ch3:formulation}{{3.4}{51}{Formulation}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \textit  {Top left}: A participant is trying to locate the green wooden block on the table given that both vision and hearing senses have been inhibited. The location of his hand is being tracked by the OptiTrack\textsuperscript  {\textregistered } system. \textit  {Top right:} Initial distribution of the uncertainty or belief we assume the human has with respect to his position. \textit  {Bottom left:} Set of recorded searches, the trajectories are with respect to the hand. \textit  {Bottom right:} Trajectories starting from same area but have different search patterns, the red trajectories all navigate to the goal via the top right corner as opposed to the blue which go by the bottom left and right corner. Among these two groups there are trajectories which seem to minimize the distance taken to reach the goal as opposed to some which seek to stay close to the edge and corners.\relax }}{52}{figure.caption.33}}
\newlabel{fig:experiment}{{3.3}{52}{\textit {Top left}: A participant is trying to locate the green wooden block on the table given that both vision and hearing senses have been inhibited. The location of his hand is being tracked by the OptiTrack\textsuperscript {\textregistered } system. \textit {Top right:} Initial distribution of the uncertainty or belief we assume the human has with respect to his position. \textit {Bottom left:} Set of recorded searches, the trajectories are with respect to the hand. \textit {Bottom right:} Trajectories starting from same area but have different search patterns, the red trajectories all navigate to the goal via the top right corner as opposed to the blue which go by the bottom left and right corner. Among these two groups there are trajectories which seem to minimize the distance taken to reach the goal as opposed to some which seek to stay close to the edge and corners.\relax }{figure.caption.33}{}}
\citation{Arul_Mask_Clap_2002}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {subsubsection}{Belief model}{53}{section*.34}}
\@writefile{brf}{\backcite{Arul_Mask_Clap_2002}{{54}{3.4}{equation.3.4.1}}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{54}{3.4}{equation.3.4.1}}}
\@writefile{toc}{\contentsline {subsubsection}{Sensing model}{54}{section*.35}}
\newlabel{eq:sensingfunction}{{3.3}{54}{Sensing model}{equation.3.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Four different time frames of the evolution of the belief particle filter. \textit  {Top left}: Initial belief distribution; a lot of uncertainty. \textit  {Top right:} First contact is made with the table, the measurement likelihood restrains the samples to be on the table's surface. \textit  {Bottom right:} First contact is an edge. \textit  {Bottom left:} Gradual localisation.\relax }}{55}{figure.caption.36}}
\newlabel{fig:pf_example}{{3.4}{55}{Four different time frames of the evolution of the belief particle filter. \textit {Top left}: Initial belief distribution; a lot of uncertainty. \textit {Top right:} First contact is made with the table, the measurement likelihood restrains the samples to be on the table's surface. \textit {Bottom right:} First contact is an edge. \textit {Bottom left:} Gradual localisation.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motion model}{55}{section*.37}}
\@writefile{toc}{\contentsline {subsubsection}{Uncertainty}{55}{section*.38}}
\citation{DiffEntropyHuber2008}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Representation of the estimated density function. \textit  {Top Left and Right:} Initial starting point, all Gaussian functions are uniformly distributed with uniform priors. The red cluster always has the highest likelihood which is taken to be the believed location of the robot's/human's end-effector. \textit  {Bottom Left:} Contact with the table has been established, the robot location differers from his belief. \textit  {Bottom Right:} Contact has been made with a corner, the clusters reflect that the robot could be at any corner (note that weights are not depicted, only cluster assignment).\relax }}{56}{figure.caption.39}}
\newlabel{fig:clustering}{{3.5}{56}{Representation of the estimated density function. \textit {Top Left and Right:} Initial starting point, all Gaussian functions are uniformly distributed with uniform priors. The red cluster always has the highest likelihood which is taken to be the believed location of the robot's/human's end-effector. \textit {Bottom Left:} Contact with the table has been established, the robot location differers from his belief. \textit {Bottom Right:} Contact has been made with a corner, the clusters reflect that the robot could be at any corner (note that weights are not depicted, only cluster assignment).\relax }{figure.caption.39}{}}
\newlabel{eq:gmm1}{{3.4}{56}{Uncertainty}{equation.3.4.4}{}}
\citation{BillardCDS08}
\@writefile{brf}{\backcite{DiffEntropyHuber2008}{{57}{3.4}{figure.caption.39}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Policies}{57}{section.3.5}}
\newlabel{chap3:policies}{{3.5}{57}{Policies}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Modelling human search strategies}{57}{subsection.3.5.1}}
\newlabel{chap3:GMM_policy}{{3.5.1}{57}{Modelling human search strategies}{subsection.3.5.1}{}}
\@writefile{brf}{\backcite{BillardCDS08}{{57}{3.5.1}{subsection.3.5.1}}}
\citation{CostalNavigation1999}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \textit  {Left: } Resulting search GMM, a total of 67 Gaussian mixture components are present. We note the many overlapping Gaussians: this results from the level of uncertainty over the different choices taken. For example humans follow along the edge of the table in different directions and might leave the edge once they are confident with respect to their location. \textit  {Right:} Information Gain map of the table environment, dark regions indicate high information gain as oppose to lighter ones. Not surprisingly, the highest are the corners, followed by the edges.\relax }}{58}{figure.caption.40}}
\newlabel{fig:gmm}{{3.6}{58}{\textit {Left: } Resulting search GMM, a total of 67 Gaussian mixture components are present. We note the many overlapping Gaussians: this results from the level of uncertainty over the different choices taken. For example humans follow along the edge of the table in different directions and might leave the edge once they are confident with respect to their location. \textit {Right:} Information Gain map of the table environment, dark regions indicate high information gain as oppose to lighter ones. Not surprisingly, the highest are the corners, followed by the edges.\relax }{figure.caption.40}{}}
\newlabel{eq:gmm2}{{3.7}{58}{Modelling human search strategies}{equation.3.5.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Coastal Navigation}{58}{subsection.3.5.2}}
\newlabel{chap3:costal_policy}{{3.5.2}{58}{Coastal Navigation}{subsection.3.5.2}{}}
\@writefile{brf}{\backcite{CostalNavigation1999}{{58}{3.5.2}{subsection.3.5.2}}}
\newlabel{eq:objective_function}{{3.8}{58}{Coastal Navigation}{equation.3.5.8}{}}
\newlabel{eq:IG}{{3.9}{59}{Coastal Navigation}{equation.3.5.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Control}{59}{subsection.3.5.3}}
\newlabel{eq:conditional}{{3.10}{59}{Control}{equation.3.5.10}{}}
\newlabel{eq:GMR}{{3.11}{59}{Control}{equation.3.5.11}{}}
\newlabel{eq:weight}{{3.12}{60}{Control}{equation.3.5.12}{}}
\newlabel{eq:w_expectation}{{3.13}{60}{Control}{equation.3.5.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Overview of the decision loop. At the top a strategy is chosen given an initial belief $p(x_{0}|y_{0})$ of the location of the end-effector (initially through sampling the conditional). A speed is applied to the given direction based on the believed distance to the goal. This velocity is passed onwards to a low level impedance controller which sends out the required torques. The resulting sensation, encoded through the Multinomial distribution over the environment features, and actual displacement are sent back to update the belief.\relax }}{61}{figure.caption.41}}
\newlabel{fig:flow_chart}{{3.7}{61}{Overview of the decision loop. At the top a strategy is chosen given an initial belief $p(x_{0}|y_{0})$ of the location of the end-effector (initially through sampling the conditional). A speed is applied to the given direction based on the believed distance to the goal. This velocity is passed onwards to a low level impedance controller which sends out the required torques. The resulting sensation, encoded through the Multinomial distribution over the environment features, and actual displacement are sent back to update the belief.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Results and discussion}{62}{section.3.6}}
\newlabel{chap3:results}{{3.6}{62}{Results and discussion}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Search \& behaviour analysis}{62}{subsection.3.6.1}}
\newlabel{sub:search_behaviour}{{3.6.1}{62}{Search \& behaviour analysis}{subsection.3.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Expected sensation. Plots of the expected sensation of the edge and corner feature for all trajectories. The axes are associated with the sensor measurements, 0 means that the corresponding feature is not sensed and 1 the feature is fully sensed. A point in the plots summarises a whole trajectory by the mean and variance of the probability of sensing a corner or edge. The radius of the circles are proportional to the variance. The doted blue rectangle represents the decision boundary for classifying a trajectory as being either risk-prone or risk-averse. A point which lies inside the rectangle is risk-prone. \textit  {Left:} Human trajectories demonstrate a wide variety of behaviours ranging from those remaining close to features to those preferring more risk. \textit  {Right:} Red points show Greedy and blue points the GMM model. \textit  {Bottom:} Green circles are associated with the Hybrid method whilst orange are those of the Coastal navigation method. The Hybrid method is a skewed version of the GMM which tends towards risky behaviour and exhibits the same kind of behaviour as the Coastal algorithm.\relax }}{63}{figure.caption.42}}
\newlabel{fig:expectedfeatures}{{3.8}{63}{Expected sensation. Plots of the expected sensation of the edge and corner feature for all trajectories. The axes are associated with the sensor measurements, 0 means that the corresponding feature is not sensed and 1 the feature is fully sensed. A point in the plots summarises a whole trajectory by the mean and variance of the probability of sensing a corner or edge. The radius of the circles are proportional to the variance. The doted blue rectangle represents the decision boundary for classifying a trajectory as being either risk-prone or risk-averse. A point which lies inside the rectangle is risk-prone. \textit {Left:} Human trajectories demonstrate a wide variety of behaviours ranging from those remaining close to features to those preferring more risk. \textit {Right:} Red points show Greedy and blue points the GMM model. \textit {Bottom:} Green circles are associated with the Hybrid method whilst orange are those of the Coastal navigation method. The Hybrid method is a skewed version of the GMM which tends towards risky behaviour and exhibits the same kind of behaviour as the Coastal algorithm.\relax }{figure.caption.42}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Percentage of risk-prone trajectories based on two decision criteria, the feature (f) and the risk (r) (information gain) metrics discussed above.\relax }}{64}{table.caption.44}}
\newlabel{tab:percentage-risk-prone}{{3.1}{64}{Percentage of risk-prone trajectories based on two decision criteria, the feature (f) and the risk (r) (information gain) metrics discussed above.\relax }{table.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Risk of searches. Illustration of risk-prone and risk-averse searches in terms of a Risk factor (\textit  {left}) and expected sensation (\textit  {right}). \textit  {Left:} Each trajectory was reduced to a single scalar, which we call the Risk factor, quantifying the risk of a trajectory. The Risk factor is inversely proportional to the sum of the information gain of a particular trajectory. The colour paired dots (risk averse) and squares (risk prone) represent trajectories which are plotted in Figure \ref  {fig:risk_examples}, to illustrate that these correspond to risk averse and prone searches. \textit  {Right:} Corresponding trajectories chosen in the Risk factor space but represented in the feature space. As expected, trajectories with a high risk map to regions of low expected feature. However the transition from the Risk space to feature space is non-linear and will result in a different risk-level classification than the feature metric previously discussed.\relax }}{65}{figure.caption.43}}
\newlabel{fig:riskexamples}{{3.9}{65}{Risk of searches. Illustration of risk-prone and risk-averse searches in terms of a Risk factor (\textit {left}) and expected sensation (\textit {right}). \textit {Left:} Each trajectory was reduced to a single scalar, which we call the Risk factor, quantifying the risk of a trajectory. The Risk factor is inversely proportional to the sum of the information gain of a particular trajectory. The colour paired dots (risk averse) and squares (risk prone) represent trajectories which are plotted in Figure \ref {fig:risk_examples}, to illustrate that these correspond to risk averse and prone searches. \textit {Right:} Corresponding trajectories chosen in the Risk factor space but represented in the feature space. As expected, trajectories with a high risk map to regions of low expected feature. However the transition from the Risk space to feature space is non-linear and will result in a different risk-level classification than the feature metric previously discussed.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Risk prone \& averse searches (red \& green trajectories). \textit  {Top left:} Two human trajectories taken from data shown in Figure \ref  {fig:riskexamples}. \textit  {Top right:} Two Greedy trajectories. \textit  {Bottom left:} GMM trajectories, all starting from the same location, the colour coding is to illustrate the different policies which were encoded and emerge given the same initial conditions. \textit  {Bottom right:} Corresponding expected features of each trajectory, the colour coding matches the trajectories to the ``GMM risk types'' sub-figure. All the searches which were generated by the GMM for this initialisation produced risk-averse searches (based on the feature metric discussed previous).\relax }}{66}{figure.caption.45}}
\newlabel{fig:risk_examples}{{3.10}{66}{Risk prone \& averse searches (red \& green trajectories). \textit {Top left:} Two human trajectories taken from data shown in Figure \ref {fig:riskexamples}. \textit {Top right:} Two Greedy trajectories. \textit {Bottom left:} GMM trajectories, all starting from the same location, the colour coding is to illustrate the different policies which were encoded and emerge given the same initial conditions. \textit {Bottom right:} Corresponding expected features of each trajectory, the colour coding matches the trajectories to the ``GMM risk types'' sub-figure. All the searches which were generated by the GMM for this initialisation produced risk-averse searches (based on the feature metric discussed previous).\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}GMM \& Coastal Navigation policy analysis}{67}{subsection.3.6.2}}
\newlabel{sub:policy_analysis}{{3.6.2}{67}{GMM \& Coastal Navigation policy analysis}{subsection.3.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Illustration of three different types of modes present during the execution of the task where the robot is being controlled by the learned GMM model. The white ball represents the actual position of the robot's end-effector. The blue ball represents the believed position of the robot's end-effector and the robot is acting according to it. The blue ball arrows represent modes. Colours encode the mode's weights given by the priors $\pi _{k}$ after conditioning ( but not re-weighted as previously described). The spectrum ranges from red (high weight) to blue (low weight). \textit  {Top left:} Three modes are present, but two agree with each other. \textit  {Top right:} Three modes are again present indicating appropriate ways to reduce the uncertainty. \textit  {Lower left:} Two modes are in opposing directions. No flipping behaviour between modes occurs since preference is given to the modes pointing in the same direction as the robot's current trajectory. \textit  {Lower right:} GMM modes when conditioned on the state represented in the lower left figure. The two modes represent the possible directions (un-normalised).\relax }}{68}{figure.caption.46}}
\newlabel{fig:modes}{{3.11}{68}{Illustration of three different types of modes present during the execution of the task where the robot is being controlled by the learned GMM model. The white ball represents the actual position of the robot's end-effector. The blue ball represents the believed position of the robot's end-effector and the robot is acting according to it. The blue ball arrows represent modes. Colours encode the mode's weights given by the priors $\pi _{k}$ after conditioning ( but not re-weighted as previously described). The spectrum ranges from red (high weight) to blue (low weight). \textit {Top left:} Three modes are present, but two agree with each other. \textit {Top right:} Three modes are again present indicating appropriate ways to reduce the uncertainty. \textit {Lower left:} Two modes are in opposing directions. No flipping behaviour between modes occurs since preference is given to the modes pointing in the same direction as the robot's current trajectory. \textit {Lower right:} GMM modes when conditioned on the state represented in the lower left figure. The two modes represent the possible directions (un-normalised).\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Illustration of the vector field for the Coastal and GMM policy. \textit  {Top Left} Coastal policy, there is only one possible direction for every state at any time, the values of $\lambda _2$ in the cost function were set experimentally. \textit  {Others:} The GMM policy for three different levels of uncertainty. For each point multiple actions are possible which is reflected by the number of arrows (only the first three most likely actions). As the uncertainty decreases the policy becomes less multi-model, but remains around the edges and corners. Note that once certain of being close to an edge there is a possibility to go either straight to the goal or stay close to the edge and corners.\relax }}{69}{figure.caption.47}}
\newlabel{fig:vectorfield}{{3.12}{69}{Illustration of the vector field for the Coastal and GMM policy. \textit {Top Left} Coastal policy, there is only one possible direction for every state at any time, the values of $\lambda _2$ in the cost function were set experimentally. \textit {Others:} The GMM policy for three different levels of uncertainty. For each point multiple actions are possible which is reflected by the number of arrows (only the first three most likely actions). As the uncertainty decreases the policy becomes less multi-model, but remains around the edges and corners. Note that once certain of being close to an edge there is a possibility to go either straight to the goal or stay close to the edge and corners.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Distance efficiency \& Uncertainty}{70}{subsection.3.6.3}}
\newlabel{sub:time_uncertainty}{{3.6.3}{70}{Distance efficiency \& Uncertainty}{subsection.3.6.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Mean distance and (variance) taken to reach the goal for 3 methods in 5 experiments. The grey shaded entries correspond to the results of the search algorithm which obtained the fastest time to reach the goal in each type of experiment/search.\relax }}{70}{table.caption.49}}
\newlabel{tab:mean-var-distance}{{3.2}{70}{Mean distance and (variance) taken to reach the goal for 3 methods in 5 experiments. The grey shaded entries correspond to the results of the search algorithm which obtained the fastest time to reach the goal in each type of experiment/search.\relax }{table.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Four search initializations, from \textit  {top left} to \textit  {bottom right} we refer to them as \#1-4. The circle indicates the true starting point of the end-effector (eof), whilst the triangle is the initial believed location of the eof. The initialisation in \#1 was chosen such that the true and believed eof locations were at opposite sides of the table. This setting was selected to highlight the draw back in methods which do not take into account uncertainty. The second initialisation \#2, reflects the situation where once again there is a large distance between true and believed location of the eof. However this time both are above the table. The starting points in \#3 are a variant on \#1 with the difference being that the believed eof position is above the table whilst the true eof location is not. The last experiment \#4 was a setup which would be favourable to algorithms that are inclined to be greedy. Both true and believed eof locations are close to one another.\relax }}{71}{figure.caption.48}}
\newlabel{fig:four-initialisations}{{3.13}{71}{Four search initializations, from \textit {top left} to \textit {bottom right} we refer to them as \#1-4. The circle indicates the true starting point of the end-effector (eof), whilst the triangle is the initial believed location of the eof. The initialisation in \#1 was chosen such that the true and believed eof locations were at opposite sides of the table. This setting was selected to highlight the draw back in methods which do not take into account uncertainty. The second initialisation \#2, reflects the situation where once again there is a large distance between true and believed location of the eof. However this time both are above the table. The starting points in \#3 are a variant on \#1 with the difference being that the believed eof position is above the table whilst the true eof location is not. The last experiment \#4 was a setup which would be favourable to algorithms that are inclined to be greedy. Both true and believed eof locations are close to one another.\relax }{figure.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces ANOVA tests the null hypothesis that all search experiments produced the same type of search with respect to the distance taken to reach the goal. All the p-values are extremely small which indicate that the null hypothesis can safely be rejected.\relax }}{72}{table.caption.50}}
\newlabel{tab:anova-1}{{3.3}{72}{ANOVA tests the null hypothesis that all search experiments produced the same type of search with respect to the distance taken to reach the goal. All the p-values are extremely small which indicate that the null hypothesis can safely be rejected.\relax }{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces ANOVA between paired search methods. The first column gives an indication of the probability that both the Greedy and GMM searches are statistically the same (the null hypothesis). This was rejected with a tolerance of below \%1. In the second column, Greedy vs Coastal searches \#1 and \#4 are statistically closer than the rest with a p-value threshold of 10\% required to be able to reject the null hypothesis. In the third column the uniform and \#3 are not statistically different and would require a higher threshold on the p-value to be so.\relax }}{72}{table.caption.51}}
\newlabel{fig:anova-2}{{3.4}{72}{ANOVA between paired search methods. The first column gives an indication of the probability that both the Greedy and GMM searches are statistically the same (the null hypothesis). This was rejected with a tolerance of below \%1. In the second column, Greedy vs Coastal searches \#1 and \#4 are statistically closer than the rest with a p-value threshold of 10\% required to be able to reject the null hypothesis. In the third column the uniform and \#3 are not statistically different and would require a higher threshold on the p-value to be so.\relax }{table.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Reduction of the uncertainty for the Uniform, \#1, \#2 and \#4 experiment, the expected value is reported \textit  {Top left}: Uniform initialisation, expected uncertainty for the Greedy (red), GMM (blue), Hybrid (green) \& Coastal (orange) search strategies. \textit  {Top right:} Experiment \#1. \textit  {Bottom left:} Experiment \#2. \textit  {Bottom right:} Experiment \#4.\relax }}{73}{figure.caption.52}}
\newlabel{fig:uncertainty}{{3.14}{73}{Reduction of the uncertainty for the Uniform, \#1, \#2 and \#4 experiment, the expected value is reported \textit {Top left}: Uniform initialisation, expected uncertainty for the Greedy (red), GMM (blue), Hybrid (green) \& Coastal (orange) search strategies. \textit {Top right:} Experiment \#1. \textit {Bottom left:} Experiment \#2. \textit {Bottom right:} Experiment \#4.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Conclusions}{73}{section.3.7}}
\citation{Bake_Saxe_Tene_2011}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{74}{3.7}{section.3.7}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Peg in hole}{75}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textit  {Top-right:} Experiment setup, blindfolded human subjects stand in the orange rectangle always facing towards the table. Human subjects where informed of their starting location and where told that they would always be facing the table. \textit  {Top-left:} Three different powr sockets. The plug is connected to a cylinder such to make it easy to hold for the human subject. Between the cylinder and the peg is an ATI 6-axis force torque sensor (Nano25 Series). \textit  {Bottom-left} Human subject doing PiH search task. The human subjects wore goggles to remove sight and wore ear defenders to greatly diminish hearing.\textit  {Bottom-right:} The KUKA LWR robot equipped with the same force torque sensor and plug that the human subjects used during the demonstration phase.\relax }}{76}{figure.caption.53}}
\newlabel{fig:experiment_setup}{{4.1}{76}{\textit {Top-right:} Experiment setup, blindfolded human subjects stand in the orange rectangle always facing towards the table. Human subjects where informed of their starting location and where told that they would always be facing the table. \textit {Top-left:} Three different powr sockets. The plug is connected to a cylinder such to make it easy to hold for the human subject. Between the cylinder and the peg is an ATI 6-axis force torque sensor (Nano25 Series). \textit {Bottom-left} Human subject doing PiH search task. The human subjects wore goggles to remove sight and wore ear defenders to greatly diminish hearing.\textit {Bottom-right:} The KUKA LWR robot equipped with the same force torque sensor and plug that the human subjects used during the demonstration phase.\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Outline}{77}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Background}{77}{section.4.2}}
\newlabel{ch4:background}{{4.2}{77}{Background}{section.4.2}{}}
\citation{search_strategies_icra_2001}
\citation{online_gpr_icra_2014}
\citation{peg_personal_icra_2010}
\citation{hybrid_1992}
\citation{fast_peg_pbd_icmc_2014}
\citation{Schaal04learningmovement}
\citation{trans_workpiece_icra_2013}
\citation{sol_pdg_pbd_2014}
\citation{learn_force_c_icirs_2011}
\citation{learn_admittance_icra_1994}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Peg-in-hole}{78}{subsection.4.2.1}}
\@writefile{brf}{\backcite{search_strategies_icra_2001}{{78}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{online_gpr_icra_2014}{{78}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{peg_personal_icra_2010}{{78}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{hybrid_1992}{{78}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{fast_peg_pbd_icmc_2014}{{78}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{Schaal04learningmovement}{{78}{4.2.1}{subsection.4.2.1}}}
\citation{search_strategies_icra_2001}
\citation{peg_imcssd_2015}
\citation{intuitive_peg_isr_2013}
\citation{compliant_manip_icra_2008}
\@writefile{brf}{\backcite{trans_workpiece_icra_2013}{{79}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{sol_pdg_pbd_2014}{{79}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{learn_force_c_icirs_2011}{{79}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{learn_admittance_icra_1994}{{79}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{search_strategies_icra_2001}{{79}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{peg_imcssd_2015}{{79}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{intuitive_peg_isr_2013}{{79}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{compliant_manip_icra_2008}{{79}{4.2.1}{subsection.4.2.1}}}
\citation{online_gpr_icra_2014}
\citation{Sutton00policygradient}
\citation{rl_ac_surv_2012}
\citation{Sutton00policygradient}
\citation{Boyan95generalizationin}
\citation{sutton98a}
\citation{rl_ac_surv_2012}
\citation{fqi_nips_peter_2009}
\citation{batch_synth_traj_2013}
\citation{fnac_ca_2008}
\citation{Riedmiller2005}
\citation{EGW05}
\citation{rl_gmm_2010}
\citation{fvi_uav_2010}
\citation{mnih-dqn-2015}
\citation{neura_fqi_2005}
\citation{DRQ_AAAI_2015}
\citation{approx_rl_overview_2011}
\@writefile{brf}{\backcite{online_gpr_icra_2014}{{80}{4.2.1}{subsection.4.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Actor-Critic \& Fitted Reinforcement Learning}{80}{subsection.4.2.2}}
\@writefile{brf}{\backcite{EGW05}{{80}{4.2.2}{subsection.4.2.2}}}
\@writefile{brf}{\backcite{mnih-dqn-2015}{{80}{4.2.2}{subsection.4.2.2}}}
\@writefile{brf}{\backcite{neura_fqi_2005}{{80}{4.2.2}{subsection.4.2.2}}}
\@writefile{brf}{\backcite{DRQ_AAAI_2015}{{80}{4.2.2}{subsection.4.2.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Experiment}{80}{section.4.3}}
\newlabel{ch4:experiment}{{4.3}{80}{Experiment}{section.4.3}{}}
\citation{Bergman99recursivebayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textit  {Left}: black points represent the starting position of the end-effector for all the demonstrations, four trajectories are illustrated. \textit  {Right:} Time taken taken for the subjects to accomplish the PiH once the socket was localised. Group A and B are depicted in red and blue. The asterisk indicates that the group is doing the task on the second socket, so Group A\textsuperscript  {*} means that Group A is doing the task with socket B and Group B\textsuperscript  {*} means that group B is doing the task with socket A.\relax }}{82}{figure.caption.54}}
\newlabel{fig:experiment_setup_data}{{4.2}{82}{\textit {Left}: black points represent the starting position of the end-effector for all the demonstrations, four trajectories are illustrated. \textit {Right:} Time taken taken for the subjects to accomplish the PiH once the socket was localised. Group A and B are depicted in red and blue. The asterisk indicates that the group is doing the task on the second socket, so Group A\textsuperscript {*} means that Group A is doing the task with socket B and Group B\textsuperscript {*} means that group B is doing the task with socket A.\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Formulation}{82}{section.4.4}}
\newlabel{ch4:formulation}{{4.4}{82}{Formulation}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Belief probability density function}{82}{subsection.4.4.1}}
\@writefile{brf}{\backcite{Bergman99recursivebayesian}{{82}{4.4.1}{subsection.4.4.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textit  {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug, the orientation is assumed to be known. (2) First contact, distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit  {Right}: \textbf  {World model}: The plug is presented by its three plug tips and the wall and sockets are fitted with bounding boxes \textbf  {Likelihood}: When the plug inters in contact with right edge of the socket. As a result all the regions, $x_t$, close the left edge are one (red points) whilst the others are zero (blue points) and areas around the socket's central ring are also set to one. \relax }}{83}{figure.caption.55}}
\newlabel{fig:PMF}{{4.3}{83}{\textit {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug, the orientation is assumed to be known. (2) First contact, distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit {Right}: \textbf {World model}: The plug is presented by its three plug tips and the wall and sockets are fitted with bounding boxes \textbf {Likelihood}: When the plug inters in contact with right edge of the socket. As a result all the regions, $x_t$, close the left edge are one (red points) whilst the others are zero (blue points) and areas around the socket's central ring are also set to one. \relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Belief compression}{83}{subsection.4.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Learning Actor and Critic}{84}{section.4.5}}
\newlabel{sec:learning-value-actor}{{4.5}{84}{Learning Actor and Critic}{section.4.5}{}}
\newlabel{eq:value_function}{{4.1}{84}{Learning Actor and Critic}{equation.4.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Actor}{84}{subsection.4.5.1}}
\newlabel{eq:GMM}{{4.2}{84}{Actor}{equation.4.5.2}{}}
\citation{Atkeson97locallyweighted}
\citation{EGW05}
\citation{NIPS2008_3501,EGW05,Riedmiller05neuralfitted}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Critic}{85}{subsection.4.5.2}}
\@writefile{brf}{\backcite{Atkeson97locallyweighted}{{85}{4.5.2}{subsection.4.5.2}}}
\newlabel{eq:W}{{4.3}{85}{Critic}{equation.4.5.3}{}}
\newlabel{eq:lwr_predict}{{4.4}{85}{Critic}{equation.4.5.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fitted Policy Evaluation}{85}{section*.56}}
\@writefile{brf}{\backcite{EGW05}{{85}{4.5.2}{section*.56}}}
\@writefile{brf}{\backcite{NIPS2008_3501}{{85}{4.5.2}{algorithm.4.1}}}
\@writefile{brf}{\backcite{EGW05}{{85}{4.5.2}{algorithm.4.1}}}
\@writefile{brf}{\backcite{Riedmiller05neuralfitted}{{85}{4.5.2}{algorithm.4.1}}}
\citation{sutton1998reinforcement}
\citation{DeisenrothNP2013}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces Fitted Policy Evaluation\relax }}{86}{algorithm.4.1}}
\newlabel{alg:fpe}{{4.1}{86}{Fitted Policy Evaluation\relax }{algorithm.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces \textit  {Left:} LWR approximate value function $\mathaccentV {hat}05E{V}^{\pi }(b)$. \textit  {Right:} first five best and worst trajectories in terms accumulated value.\relax }}{86}{figure.caption.57}}
\newlabel{fig:Figure1}{{4.4}{86}{\textit {Left:} LWR approximate value function $\hat {V}^{\pi }(\B )$. \textit {Right:} first five best and worst trajectories in terms accumulated value.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Actor update}{86}{subsection.4.5.3}}
\@writefile{brf}{\backcite{sutton1998reinforcement}{{86}{4.5.3}{subsection.4.5.3}}}
\citation{peter_nac_2008}
\@writefile{brf}{\backcite{DeisenrothNP2013}{{87}{4.5.3}{subsection.4.5.3}}}
\newlabel{eq:disc_return}{{4.5}{87}{Actor update}{equation.4.5.5}{}}
\newlabel{eq:expected_reward}{{4.6}{87}{Actor update}{equation.4.5.6}{}}
\newlabel{eq:grad_log_cost}{{4.7}{87}{Actor update}{equation.4.5.7}{}}
\newlabel{eq:advantage_f}{{4.8}{87}{Actor update}{equation.4.5.8}{}}
\@writefile{brf}{\backcite{peter_nac_2008}{{87}{4.5.3}{equation.4.5.8}}}
\citation{gesture_calinon_2010}
\citation{gmr_2004}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Control architecture}{88}{section.4.6}}
\newlabel{seq:control_architecture}{{4.6}{88}{Control architecture}{section.4.6}{}}
\newlabel{eq:gmm_conditional}{{4.9}{88}{Control architecture}{equation.4.6.9}{}}
\@writefile{brf}{\backcite{gesture_calinon_2010}{{88}{4.6}{equation.4.6.9}}}
\@writefile{brf}{\backcite{gmr_2004}{{88}{4.6}{equation.4.6.9}}}
\newlabel{eq:alpha_eq}{{4.10}{88}{Control architecture}{equation.4.6.10}{}}
\newlabel{eq:alpha_expectation}{{4.11}{88}{Control architecture}{equation.4.6.11}{}}
\newlabel{eq:modulation}{{4.12}{89}{Control architecture}{equation.4.6.12}{}}
\newlabel{eq:prop_speed}{{4.13}{89}{Control architecture}{equation.4.6.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Control architecture. The PMF (belief) received a measured velocity, $\mathaccentV {dot}05F{\mathaccentV {tilde}07E{x}}$, and sensor feature $\mathaccentV {tilde}07E{y}$ and gets updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller, Equation \ref  {eq:prop_speed}.\relax }}{90}{figure.caption.58}}
\newlabel{fig:control_flow}{{4.5}{90}{Control architecture. The PMF (belief) received a measured velocity, $\dot {\tilde {x}}$, and sensor feature $\tilde {y}$ and gets updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller, Equation \ref {eq:prop_speed}.\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Q-EM and GMM policy vector field. \textit  {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area, since this level corresponds to very little uncertainty; we are already localised. We can see that the policies vector converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector filed directs the end-effector to go towards the left or right edge of the wall. \textit  {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation.\relax }}{91}{figure.caption.59}}
\newlabel{fig:policy_vf}{{4.6}{91}{Q-EM and GMM policy vector field. \textit {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area, since this level corresponds to very little uncertainty; we are already localised. We can see that the policies vector converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector filed directs the end-effector to go towards the left or right edge of the wall. \textit {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation.\relax }{figure.caption.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Results}{92}{section.4.7}}
\newlabel{ch4:results}{{4.7}{92}{Results}{section.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Distance taken to reach socket's edge (Qualitative)}{93}{subsection.4.7.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Three simulated search experiments. \textbf  {Experiment 1:} Three start positions are considered: \textit  {Left}, \textit  {Center} and \textit  {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. In the second row of Experiment 1, we illustrate the trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textbf  {Experiment 2:} Two cases are considered: \textit  {Case 1} blue, the initial belief state (circle) is fixed facing the left edge of the wall and the true location (diamond) is facing the socket. \textit  {Case 2} pink, the initial belief state (circle) is fixed to the right facing the edge of the wall and the true location is the left edge of the wall. In the second row, the trajectories are plotted for \textit  {Case 1}. \textbf  {Experiment 3:} A 150 start locations are deterministically generated from a grid in the start area. In the second row, we plot the distribution of the areas visited by the true position during the search.\relax }}{94}{figure.caption.60}}
\newlabel{fig:box_exp_sim}{{4.7}{94}{Three simulated search experiments. \textbf {Experiment 1:} Three start positions are considered: \textit {Left}, \textit {Center} and \textit {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. In the second row of Experiment 1, we illustrate the trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textbf {Experiment 2:} Two cases are considered: \textit {Case 1} blue, the initial belief state (circle) is fixed facing the left edge of the wall and the true location (diamond) is facing the socket. \textit {Case 2} pink, the initial belief state (circle) is fixed to the right facing the edge of the wall and the true location is the left edge of the wall. In the second row, the trajectories are plotted for \textit {Case 1}. \textbf {Experiment 3:} A 150 start locations are deterministically generated from a grid in the start area. In the second row, we plot the distribution of the areas visited by the true position during the search.\relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces First contact with the wall, during experiment 1. (a) Contact distribution for initial condition ``Center'' . (b) Contact distribution for initial condition was ``Right''. The ellipses correspond to two standard deviations of a fitted Gaussian function.\relax }}{95}{figure.caption.61}}
\newlabel{fig:first_contact}{{4.8}{95}{First contact with the wall, during experiment 1. (a) Contact distribution for initial condition ``Center'' . (b) Contact distribution for initial condition was ``Right''. The ellipses correspond to two standard deviations of a fitted Gaussian function.\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Distance taken to reach socket's edge (Quantitative)}{95}{subsection.4.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Distance travelled until the socket's edge is reached. (a) Three groups correspond to the initial conditions: Center, Left and Right depicted in Figure \ref  {fig:three_searches}, \textit  {top left}. The Q-EM method is always better than the other methods, in terms of distance. (b) Results of the two initial conditions depicted in Figure \ref  {fig:three_searches}, \textit  {top middle}, both the true position and most likely state is fixed. The Q-EM method always improves on the GMM. (c) Results corresponding to Experiment 3, Figure \ref  {fig:three_searches}, \textit  {top right}. Again the Q-EM method is better, but at a less significant level.\relax }}{96}{figure.caption.62}}
\newlabel{fig:three_searches}{{4.9}{96}{Distance travelled until the socket's edge is reached. (a) Three groups correspond to the initial conditions: Center, Left and Right depicted in Figure \ref {fig:three_searches}, \textit {top left}. The Q-EM method is always better than the other methods, in terms of distance. (b) Results of the two initial conditions depicted in Figure \ref {fig:three_searches}, \textit {top middle}, both the true position and most likely state is fixed. The Q-EM method always improves on the GMM. (c) Results corresponding to Experiment 3, Figure \ref {fig:three_searches}, \textit {top right}. Again the Q-EM method is better, but at a less significant level.\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{96}{figure.caption.62}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{96}{figure.caption.62}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{96}{figure.caption.62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Importance of data}{96}{subsection.4.7.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Original demonstrations of teacher \# 5. The teacher demonstrates a preference to first go to the top of the wall and then leaves contact with the table to position himself in front of the socket, before trying to to find it\relax }}{97}{figure.caption.63}}
\newlabel{fig:subj_5_traj}{{4.10}{97}{Original demonstrations of teacher \# 5. The teacher demonstrates a preference to first go to the top of the wall and then leaves contact with the table to position himself in front of the socket, before trying to to find it\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Value function learned from the 15 demonstrations of teacher \#5.\relax }}{97}{figure.caption.64}}
\newlabel{fig:value_function_subj_5}{{4.11}{97}{Value function learned from the 15 demonstrations of teacher \#5.\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Marginalised Gaussian Mixture parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. The transparency of the Gaussian functions illustrated is proportional to their weight. \textit  {Left column}: The Gaussian functions of the Q-EM have shifted from the left corner to the right, this is a result of the value function being higher in the top right corner region, see Figure \ref  {fig:value_function_subj_5}. \textit  {Center column}: The original data of the teacher went quite fare back which results in a Gaussian function given a direction which moves away from the table (green arrow), whilst in the case of the Q-EM parameters this effect is reduced and moved closer towards the table. We can also see from the two plots of the Q-EM parameters that they then to follow the paths encoded by the value function. \textit  {Right column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives much more consistent trajectories which seem to replicate to some extend the pattern of making a jump (no contact with the table) from the top right corner to the socket's edge.\relax }}{98}{figure.caption.65}}
\newlabel{fig:gmm_exp4}{{4.12}{98}{Marginalised Gaussian Mixture parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. The transparency of the Gaussian functions illustrated is proportional to their weight. \textit {Left column}: The Gaussian functions of the Q-EM have shifted from the left corner to the right, this is a result of the value function being higher in the top right corner region, see Figure \ref {fig:value_function_subj_5}. \textit {Center column}: The original data of the teacher went quite fare back which results in a Gaussian function given a direction which moves away from the table (green arrow), whilst in the case of the Q-EM parameters this effect is reduced and moved closer towards the table. We can also see from the two plots of the Q-EM parameters that they then to follow the paths encoded by the value function. \textit {Right column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives much more consistent trajectories which seem to replicate to some extend the pattern of making a jump (no contact with the table) from the top right corner to the socket's edge.\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.\relax }}{99}{figure.caption.66}}
\newlabel{fig:experiment4_stats}{{4.13}{99}{Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Generalisation}{99}{subsection.4.7.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider the starting a Fixed starting location for both the true and most likely location of the end-effector. The red square depicts the extend of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. \relax }}{100}{figure.caption.67}}
\newlabel{fig:experiment5_traj}{{4.14}{100}{Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider the starting a Fixed starting location for both the true and most likely location of the end-effector. The red square depicts the extend of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. \relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{100}{figure.caption.67}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Distance taken to reach the socket's edge. For the Fixed setup (see Figure \ref  {fig:experiment5_traj}) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. \relax }}{100}{figure.caption.68}}
\newlabel{fig:experiment5_stats}{{4.15}{100}{Distance taken to reach the socket's edge. For the Fixed setup (see Figure \ref {fig:experiment5_traj}) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. \relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{100}{figure.caption.68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.5}Distance taken to connect the peg to the socket}{101}{subsection.4.7.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces 25 search trajectories for each of the three search policies for socket A. \relax }}{102}{figure.caption.69}}
\newlabel{fig:real_policy}{{4.16}{102}{25 search trajectories for each of the three search policies for socket A. \relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{102}{figure.caption.69}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the left of socket A. The second row are screen captures of the ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the peg remained in contact with the ring of the socket until the top was reached before making a connection. (b) Same initial condition as in (a) but with socket C. The policy leads the peg down to the bottom corner of the socket before going the center of the top edge, localising itself, and then makes a connection.\relax }}{103}{figure.caption.70}}
\newlabel{fig:real_pictures}{{4.17}{103}{KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the left of socket A. The second row are screen captures of the ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the peg remained in contact with the ring of the socket until the top was reached before making a connection. (b) Same initial condition as in (a) but with socket C. The policy leads the peg down to the bottom corner of the socket before going the center of the top edge, localising itself, and then makes a connection.\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{103}{figure.caption.70}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{103}{figure.caption.70}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Distance taken to connect peg to socket once the socket is localised. (a) \textbf  {Socket A}. The human Group A are the set of teachers who first started with socket A, they did not have any other training before hand. Group B\textsuperscript  {*} first gave demonstrations on Socket B before giving demonstrations on Socket A. Group B\textsuperscript  {*} is better than group A at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the peg to the socket. (b) \textbf  {Socket B}. Both groups A\textsuperscript  {*} and B are similar in terms of the distance they took to insert the peg into the socket and as was the case for (a), the search policies travel less to accomplish the task. (c) Distance taken (measured from point of contact of peg with socket edge) to connect the peg to the socket. \relax }}{103}{figure.caption.71}}
\newlabel{fig:real_statistics}{{4.18}{103}{Distance taken to connect peg to socket once the socket is localised. (a) \textbf {Socket A}. The human Group A are the set of teachers who first started with socket A, they did not have any other training before hand. Group B\textsuperscript {*} first gave demonstrations on Socket B before giving demonstrations on Socket A. Group B\textsuperscript {*} is better than group A at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the peg to the socket. (b) \textbf {Socket B}. Both groups A\textsuperscript {*} and B are similar in terms of the distance they took to insert the peg into the socket and as was the case for (a), the search policies travel less to accomplish the task. (c) Distance taken (measured from point of contact of peg with socket edge) to connect the peg to the socket. \relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{103}{figure.caption.71}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{103}{figure.caption.71}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{103}{figure.caption.71}}
\citation{Chambrier2014}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Discussion \& Conclusion}{104}{section.4.8}}
\newlabel{ch4:conclusion}{{4.8}{104}{Discussion \& Conclusion}{section.4.8}{}}
\@writefile{brf}{\backcite{Chambrier2014}{{104}{4.8}{section.4.8}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Appendix}{105}{section.4.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}EM policy search}{105}{subsection.4.9.1}}
\newlabel{app:lb}{{4.9.1}{105}{EM policy search}{subsection.4.9.1}{}}
\citation{rl_gradient_survey_2013}
\newlabel{eq:lower_bound}{{4.15}{106}{EM policy search}{equation.4.9.15}{}}
\newlabel{eq:grad_log_cost_2}{{4.18}{106}{EM policy search}{equation.4.9.18}{}}
\@writefile{brf}{\backcite{rl_gradient_survey_2013}{{106}{4.9.1}{equation.4.9.18}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.2}Q-EM for GMM derivation}{106}{subsection.4.9.2}}
\newlabel{app:grad}{{4.9.2}{106}{Q-EM for GMM derivation}{subsection.4.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.3}Unbiased estimator}{107}{subsection.4.9.3}}
\newlabel{app:unbiased_delta}{{4.9.3}{107}{Unbiased estimator}{subsection.4.9.3}{}}
\bibstyle{plainnat}
\bibdata{bib/peg_hole.bib,bib/search_human_blind.bib,bib/RL.bib,bib/pomdp.bib,bib/cpomdp.bib,bib/citations.bib,bib/DT.bib,bib/ToM.bib,bib/spatial_navigation.bib,bib/ProspectTheory.bib,imitation_learning.bib,bib/ch3-citations.bib}
\bibcite{FIRM_2011}{{1}{2011}{{a.~Agha-mohammadi et~al.}}{{a.~Agha-mohammadi, Chakravorty, and Amato}}}
\bibcite{rob_online_bs_icra_2014}{{2}{2014}{{a.~Agha-mohammadi et~al.}}{{a.~Agha-mohammadi, Agarwal, Mahadevan, Chakravorty, Tomkins, Denny, and Amato}}}
\bibcite{sis_pomdp_2002}{{3}{2002}{{Aberdeen and Baxter}}{{}}}
\bibcite{sol_pdg_pbd_2014}{{4}{2014}{{Abu-Dakka et~al.}}{{Abu-Dakka, Nemec, Kramberger, Buch, Kr{\"u}ger, and Ude}}}
\bibcite{Arul_Mask_Clap_2002}{{5}{2002}{{Arulampalam et~al.}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\bibcite{Atkeson97locallyweighted}{{6}{1997}{{Atkeson et~al.}}{{Atkeson, Moore, and Schaal}}}
\bibcite{Bake_Saxe_Tene_2011}{{7}{2011}{{Bake et~al.}}{{Bake, Tenenbaum, and Saxe}}}
\bibcite{Bake_Tene_Saxe_2006}{{8}{2006}{{Baker et~al.}}{{Baker, Tenenbaum, and Saxe}}}
\bibcite{Baron-Cohen}{{9}{1995}{{Baron-Cohen}}{{}}}
\bibcite{gpomdp_2000}{{10}{2000}{{Baxter and Bartlett}}{{}}}
\bibcite{Bergman99recursivebayesian}{{11}{1999}{{Bergman and Bergman}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{109}{chapter*.72}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{Bernoulli1954}{{12}{1954}{{Bernoulli}}{{}}}
\bibcite{Billard_schol_2013}{{13}{2013}{{Billard and Grollman}}{{}}}
\bibcite{Billard08chapter}{{14}{2008{a}}{{Billard et~al.}}{{Billard, Calinon, Dillmann, and Schaal}}}
\bibcite{BillardCDS08}{{15}{2008{b}}{{Billard et~al.}}{{Billard, Calinon, Dillmann, and Schaal}}}
\bibcite{solving_continous_pomdps_2013}{{16}{2013}{{Brechtel et~al.}}{{Brechtel, Gindele, and Dillmann}}}
\bibcite{mc_update_ppomdps}{{17}{2011}{{Brooks and Williams}}{{}}}
\bibcite{spatial_memory_how_ego_allo_combine_2006}{{18}{2006}{{Burgess}}{{}}}
\bibcite{MRF_ToM}{{19}{2009}{{Butterfield et~al.}}{{Butterfield, Jenkins, Sobel, and Schwertfeger}}}
\bibcite{gesture_calinon_2010}{{20}{2010}{{Calinon et~al.}}{{Calinon, D'halluin, Sauser, Caldwell, and Billard}}}
\bibcite{ActingUncertainty_1996}{{21}{1996{a}}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{acting_uncer_1996}{{22}{1996{b}}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{Chambrier2014}{{23}{2014}{{Chambrier and Billard}}{{}}}
\bibcite{u_aware_grasp_ICRA_2015}{{24}{2015}{{Chen and von Wichert}}{{}}}
\bibcite{online_gpr_icra_2014}{{25}{2014}{{Cheng and Chen}}{{}}}
\bibcite{search_strategies_icra_2001}{{26}{2001}{{Chhatpar and Branicky}}{{}}}
\bibcite{p_search_surv_2011}{{27}{2011}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{rl_gradient_survey_2013}{{28}{2013{a}}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{DeisenrothNP2013}{{29}{2013{b}}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{ToM_HRI_2106}{{30}{2016}{{Devin and Alami}}{{}}}
\bibcite{POMDP_approach_2010}{{31}{2010}{{Du et~al.}}{{Du, Hsu, Kurniawati, Lee, Ong, and Png}}}
\bibcite{Erez10ascalable}{{32}{2010}{{Erez and Smart}}{{}}}
\bibcite{EGW05}{{33}{2005{a}}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{Tree_batch_2005}{{34}{2005{b}}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{hybrid_1992}{{35}{1992}{{Fisher and Mujtaba}}{{}}}
\bibcite{ac_survey_2012}{{36}{2012}{{Grondman et~al.}}{{Grondman, Busoniu, Lopes, and Babuska}}}
\bibcite{learn_admittance_icra_1994}{{37}{1994}{{Gullapalli et~al.}}{{Gullapalli, Barto, and Grupen}}}
\bibcite{Hamner_2006_5810}{{38}{2006}{{Hamner et~al.}}{{Hamner, Singh, and Scherer}}}
\bibcite{Hauser_2011}{{39}{2011}{{Hauser}}{{}}}
\bibcite{DRQ_AAAI_2015}{{40}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{Quadrator_2008}{{41}{2008}{{He et~al.}}{{He, Prentice, and Roy}}}
\bibcite{next_best_touch}{{42}{2013}{{Hebert et~al.}}{{Hebert, Howard, Hudson, Ma, and Burdick}}}
\bibcite{un_water_inspection_icra_2012}{{43}{2012}{{Hollinger et~al.}}{{Hollinger, Englot, Hover, Mitra, and Sukhatme}}}
\bibcite{Hsiao_RSS_10}{{44}{2010}{{Hsiao et~al.}}{{Hsiao, Kaelbling, and Lozano-Perez}}}
\bibcite{DiffEntropyHuber2008}{{45}{2008}{{Huber et~al.}}{{Huber, Bailey, Durrant-Whyte, and Hanebeck}}}
\bibcite{Iachini2014}{{46}{2014}{{Iachini et~al.}}{{Iachini, Ruggiero, and Ruotolo}}}
\bibcite{Efficient_touch_2012}{{47}{2012}{{Javdani et~al.}}{{Javdani, Klingensmith, Bagnell, Pollard, and Srinivasa}}}
\bibcite{DARPA_2015}{{48}{2015}{{Johnson et~al.}}{{Johnson, Shrewsbury, Bertrand, Wu, Duran, Floyd, Abeles, Stephen, Mertins, Lesman, Carff, Rifenburgh, Kaveti, Straatman, Smith, Griffioen, Layton, de~Boer, Koolen, Neuhaus, and Pratt}}}
\bibcite{Kaelbling_1998}{{49}{1998}{{Kaelbling et~al.}}{{Kaelbling, Littman, and Cassandra}}}
\bibcite{learn_force_c_icirs_2011}{{50}{2011}{{Kalakrishnan et~al.}}{{Kalakrishnan, Righetti, Pastor, and Schaal}}}
\bibcite{Kasper2001153}{{51}{2001}{{Kasper et~al.}}{{Kasper, Fricke, Steuernagel, and von Puttkamer}}}
\bibcite{heli_2004}{{52}{2004}{{Kim et~al.}}{{Kim, Jordan, Sastry, and Ng}}}
\bibcite{PoWER_2009}{{53}{2009}{{Kober and Peters}}{{}}}
\bibcite{RL_robots_surv_2013}{{54}{2013}{{Kober et~al.}}{{Kober, Bagnell, and Peters}}}
\bibcite{compliant_manip_icra_2008}{{55}{2008}{{kook Yun}}{{}}}
\bibcite{pancake_2010}{{56}{2010{a}}{{Kormushev et~al.}}{{Kormushev, Calinon, and Caldwell}}}
\bibcite{archery_2010}{{57}{2010{b}}{{Kormushev et~al.}}{{Kormushev, Calinon, Saegusa, and Metta}}}
\bibcite{SARSOP}{{58}{2008}{{Kurniawati et~al.}}{{Kurniawati, Hsu, and Lee}}}
\bibcite{human_stsm_2015}{{59}{}{{Lavenexa et~al.}}{{Lavenexa, Boujonb, Ndarugendamwob, and Lavenexa}}}
\bibcite{Leslie_TOMM}{{60}{1994}{{Leslie}}{{}}}
\bibcite{Li_2015}{{61}{2016}{{Li et~al.}}{{Li, Hang, Kragic, and Billard}}}
\bibcite{bs_compression_2010}{{62}{2010}{{Li et~al.}}{{Li, Cheung, and Liu}}}
\bibcite{GeorgiosLidoris}{{63}{2011}{{Lidoris}}{{}}}
\bibcite{qmdp_ijcnn_2014}{{64}{2014}{{Lin et~al.}}{{Lin, Lu, and Makedon}}}
\bibcite{Littman95}{{65}{1995}{{Littman et~al.}}{{Littman, Cassandra, and Kaelbling}}}
\bibcite{peg_imcssd_2015}{{66}{2015}{{M.~Bdiwi}}{{}}}
\bibcite{peg_personal_icra_2010}{{67}{2010}{{Meeussen et~al.}}{{Meeussen, Wise, Glaser, Chitta, McGann, Mihelich, Marder-Eppstein, Muja, Eruhimov, Foote, Hsu, Rusu, Marthi, Bradski, Konolige, Gerkey, and Berger}}}
\bibcite{cogprints730}{{68}{1956}{{Miller}}{{}}}
\bibcite{mnih-dqn-2015}{{69}{2015}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis}}}
\bibcite{trans_workpiece_icra_2013}{{70}{2013}{{Nemec et~al.}}{{Nemec, Abu-Dakka, Ridge, Ude, J\IeC {\o }rgensen, Savarimuthu, Jouffroy, Petersen, and Kr\IeC {\"u}ger}}}
\bibcite{NIPS2008_3501}{{71}{2009}{{Neumann and Peters}}{{}}}
\bibcite{Pegasus_2000}{{72}{2000}{{Ng and Jordan}}{{}}}
\bibcite{Nicolescu01learningand}{{73}{2001}{{Nicolescu and Mataric}}{{}}}
\bibcite{RL_book_sa}{{74}{2012}{{Now\'{e} et~al.}}{{Now\'{e}, Vrancx, and De~Hauwere}}}
\bibcite{where_look_2012}{{75}{2012}{{Nunez-Varela et~al.}}{{Nunez-Varela, Ravindran, and Wyatt}}}
\bibcite{intuitive_peg_isr_2013}{{76}{2013}{{Park et~al.}}{{Park, Bae, Park, Baeg, and Park}}}
\bibcite{Pasqualotto2013175}{{77}{2013}{{Pasqualotto et~al.}}{{Pasqualotto, Spiller, Jansari, and Proulx}}}
\bibcite{peter_nac_2008}{{78}{2008{a}}{{Peters and Schaal}}{{}}}
\bibcite{NAC_2008}{{79}{2008{b}}{{Peters and Schaal}}{{}}}
\bibcite{PBVI}{{80}{2003}{{Pineau et~al.}}{{Pineau, Gordon, and Thrun}}}
\bibcite{non_gauss_bel_plan_2012}{{81}{2012}{{Platt et~al.}}{{Platt, Kaelbling, Lozano-Perez, and Tedrake}}}
\bibcite{bsp_rss_2010a}{{82}{2010}{{Platt et~al.}}{{Platt, Tedrake, Kaelbling, and Lozano-P\'{e}rez}}}
\bibcite{PBVI_C_2006}{{83}{2006}{{Porta et~al.}}{{Porta, Vlassis, Spaan, and Poupart}}}
\bibcite{BelRoadMap_2009}{{84}{2009}{{Prentice and Roy}}{{}}}
\bibcite{decision_un_2013}{{85}{2013}{{Preuschoff et~al.}}{{Preuschoff, Mohr, and Hsu}}}
\bibcite{rai2013learning}{{86}{2013}{{Rai et~al.}}{{Rai, De~Chambrier, and Billard}}}
\bibcite{Sondik_1973}{{87}{1973}{{Richard D.~Smallwood}}{{}}}
\bibcite{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}{{88}{2012}{{Richardson et~al.}}{{Richardson, Bake, Tenenbaum, and Saxe}}}
\bibcite{Riedmiller05neuralfitted}{{89}{2005{a}}{{Riedmiller}}{{}}}
\bibcite{neura_fqi_2005}{{90}{2005{b}}{{Riedmiller}}{{}}}
\bibcite{Ross08onlineplanning}{{91}{2008}{{Ross et~al.}}{{Ross, Pineau, Paquet, and Chaib-draa}}}
\bibcite{CostalNavigation1999}{{92}{1999}{{Roy et~al.}}{{Roy, Burgard, Fox, and Thrun}}}
\bibcite{belief_compression_2005}{{93}{2005}{{Roy}}{{}}}
\bibcite{EPCA_2003}{{94}{2003}{{Roy and Gordon}}{{}}}
\bibcite{Roy99coastalnavigation}{{95}{1999}{{Roy and Thrun}}{{}}}
\bibcite{ToM_humanoid}{{96}{2002}{{Scassellati}}{{}}}
\bibcite{Schaal04learningmovement}{{97}{2004}{{Schaal et~al.}}{{Schaal, Peters, Nakanishi, and Ijspeert}}}
\bibcite{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}{{98}{2010}{{Silver et~al.}}{{Silver, Bagnell, and Stentz}}}
\bibcite{HSV}{{99}{2004}{{Smith and Simmons}}{{}}}
\bibcite{HSVI2}{{100}{2012}{{Smith and Simmons}}{{}}}
\bibcite{Towards_a_ToM_2010}{{101}{2010}{{Sodian and Kristen}}{{}}}
\bibcite{Spaan05icra}{{102}{2005}{{Spaan and Vlassis}}{{}}}
\bibcite{stachniss05robotics}{{103}{2005}{{Stachniss et~al.}}{{Stachniss, Grisetti, and Burgard}}}
\bibcite{stankiewicz2006lost}{{104}{2006}{{Stankiewicz et~al.}}{{Stankiewicz, Legge, Mansfield, and Schlicht}}}
\bibcite{dmp_iros_2011}{{105}{2011}{{Stulp et~al.}}{{Stulp, Theodorou, Kalakrishnan, Pastor, Righetti, and Schaal}}}
\bibcite{dmp_seq_2012}{{106}{2012}{{Stulp et~al.}}{{Stulp, Theodorou, and Schaal}}}
\bibcite{Needle_2014}{{107}{2014}{{Sun and Alterovitz}}{{}}}
\bibcite{gmr_2004}{{108}{2004}{{Sung}}{{}}}
\bibcite{sutton1998reinforcement}{{109}{1998}{{Sutton and Barto}}{{}}}
\bibcite{MC-POMDP}{{110}{2000}{{Thrun}}{{}}}
\bibcite{Thrun_2005}{{111}{2005}{{Thrun et~al.}}{{Thrun, Burgard, and Fox}}}
\bibcite{dense_entropy_icra_2014}{{112}{2014}{{Vallve and Andrade{-}Cetto}}{{}}}
\bibcite{LQG_MP_2011}{{113}{2011}{{Van Den~Berg et~al.}}{{Van Den~Berg, Abbeel, and Goldberg}}}
\bibcite{van_den_Berg_2012}{{114}{2012}{{van~den Berg et~al.}}{{van~den Berg, Patil, and Alterovitz}}}
\bibcite{FSVI}{{115}{2007}{{Veloso}}{{}}}
\bibcite{pomdp_iros_tous_2015}{{116}{2015}{{Vien and Toussaint}}{{}}}
\bibcite{eNAC_2003}{{117}{2003}{{Vijayakumar et~al.}}{{Vijayakumar, Shibata, and Schaal}}}
\bibcite{VonNeumann1944}{{118}{1990}{{Von~Neumann and Morgenstern}}{{}}}
\bibcite{Wang2016}{{119}{2016}{{Wang et~al.}}{{Wang, Uchibe, and Doya}}}
\bibcite{Wang_2007}{{120}{2007}{{Wang}}{{}}}
\bibcite{updating_egocentric_human_navigation_2000}{{121}{2000}{{Wang and Spelke}}{{}}}
\bibcite{reinforce_1992}{{122}{1992}{{Williams}}{{}}}
\bibcite{Biomechanics_2009}{{123}{2009}{{Winter}}{{}}}
\bibcite{what_det_our_nav_ability_2010}{{124}{2010}{{Wolbers and Hegarty}}{{}}}
\bibcite{spatial_updating_2008}{{125}{2008}{{Wolbers et~al.}}{{Wolbers, Hegarty, Buchel, and Loomis}}}
\bibcite{fast_peg_pbd_icmc_2014}{{126}{2014}{{Yang et~al.}}{{Yang, Lin, Song, Nemec, Ude, Buch, Kr\IeC {\"u}ger, and Savarimuthu}}}
\bibcite{seq_traj_replan_iros_2013}{{127}{2013}{{Zito et~al.}}{{Zito, Kopicki, Stolkin, Borst, Schmidt, Roa, and Wyatt}}}

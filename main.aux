\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{DARPA_2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}}
\@writefile{brf}{\backcite{DARPA_2015}{{1}{1.1}{section.1.1}}}
\citation{decision_un_2013}
\citation{ActingUncertainty_1996}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of the decision making under uncertainty in both robotics and everyday life situations. (a) European Space Agency (ESA), remote orbital peg in hole task. (b)-(c) ESA, simulated exploration of a cave on Mars in the dark. (d)-(e) MIT DAC team, Atlas robot doing valve task, \url  {http://drc.mit.edu/}. Other pictures include underwater exploration and industrial peg-in-hole assembly.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ch1-example}{{1.1}{2}{Examples of the decision making under uncertainty in both robotics and everyday life situations. (a) European Space Agency (ESA), remote orbital peg in hole task. (b)-(c) ESA, simulated exploration of a cave on Mars in the dark. (d)-(e) MIT DAC team, Atlas robot doing valve task, \url {http://drc.mit.edu/}. Other pictures include underwater exploration and industrial peg-in-hole assembly.\relax }{figure.caption.1}{}}
\@writefile{brf}{\backcite{decision_un_2013}{{2}{1.1}{figure.caption.1}}}
\@writefile{brf}{\backcite{ActingUncertainty_1996}{{2}{1.1}{figure.caption.1}}}
\citation{stankiewicz2006lost}
\citation{Billard08chapter}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{3}{1.1}{figure.caption.1}}}
\@writefile{brf}{\backcite{Billard08chapter}{{3}{1.1}{figure.caption.1}}}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contribution}{4}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Learning to reason with uncertainty as humans}{4}{subsection.1.2.1}}
\newlabel{sub:contr1}{{1.2.1}{4}{Learning to reason with uncertainty as humans}{subsection.1.2.1}{}}
\citation{rai2013learning}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{5}{1.2.1}{subsection.1.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Reinforcement learning in belief space}{5}{subsection.1.2.2}}
\newlabel{sub:contr2}{{1.2.2}{5}{Reinforcement learning in belief space}{subsection.1.2.2}{}}
\@writefile{brf}{\backcite{rai2013learning}{{5}{1.2.2}{subsection.1.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Non-parametric Bayesian state space filter}{6}{subsection.1.2.3}}
\newlabel{sub:contr3}{{1.2.3}{6}{Non-parametric Bayesian state space filter}{subsection.1.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Thesis outline}{6}{section.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Roadmap of the Thesis with key points. \relax }}{7}{figure.caption.2}}
\newlabel{fig:rmap_thesis}{{1.2}{7}{Roadmap of the Thesis with key points. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Chapter outline.\relax }}{9}{figure.caption.3}}
\newlabel{fig:ch2_outline}{{2.1}{9}{Chapter outline.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Decisions under uncertainty}{10}{section.2.1}}
\newlabel{sec:deci_un}{{2.1}{10}{Decisions under uncertainty}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Relation between beliefs, desires and actions and are all considered to be rational.\relax }}{11}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Decision theory}{11}{subsection.2.1.1}}
\newlabel{sec:ch2_DT}{{2.1.1}{11}{Decision theory}{subsection.2.1.1}{}}
\citation{Bernoulli1954}
\citation{VonNeumann1944}
\@writefile{brf}{\backcite{Bernoulli1954}{{12}{2.1.1}{figure.caption.4}}}
\newlabel{eq:exp_utility}{{2.1.1}{12}{Decision theory}{figure.caption.4}{}}
\@writefile{brf}{\backcite{VonNeumann1944}{{12}{2.1.1}{figure.caption.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sequential decision making}{13}{section.2.2}}
\newlabel{sec:sqp}{{2.2}{13}{Sequential decision making}{section.2.2}{}}
\newlabel{eq:joint_state_actions_util}{{2.2.1}{13}{Sequential decision making}{equation.2.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Definition of common variables used.\relax }}{14}{table.caption.5}}
\newlabel{tab:notation}{{2.1}{14}{Definition of common variables used.\relax }{table.caption.5}{}}
\newlabel{fig:mdp_off}{{2.3(a)}{15}{Subfigure 2 2.3(a)}{subfigure.2.3.1}{}}
\newlabel{sub@fig:mdp_off}{{(a)}{15}{Subfigure 2 2.3(a)\relax }{subfigure.2.3.1}{}}
\newlabel{fig:mdp_on}{{2.3(b)}{15}{Subfigure 2 2.3(b)}{subfigure.2.3.2}{}}
\newlabel{sub@fig:mdp_on}{{(b)}{15}{Subfigure 2 2.3(b)\relax }{subfigure.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf  {(a)} the decision nodes are not considered random variables whilst in \textbf  {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }}{15}{figure.caption.6}}
\newlabel{fig:mdp}{{2.3}{15}{Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf {(a)} the decision nodes are not considered random variables whilst in \textbf {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {off-policy}}}{15}{figure.caption.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {on-policy}}}{15}{figure.caption.6}}
\newlabel{eq:joint_state_actions}{{2.2.2}{16}{Sequential decision making}{equation.2.2.2}{}}
\newlabel{eq:temporal_expected_utility}{{2.2.4}{16}{Sequential decision making}{equation.2.2.4}{}}
\newlabel{eq:expansion}{{2.2.5}{16}{Sequential decision making}{equation.2.2.5}{}}
\newlabel{eq:bellman}{{2.2.6}{16}{Sequential decision making}{equation.2.2.6}{}}
\newlabel{eq:on_policy_bellman}{{2.2.7}{17}{Sequential decision making}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}POMDP}{17}{subsection.2.2.1}}
\newlabel{eq:sensor}{{2.2.8}{18}{POMDP}{equation.2.2.8}{}}
\newlabel{eq:likelihood}{{2.2.9}{18}{POMDP}{equation.2.2.9}{}}
\newlabel{fig:motion_update}{{2.4(a)}{19}{Subfigure 2 2.4(a)}{subfigure.2.4.1}{}}
\newlabel{sub@fig:motion_update}{{(a)}{19}{Subfigure 2 2.4(a)\relax }{subfigure.2.4.1}{}}
\newlabel{fig:measurement}{{2.4(b)}{19}{Subfigure 2 2.4(b)}{subfigure.2.4.2}{}}
\newlabel{sub@fig:measurement}{{(b)}{19}{Subfigure 2 2.4(b)\relax }{subfigure.2.4.2}{}}
\newlabel{fig:likelihood}{{2.4(c)}{19}{Subfigure 2 2.4(c)}{subfigure.2.4.3}{}}
\newlabel{sub@fig:likelihood}{{(c)}{19}{Subfigure 2 2.4(c)\relax }{subfigure.2.4.3}{}}
\newlabel{fig:measurement_update}{{2.4(d)}{19}{Subfigure 2 2.4(d)}{subfigure.2.4.4}{}}
\newlabel{sub@fig:measurement_update}{{(d)}{19}{Subfigure 2 2.4(d)\relax }{subfigure.2.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \textbf  {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf  {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf  {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf  {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }}{19}{figure.caption.7}}
\newlabel{fig:belief_update_example}{{2.4}{19}{\textbf {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{19}{figure.caption.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{19}{figure.caption.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{19}{figure.caption.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{19}{figure.caption.7}}
\newlabel{eq:motion_update}{{2.2.10}{20}{POMDP}{equation.2.2.10}{}}
\newlabel{eq:measurement_update}{{2.2.11}{20}{POMDP}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Bayesian state space filter.\relax }}{20}{figure.caption.8}}
\newlabel{fig:baysian_filter}{{2.5}{20}{Bayesian state space filter.\relax }{figure.caption.8}{}}
\newlabel{fig:sub_pomdp}{{2.6(a)}{21}{Subfigure 2 2.6(a)}{subfigure.2.6.1}{}}
\newlabel{sub@fig:sub_pomdp}{{(a)}{21}{Subfigure 2 2.6(a)\relax }{subfigure.2.6.1}{}}
\newlabel{fig:sub_bmdp}{{2.6(b)}{21}{Subfigure 2 2.6(b)}{subfigure.2.6.2}{}}
\newlabel{sub@fig:sub_bmdp}{{(b)}{21}{Subfigure 2 2.6(b)\relax }{subfigure.2.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \textbf  {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf  {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }}{21}{figure.caption.9}}
\newlabel{fig:pomdp}{{2.6}{21}{\textbf {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{21}{figure.caption.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{21}{figure.caption.9}}
\newlabel{eq:belief_bellman}{{2.2.15}{21}{POMDP}{equation.2.2.15}{}}
\citation{Sondik_1973}
\citation{Thrun_2005}
\citation{Kaelbling_1998}
\newlabel{eq:belief_state_transformation}{{2.2.16}{22}{POMDP}{equation.2.2.16}{}}
\newlabel{eq:max_component}{{2.2.17}{22}{POMDP}{equation.2.2.17}{}}
\newlabel{eq:final_belief_bellman}{{2.2.18}{22}{POMDP}{equation.2.2.18}{}}
\@writefile{brf}{\backcite{Sondik_1973}{{22}{2.2.1}{equation.2.2.18}}}
\@writefile{brf}{\backcite{Thrun_2005}{{22}{2.2.1}{equation.2.2.18}}}
\@writefile{brf}{\backcite{Kaelbling_1998}{{22}{2.2.1}{equation.2.2.18}}}
\citation{POMDP_approach_2010}
\citation{Thrun_2005}
\citation{PBVI}
\citation{HSV}
\citation{HSVI2}
\citation{FSVI}
\citation{SARSOP}
\citation{POMDP_approach_2010}
\@writefile{brf}{\backcite{POMDP_approach_2010}{{23}{2.2.1}{equation.2.2.18}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Literature review}{23}{section.2.3}}
\newlabel{sec:lit_rev}{{2.3}{23}{Literature review}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Value Iteration}{23}{subsection.2.3.1}}
\newlabel{lit:VI}{{2.3.1}{23}{Value Iteration}{subsection.2.3.1}{}}
\@writefile{brf}{\backcite{Thrun_2005}{{23}{2.3.1}{subsection.2.3.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Mind-map of AI and robotic methods for acting under uncertainty.\relax }}{24}{figure.caption.10}}
\newlabel{fig:mindmap}{{2.7}{24}{Mind-map of AI and robotic methods for acting under uncertainty.\relax }{figure.caption.10}{}}
\citation{Spaan05icra}
\citation{PBVI_C_2006}
\citation{solving_continous_pomdps_2013}
\@writefile{toc}{\contentsline {subsubsection}{Point-base Value Iteration}{25}{section*.11}}
\@writefile{brf}{\backcite{PBVI}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{HSV}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{HSVI2}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{FSVI}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{SARSOP}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{POMDP_approach_2010}{{25}{2.3.1}{section*.11}}}
\citation{Ross08onlineplanning}
\@writefile{brf}{\backcite{Spaan05icra}{{26}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{PBVI_C_2006}{{26}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{solving_continous_pomdps_2013}{{26}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{Ross08onlineplanning}{{26}{2.3.1}{section*.11}}}
\@writefile{toc}{\contentsline {subsubsection}{Approximate Value Iteration}{26}{section*.12}}
\citation{MC-POMDP}
\citation{Tree_batch_2005}
\citation{mc_update_ppomdps}
\citation{neura_fqi_2005}
\citation{DRQ_AAAI_2015}
\citation{mnih-dqn-2015}
\@writefile{brf}{\backcite{MC-POMDP}{{27}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{Tree_batch_2005}{{27}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{mc_update_ppomdps}{{27}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{neura_fqi_2005}{{27}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{DRQ_AAAI_2015}{{27}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{mnih-dqn-2015}{{27}{2.3.1}{section*.12}}}
\citation{Roy99coastalnavigation}
\citation{belief_compression_2005}
\citation{EPCA_2003}
\citation{bs_compression_2010}
\@writefile{toc}{\contentsline {subsubsection}{Latent Value Iteration}{28}{section*.13}}
\@writefile{brf}{\backcite{Roy99coastalnavigation}{{28}{2.3.1}{section*.13}}}
\@writefile{brf}{\backcite{belief_compression_2005}{{28}{2.3.1}{section*.13}}}
\@writefile{brf}{\backcite{EPCA_2003}{{28}{2.3.1}{section*.13}}}
\@writefile{brf}{\backcite{bs_compression_2010}{{28}{2.3.1}{section*.13}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: Value Iteration}{28}{section*.14}}
\citation{gpomdp_2000}
\citation{reinforce_1992}
\citation{gpomdp_2000}
\citation{sis_pomdp_2002}
\citation{Pegasus_2000}
\citation{heli_2004}
\citation{dmp_iros_2011}
\citation{dmp_seq_2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Policy search}{29}{subsection.2.3.2}}
\newlabel{lit:policy_search}{{2.3.2}{29}{Policy search}{subsection.2.3.2}{}}
\@writefile{brf}{\backcite{gpomdp_2000}{{29}{2.3.2}{subsection.2.3.2}}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient: policy search}{29}{section*.15}}
\@writefile{brf}{\backcite{reinforce_1992}{{29}{2.3.2}{section*.15}}}
\citation{PoWER_2009}
\citation{archery_2010}
\citation{pancake_2010}
\citation{Wang2016}
\citation{p_search_surv_2011}
\citation{RL_robots_surv_2013}
\@writefile{brf}{\backcite{gpomdp_2000}{{30}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{sis_pomdp_2002}{{30}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{Pegasus_2000}{{30}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{heli_2004}{{30}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{dmp_iros_2011}{{30}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{dmp_seq_2012}{{30}{2.3.2}{section*.15}}}
\@writefile{toc}{\contentsline {subsubsection}{Expectation-Maximisation: policy search}{30}{section*.16}}
\@writefile{brf}{\backcite{PoWER_2009}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{archery_2010}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{pancake_2010}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{Wang2016}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{p_search_surv_2011}{{30}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{RL_robots_surv_2013}{{30}{2.3.2}{section*.16}}}
\citation{ac_survey_2012}
\citation{eNAC_2003}
\citation{NAC_2008}
\@writefile{toc}{\contentsline {subsubsection}{Actor-critic: policy search}{31}{section*.17}}
\@writefile{brf}{\backcite{ac_survey_2012}{{31}{2.3.2}{section*.17}}}
\@writefile{brf}{\backcite{eNAC_2003}{{31}{2.3.2}{section*.17}}}
\@writefile{brf}{\backcite{NAC_2008}{{31}{2.3.2}{section*.17}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: policy search}{31}{section*.18}}
\citation{BelRoadMap_2009}
\citation{Quadrator_2008}
\citation{FIRM_2011}
\citation{rob_online_bs_icra_2014}
\citation{bsp_rss_2010a}
\citation{LQG_MP_2011}
\citation{Erez10ascalable}
\citation{van_den_Berg_2012}
\citation{Needle_2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Planning}{32}{subsection.2.3.3}}
\newlabel{lit:Planning}{{2.3.3}{32}{Planning}{subsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Belief space road maps}{32}{section*.19}}
\@writefile{brf}{\backcite{BelRoadMap_2009}{{32}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{Quadrator_2008}{{32}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{FIRM_2011}{{32}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{rob_online_bs_icra_2014}{{32}{2.3.3}{section*.19}}}
\@writefile{toc}{\contentsline {subsubsection}{Optimal control}{32}{section*.20}}
\@writefile{brf}{\backcite{bsp_rss_2010a}{{32}{2.3.3}{section*.20}}}
\citation{non_gauss_bel_plan_2012}
\citation{seq_traj_replan_iros_2013}
\@writefile{brf}{\backcite{LQG_MP_2011}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{Erez10ascalable}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{van_den_Berg_2012}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{Needle_2014}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{non_gauss_bel_plan_2012}{{33}{2.3.3}{section*.20}}}
\@writefile{brf}{\backcite{seq_traj_replan_iros_2013}{{33}{2.3.3}{section*.20}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: planning}{33}{section*.21}}
\citation{un_water_inspection_icra_2012}
\citation{u_aware_grasp_ICRA_2015}
\citation{Li_2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Heuristics}{34}{subsection.2.3.4}}
\newlabel{lit:heuristics}{{2.3.4}{34}{Heuristics}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Myopic \& Q-MDP}{34}{section*.22}}
\@writefile{brf}{\backcite{un_water_inspection_icra_2012}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{u_aware_grasp_ICRA_2015}{{34}{2.3.4}{section*.22}}}
\citation{Littman95}
\citation{RL_book_sa}
\citation{Thrun_2005}
\citation{acting_uncer_1996}
\citation{qmdp_ijcnn_2014}
\citation{where_look_2012}
\citation{Hauser_2011}
\citation{pomdp_iros_tous_2015}
\citation{CostalNavigation1999}
\citation{stachniss05robotics}
\citation{dense_entropy_icra_2014}
\@writefile{brf}{\backcite{Li_2015}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Littman95}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{RL_book_sa}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Thrun_2005}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{acting_uncer_1996}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{qmdp_ijcnn_2014}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{where_look_2012}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Hauser_2011}{{35}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{pomdp_iros_tous_2015}{{35}{2.3.4}{section*.22}}}
\@writefile{toc}{\contentsline {subsubsection}{Information gain}{35}{section*.23}}
\citation{Hsiao_RSS_10}
\citation{Efficient_touch_2012}
\citation{next_best_touch}
\@writefile{brf}{\backcite{CostalNavigation1999}{{36}{2.3.4}{section*.23}}}
\@writefile{brf}{\backcite{stachniss05robotics}{{36}{2.3.4}{section*.23}}}
\@writefile{brf}{\backcite{dense_entropy_icra_2014}{{36}{2.3.4}{section*.23}}}
\@writefile{brf}{\backcite{Hsiao_RSS_10}{{36}{2.3.4}{section*.23}}}
\@writefile{brf}{\backcite{Efficient_touch_2012}{{36}{2.3.4}{section*.23}}}
\@writefile{brf}{\backcite{next_best_touch}{{36}{2.3.4}{section*.23}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: heuristic}{36}{section*.24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Summary: literature}{37}{subsection.2.3.5}}
\citation{Billard08chapter}
\citation{Billard_schol_2013}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Approach}{38}{section.2.4}}
\newlabel{sec:approach}{{2.4}{38}{Approach}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }}{39}{figure.caption.25}}
\newlabel{fig:mind_summary}{{2.8}{39}{Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }{figure.caption.25}{}}
\@writefile{brf}{\backcite{Billard08chapter}{{39}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{Billard_schol_2013}{{39}{2.4}{section.2.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }}{40}{figure.caption.26}}
\newlabel{fig:belief-pipeline}{{2.9}{40}{Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \textbf  {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf  {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf  {Learn policy:} A generative policy, $\policy  (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }}{41}{figure.caption.27}}
\newlabel{fig:human_search}{{2.10}{41}{\textbf {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf {Learn policy:} A generative policy, $\policy (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }{figure.caption.27}{}}
\citation{Biomechanics_2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }}{42}{figure.caption.28}}
\newlabel{fig:control_architecture}{{2.11}{42}{Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }{figure.caption.28}{}}
\@writefile{brf}{\backcite{Biomechanics_2009}{{42}{2.4}{figure.caption.27}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning to reason with uncertainty as humans}{43}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf  {Blindfolded search task} \textit  {Left:} Search task, a human demonstrator searching for the green wooden block on the table given that both his hearing and vision senses have been impeded. He starts (hand) at the white spot near position (1). The the red and blue trajectories are examples of possible searches. \textit  {Middle:} Inferred belief the human might have with respect to his position. If the human always starts at (1) and his belief is known, all following beliefs (2) can be inferred from Bayes rule. \textit  {Right:} WAM Robot 7 DOF reproduces the search strategies demonstrated by humans to find the object.\relax }}{44}{figure.caption.29}}
\newlabel{fig:searching}{{3.1}{44}{\textbf {Blindfolded search task} \textit {Left:} Search task, a human demonstrator searching for the green wooden block on the table given that both his hearing and vision senses have been impeded. He starts (hand) at the white spot near position (1). The the red and blue trajectories are examples of possible searches. \textit {Middle:} Inferred belief the human might have with respect to his position. If the human always starts at (1) and his belief is known, all following beliefs (2) can be inferred from Bayes rule. \textit {Right:} WAM Robot 7 DOF reproduces the search strategies demonstrated by humans to find the object.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Outline}{45}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Background}{45}{section.3.2}}
\newlabel{ch3:background}{{3.2}{45}{Background}{section.3.2}{}}
\citation{Wang_2007}
\citation{what_det_our_nav_ability_2010}
\citation{spatial_updating_2008}
\citation{spatial_memory_how_ego_allo_combine_2006}
\citation{updating_egocentric_human_navigation_2000}
\citation{Pasqualotto2013175}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Spatial navigation}{46}{subsection.3.2.1}}
\@writefile{brf}{\backcite{Wang_2007}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{what_det_our_nav_ability_2010}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{spatial_updating_2008}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{spatial_memory_how_ego_allo_combine_2006}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{updating_egocentric_human_navigation_2000}{{46}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{Pasqualotto2013175}{{46}{3.2.1}{subsection.3.2.1}}}
\citation{spatial_memory_how_ego_allo_combine_2006}
\citation{cogprints730}
\citation{human_stsm_2015}
\citation{Iachini2014}
\citation{stankiewicz2006lost}
\@writefile{brf}{\backcite{spatial_memory_how_ego_allo_combine_2006}{{47}{3.2.1}{subsection.3.2.1}}}
\@writefile{toc}{\contentsline {subsubsection}{Spatial cognition and memory}{47}{section*.30}}
\@writefile{brf}{\backcite{cogprints730}{{47}{3.2.1}{section*.30}}}
\@writefile{brf}{\backcite{human_stsm_2015}{{47}{3.2.1}{section*.30}}}
\@writefile{brf}{\backcite{Iachini2014}{{47}{3.2.1}{section*.30}}}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{47}{3.2.1}{section*.30}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: spatial cognition}{47}{section*.31}}
\citation{stankiewicz2006lost}
\citation{Towards_a_ToM_2010}
\citation{ToM_humanoid}
\citation{Leslie_TOMM}
\citation{Baron-Cohen}
\citation{MRF_ToM}
\citation{ToM_HRI_2106}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{48}{3.2.1}{section*.31}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Human beliefs}{48}{subsection.3.2.2}}
\@writefile{brf}{\backcite{Towards_a_ToM_2010}{{48}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{ToM_humanoid}{{48}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Leslie_TOMM}{{48}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Baron-Cohen}{{48}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{MRF_ToM}{{48}{3.2.2}{subsection.3.2.2}}}
\citation{Bake_Saxe_Tene_2011}
\citation{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}
\citation{Bake_Tene_Saxe_2006}
\citation{Bake_Saxe_Tene_2011}
\citation{Kasper2001153}
\citation{Hamner_2006_5810}
\citation{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}
\citation{Nicolescu01learningand}
\@writefile{brf}{\backcite{ToM_HRI_2106}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Bake_Tene_Saxe_2006}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{49}{3.2.2}{subsection.3.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Programming by demonstration \& uncertainty}{49}{subsection.3.2.3}}
\@writefile{brf}{\backcite{Kasper2001153}{{49}{3.2.3}{subsection.3.2.3}}}
\@writefile{brf}{\backcite{Hamner_2006_5810}{{49}{3.2.3}{subsection.3.2.3}}}
\citation{GeorgiosLidoris}
\@writefile{brf}{\backcite{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}{{50}{3.2.3}{subsection.3.2.3}}}
\@writefile{brf}{\backcite{Nicolescu01learningand}{{50}{3.2.3}{subsection.3.2.3}}}
\@writefile{brf}{\backcite{GeorgiosLidoris}{{50}{3.2.3}{subsection.3.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Experiment: table search}{50}{section.3.3}}
\newlabel{ch3:experiment}{{3.3}{50}{Experiment: table search}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Table search task. Blindfolded human subjects after a disorientation step are placed in one of the four starting locations. The heading of the subject is always kept the same. The human's objective is to locate the green block on the table. Throughout all experiments the green wooden block is kept in the same location.\relax }}{51}{figure.caption.32}}
\newlabel{fig:tab_search_task}{{3.2}{51}{Table search task. Blindfolded human subjects after a disorientation step are placed in one of the four starting locations. The heading of the subject is always kept the same. The human's objective is to locate the green block on the table. Throughout all experiments the green wooden block is kept in the same location.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Formulation}{51}{section.3.4}}
\newlabel{ch3:formulation}{{3.4}{51}{Formulation}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \textit  {Top left}: A participant is trying to locate the green wooden block on the table given that both vision and hearing senses have been inhibited. The location of his hand is being tracked by the OptiTrack\textsuperscript  {\textregistered } system. \textit  {Top right:} Initial distribution of the uncertainty or belief we assume the human has with respect to his position. \textit  {Bottom left:} Set of recorded searches, the trajectories are with respect to the hand. \textit  {Bottom right:} Trajectories starting from same area but have different search patterns, the red trajectories all navigate to the goal via the top right corner as opposed to the blue which go by the bottom left and right corner. Among these two groups there are trajectories which seem to minimize the distance taken to reach the goal as opposed to some which seek to stay close to the edge and corners.\relax }}{52}{figure.caption.33}}
\newlabel{fig:experiment}{{3.3}{52}{\textit {Top left}: A participant is trying to locate the green wooden block on the table given that both vision and hearing senses have been inhibited. The location of his hand is being tracked by the OptiTrack\textsuperscript {\textregistered } system. \textit {Top right:} Initial distribution of the uncertainty or belief we assume the human has with respect to his position. \textit {Bottom left:} Set of recorded searches, the trajectories are with respect to the hand. \textit {Bottom right:} Trajectories starting from same area but have different search patterns, the red trajectories all navigate to the goal via the top right corner as opposed to the blue which go by the bottom left and right corner. Among these two groups there are trajectories which seem to minimize the distance taken to reach the goal as opposed to some which seek to stay close to the edge and corners.\relax }{figure.caption.33}{}}
\citation{Arul_Mask_Clap_2002}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {subsubsection}{Belief model}{53}{section*.34}}
\@writefile{brf}{\backcite{Arul_Mask_Clap_2002}{{54}{3.4}{equation.3.4.1}}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{54}{3.4}{equation.3.4.1}}}
\@writefile{toc}{\contentsline {subsubsection}{Sensing model}{54}{section*.35}}
\newlabel{eq:sensingfunction}{{3.4.3}{54}{Sensing model}{equation.3.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Four different time frames of the evolution of the belief particle filter. \textit  {Top left}: Initial belief distribution; a lot of uncertainty. \textit  {Top right:} First contact is made with the table, the measurement likelihood restrains the samples to be on the table's surface. \textit  {Bottom right:} First contact is an edge. \textit  {Bottom left:} Gradual localisation.\relax }}{55}{figure.caption.36}}
\newlabel{fig:pf_example}{{3.4}{55}{Four different time frames of the evolution of the belief particle filter. \textit {Top left}: Initial belief distribution; a lot of uncertainty. \textit {Top right:} First contact is made with the table, the measurement likelihood restrains the samples to be on the table's surface. \textit {Bottom right:} First contact is an edge. \textit {Bottom left:} Gradual localisation.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motion model}{55}{section*.37}}
\@writefile{toc}{\contentsline {subsubsection}{Uncertainty}{55}{section*.38}}
\citation{DiffEntropyHuber2008}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Representation of the estimated density function. \textit  {Top Left and Right:} Initial starting point, all Gaussian functions are uniformly distributed with uniform priors. The red cluster always has the highest likelihood which is taken to be the believed location of the robot's/human's end-effector. \textit  {Bottom Left:} Contact with the table has been established, the robot location differers from his belief. \textit  {Bottom Right:} Contact has been made with a corner, the clusters reflect that the robot could be at any corner (note that weights are not depicted, only cluster assignment).\relax }}{56}{figure.caption.39}}
\newlabel{fig:clustering}{{3.5}{56}{Representation of the estimated density function. \textit {Top Left and Right:} Initial starting point, all Gaussian functions are uniformly distributed with uniform priors. The red cluster always has the highest likelihood which is taken to be the believed location of the robot's/human's end-effector. \textit {Bottom Left:} Contact with the table has been established, the robot location differers from his belief. \textit {Bottom Right:} Contact has been made with a corner, the clusters reflect that the robot could be at any corner (note that weights are not depicted, only cluster assignment).\relax }{figure.caption.39}{}}
\newlabel{eq:gmm1}{{3.4.4}{56}{Uncertainty}{equation.3.4.4}{}}
\citation{BillardCDS08}
\@writefile{brf}{\backcite{DiffEntropyHuber2008}{{57}{3.4}{figure.caption.39}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Policies}{57}{section.3.5}}
\newlabel{chap3:policies}{{3.5}{57}{Policies}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Modelling human search strategies}{57}{subsection.3.5.1}}
\newlabel{chap3:GMM_policy}{{3.5.1}{57}{Modelling human search strategies}{subsection.3.5.1}{}}
\@writefile{brf}{\backcite{BillardCDS08}{{57}{3.5.1}{subsection.3.5.1}}}
\citation{CostalNavigation1999}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \textit  {Left: } Resulting search GMM, a total of 67 Gaussian mixture components are present. We note the many overlapping Gaussians: this results from the level of uncertainty over the different choices taken. For example humans follow along the edge of the table in different directions and might leave the edge once they are confident with respect to their location. \textit  {Right:} Information Gain map of the table environment, dark regions indicate high information gain as oppose to lighter ones. Not surprisingly, the highest are the corners, followed by the edges.\relax }}{58}{figure.caption.40}}
\newlabel{fig:gmm}{{3.6}{58}{\textit {Left: } Resulting search GMM, a total of 67 Gaussian mixture components are present. We note the many overlapping Gaussians: this results from the level of uncertainty over the different choices taken. For example humans follow along the edge of the table in different directions and might leave the edge once they are confident with respect to their location. \textit {Right:} Information Gain map of the table environment, dark regions indicate high information gain as oppose to lighter ones. Not surprisingly, the highest are the corners, followed by the edges.\relax }{figure.caption.40}{}}
\newlabel{eq:gmm2}{{3.5.1}{58}{Modelling human search strategies}{equation.3.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Coastal Navigation}{58}{subsection.3.5.2}}
\newlabel{chap3:costal_policy}{{3.5.2}{58}{Coastal Navigation}{subsection.3.5.2}{}}
\@writefile{brf}{\backcite{CostalNavigation1999}{{58}{3.5.2}{subsection.3.5.2}}}
\newlabel{eq:objective_function}{{3.5.2}{58}{Coastal Navigation}{equation.3.5.2}{}}
\newlabel{eq:IG}{{3.5.3}{59}{Coastal Navigation}{equation.3.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Control}{59}{subsection.3.5.3}}
\newlabel{eq:conditional}{{3.5.4}{59}{Control}{equation.3.5.4}{}}
\newlabel{eq:GMR}{{3.5.5}{59}{Control}{equation.3.5.5}{}}
\newlabel{eq:weight}{{3.5.6}{60}{Control}{equation.3.5.6}{}}
\newlabel{eq:w_expectation}{{3.5.7}{60}{Control}{equation.3.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Overview of the decision loop. At the top a strategy is chosen given an initial belief $p(x_{0}|y_{0})$ of the location of the end-effector (initially through sampling the conditional). A speed is applied to the given direction based on the believed distance to the goal. This velocity is passed onwards to a low level impedance controller which sends out the required torques. The resulting sensation, encoded through the Multinomial distribution over the environment features, and actual displacement are sent back to update the belief.\relax }}{61}{figure.caption.41}}
\newlabel{fig:flow_chart}{{3.7}{61}{Overview of the decision loop. At the top a strategy is chosen given an initial belief $p(x_{0}|y_{0})$ of the location of the end-effector (initially through sampling the conditional). A speed is applied to the given direction based on the believed distance to the goal. This velocity is passed onwards to a low level impedance controller which sends out the required torques. The resulting sensation, encoded through the Multinomial distribution over the environment features, and actual displacement are sent back to update the belief.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Results and discussion}{62}{section.3.6}}
\newlabel{chap3:results}{{3.6}{62}{Results and discussion}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Search \& behaviour analysis}{62}{subsection.3.6.1}}
\newlabel{sub:search_behaviour}{{3.6.1}{62}{Search \& behaviour analysis}{subsection.3.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Expected sensation. Plots of the expected sensation of the edge and corner feature for all trajectories. The axes are associated with the sensor measurements, 0 means that the corresponding feature is not sensed and 1 the feature is fully sensed. A point in the plots summarises a whole trajectory by the mean and variance of the probability of sensing a corner or edge. The radius of the circles are proportional to the variance. The doted blue rectangle represents the decision boundary for classifying a trajectory as being either risk-prone or risk-averse. A point which lies inside the rectangle is risk-prone. \textit  {Left:} Human trajectories demonstrate a wide variety of behaviours ranging from those remaining close to features to those preferring more risk. \textit  {Right:} Red points show Greedy and blue points the GMM model. \textit  {Bottom:} Green circles are associated with the Hybrid method whilst orange are those of the Coastal navigation method. The Hybrid method is a skewed version of the GMM which tends towards risky behaviour and exhibits the same kind of behaviour as the Coastal algorithm.\relax }}{63}{figure.caption.42}}
\newlabel{fig:expectedfeatures}{{3.8}{63}{Expected sensation. Plots of the expected sensation of the edge and corner feature for all trajectories. The axes are associated with the sensor measurements, 0 means that the corresponding feature is not sensed and 1 the feature is fully sensed. A point in the plots summarises a whole trajectory by the mean and variance of the probability of sensing a corner or edge. The radius of the circles are proportional to the variance. The doted blue rectangle represents the decision boundary for classifying a trajectory as being either risk-prone or risk-averse. A point which lies inside the rectangle is risk-prone. \textit {Left:} Human trajectories demonstrate a wide variety of behaviours ranging from those remaining close to features to those preferring more risk. \textit {Right:} Red points show Greedy and blue points the GMM model. \textit {Bottom:} Green circles are associated with the Hybrid method whilst orange are those of the Coastal navigation method. The Hybrid method is a skewed version of the GMM which tends towards risky behaviour and exhibits the same kind of behaviour as the Coastal algorithm.\relax }{figure.caption.42}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Percentage of risk-prone trajectories based on two decision criteria, the feature (f) and the risk (r) (information gain) metrics discussed above.\relax }}{64}{table.caption.44}}
\newlabel{tab:percentage-risk-prone}{{3.1}{64}{Percentage of risk-prone trajectories based on two decision criteria, the feature (f) and the risk (r) (information gain) metrics discussed above.\relax }{table.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Risk of searches. Illustration of risk-prone and risk-averse searches in terms of a Risk factor (\textit  {left}) and expected sensation (\textit  {right}). \textit  {Left:} Each trajectory was reduced to a single scalar, which we call the Risk factor, quantifying the risk of a trajectory. The Risk factor is inversely proportional to the sum of the information gain of a particular trajectory. The colour paired dots (risk averse) and squares (risk prone) represent trajectories which are plotted in Figure \ref  {fig:risk_examples}, to illustrate that these correspond to risk averse and prone searches. \textit  {Right:} Corresponding trajectories chosen in the Risk factor space but represented in the feature space. As expected, trajectories with a high risk map to regions of low expected feature. However the transition from the Risk space to feature space is non-linear and will result in a different risk-level classification than the feature metric previously discussed.\relax }}{65}{figure.caption.43}}
\newlabel{fig:riskexamples}{{3.9}{65}{Risk of searches. Illustration of risk-prone and risk-averse searches in terms of a Risk factor (\textit {left}) and expected sensation (\textit {right}). \textit {Left:} Each trajectory was reduced to a single scalar, which we call the Risk factor, quantifying the risk of a trajectory. The Risk factor is inversely proportional to the sum of the information gain of a particular trajectory. The colour paired dots (risk averse) and squares (risk prone) represent trajectories which are plotted in Figure \ref {fig:risk_examples}, to illustrate that these correspond to risk averse and prone searches. \textit {Right:} Corresponding trajectories chosen in the Risk factor space but represented in the feature space. As expected, trajectories with a high risk map to regions of low expected feature. However the transition from the Risk space to feature space is non-linear and will result in a different risk-level classification than the feature metric previously discussed.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Risk prone \& averse searches (red \& green trajectories). \textit  {Top left:} Two human trajectories taken from data shown in Figure \ref  {fig:riskexamples}. \textit  {Top right:} Two Greedy trajectories. \textit  {Bottom left:} GMM trajectories, all starting from the same location, the colour coding is to illustrate the different policies which were encoded and emerge given the same initial conditions. \textit  {Bottom right:} Corresponding expected features of each trajectory, the colour coding matches the trajectories to the ``GMM risk types'' sub-figure. All the searches which were generated by the GMM for this initialisation produced risk-averse searches (based on the feature metric discussed previous).\relax }}{66}{figure.caption.45}}
\newlabel{fig:risk_examples}{{3.10}{66}{Risk prone \& averse searches (red \& green trajectories). \textit {Top left:} Two human trajectories taken from data shown in Figure \ref {fig:riskexamples}. \textit {Top right:} Two Greedy trajectories. \textit {Bottom left:} GMM trajectories, all starting from the same location, the colour coding is to illustrate the different policies which were encoded and emerge given the same initial conditions. \textit {Bottom right:} Corresponding expected features of each trajectory, the colour coding matches the trajectories to the ``GMM risk types'' sub-figure. All the searches which were generated by the GMM for this initialisation produced risk-averse searches (based on the feature metric discussed previous).\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}GMM \& Coastal Navigation policy analysis}{67}{subsection.3.6.2}}
\newlabel{sub:policy_analysis}{{3.6.2}{67}{GMM \& Coastal Navigation policy analysis}{subsection.3.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Illustration of three different types of modes present during the execution of the task where the robot is being controlled by the learned GMM model. The white ball represents the actual position of the robot's end-effector. The blue ball represents the believed position of the robot's end-effector and the robot is acting according to it. The blue ball arrows represent modes. Colours encode the mode's weights given by the priors $\pi _{k}$ after conditioning ( but not re-weighted as previously described). The spectrum ranges from red (high weight) to blue (low weight). \textit  {Top left:} Three modes are present, but two agree with each other. \textit  {Top right:} Three modes are again present indicating appropriate ways to reduce the uncertainty. \textit  {Lower left:} Two modes are in opposing directions. No flipping behaviour between modes occurs since preference is given to the modes pointing in the same direction as the robot's current trajectory. \textit  {Lower right:} GMM modes when conditioned on the state represented in the lower left figure. The two modes represent the possible directions (un-normalised).\relax }}{68}{figure.caption.46}}
\newlabel{fig:modes}{{3.11}{68}{Illustration of three different types of modes present during the execution of the task where the robot is being controlled by the learned GMM model. The white ball represents the actual position of the robot's end-effector. The blue ball represents the believed position of the robot's end-effector and the robot is acting according to it. The blue ball arrows represent modes. Colours encode the mode's weights given by the priors $\pi _{k}$ after conditioning ( but not re-weighted as previously described). The spectrum ranges from red (high weight) to blue (low weight). \textit {Top left:} Three modes are present, but two agree with each other. \textit {Top right:} Three modes are again present indicating appropriate ways to reduce the uncertainty. \textit {Lower left:} Two modes are in opposing directions. No flipping behaviour between modes occurs since preference is given to the modes pointing in the same direction as the robot's current trajectory. \textit {Lower right:} GMM modes when conditioned on the state represented in the lower left figure. The two modes represent the possible directions (un-normalised).\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Illustration of the vector field for the Coastal and GMM policy. \textit  {Top Left} Coastal policy, there is only one possible direction for every state at any time, the values of $\lambda _2$ in the cost function were set experimentally. \textit  {Others:} The GMM policy for three different levels of uncertainty. For each point multiple actions are possible which is reflected by the number of arrows (only the first three most likely actions). As the uncertainty decreases the policy becomes less multi-model, but remains around the edges and corners. Note that once certain of being close to an edge there is a possibility to go either straight to the goal or stay close to the edge and corners.\relax }}{69}{figure.caption.47}}
\newlabel{fig:vectorfield}{{3.12}{69}{Illustration of the vector field for the Coastal and GMM policy. \textit {Top Left} Coastal policy, there is only one possible direction for every state at any time, the values of $\lambda _2$ in the cost function were set experimentally. \textit {Others:} The GMM policy for three different levels of uncertainty. For each point multiple actions are possible which is reflected by the number of arrows (only the first three most likely actions). As the uncertainty decreases the policy becomes less multi-model, but remains around the edges and corners. Note that once certain of being close to an edge there is a possibility to go either straight to the goal or stay close to the edge and corners.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Distance efficiency \& Uncertainty}{70}{subsection.3.6.3}}
\newlabel{sub:time_uncertainty}{{3.6.3}{70}{Distance efficiency \& Uncertainty}{subsection.3.6.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Mean distance and (variance) taken to reach the goal for 3 methods in 5 experiments. The grey shaded entries correspond to the results of the search algorithm which obtained the fastest time to reach the goal in each type of experiment/search.\relax }}{70}{table.caption.49}}
\newlabel{tab:mean-var-distance}{{3.2}{70}{Mean distance and (variance) taken to reach the goal for 3 methods in 5 experiments. The grey shaded entries correspond to the results of the search algorithm which obtained the fastest time to reach the goal in each type of experiment/search.\relax }{table.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Four search initializations, from \textit  {top left} to \textit  {bottom right} we refer to them as \#1-4. The circle indicates the true starting point of the end-effector (eof), whilst the triangle is the initial believed location of the eof. The initialisation in \#1 was chosen such that the true and believed eof locations were at opposite sides of the table. This setting was selected to highlight the draw back in methods which do not take into account uncertainty. The second initialisation \#2, reflects the situation where once again there is a large distance between true and believed location of the eof. However this time both are above the table. The starting points in \#3 are a variant on \#1 with the difference being that the believed eof position is above the table whilst the true eof location is not. The last experiment \#4 was a setup which would be favourable to algorithms that are inclined to be greedy. Both true and believed eof locations are close to one another.\relax }}{71}{figure.caption.48}}
\newlabel{fig:four-initialisations}{{3.13}{71}{Four search initializations, from \textit {top left} to \textit {bottom right} we refer to them as \#1-4. The circle indicates the true starting point of the end-effector (eof), whilst the triangle is the initial believed location of the eof. The initialisation in \#1 was chosen such that the true and believed eof locations were at opposite sides of the table. This setting was selected to highlight the draw back in methods which do not take into account uncertainty. The second initialisation \#2, reflects the situation where once again there is a large distance between true and believed location of the eof. However this time both are above the table. The starting points in \#3 are a variant on \#1 with the difference being that the believed eof position is above the table whilst the true eof location is not. The last experiment \#4 was a setup which would be favourable to algorithms that are inclined to be greedy. Both true and believed eof locations are close to one another.\relax }{figure.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces ANOVA tests the null hypothesis that all search experiments produced the same type of search with respect to the distance taken to reach the goal. All the p-values are extremely small which indicate that the null hypothesis can safely be rejected.\relax }}{72}{table.caption.50}}
\newlabel{tab:anova-1}{{3.3}{72}{ANOVA tests the null hypothesis that all search experiments produced the same type of search with respect to the distance taken to reach the goal. All the p-values are extremely small which indicate that the null hypothesis can safely be rejected.\relax }{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces ANOVA between paired search methods. The first column gives an indication of the probability that both the Greedy and GMM searches are statistically the same (the null hypothesis). This was rejected with a tolerance of below \%1. In the second column, Greedy vs Coastal searches \#1 and \#4 are statistically closer than the rest with a p-value threshold of 10\% required to be able to reject the null hypothesis. In the third column the uniform and \#3 are not statistically different and would require a higher threshold on the p-value to be so.\relax }}{72}{table.caption.51}}
\newlabel{fig:anova-2}{{3.4}{72}{ANOVA between paired search methods. The first column gives an indication of the probability that both the Greedy and GMM searches are statistically the same (the null hypothesis). This was rejected with a tolerance of below \%1. In the second column, Greedy vs Coastal searches \#1 and \#4 are statistically closer than the rest with a p-value threshold of 10\% required to be able to reject the null hypothesis. In the third column the uniform and \#3 are not statistically different and would require a higher threshold on the p-value to be so.\relax }{table.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Reduction of the uncertainty for the Uniform, \#1, \#2 and \#4 experiment, the expected value is reported \textit  {Top left}: Uniform initialisation, expected uncertainty for the Greedy (red), GMM (blue), Hybrid (green) \& Coastal (orange) search strategies. \textit  {Top right:} Experiment \#1. \textit  {Bottom left:} Experiment \#2. \textit  {Bottom right:} Experiment \#4.\relax }}{73}{figure.caption.52}}
\newlabel{fig:uncertainty}{{3.14}{73}{Reduction of the uncertainty for the Uniform, \#1, \#2 and \#4 experiment, the expected value is reported \textit {Top left}: Uniform initialisation, expected uncertainty for the Greedy (red), GMM (blue), Hybrid (green) \& Coastal (orange) search strategies. \textit {Top right:} Experiment \#1. \textit {Bottom left:} Experiment \#2. \textit {Bottom right:} Experiment \#4.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Conclusions}{73}{section.3.7}}
\citation{Bake_Saxe_Tene_2011}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{74}{3.7}{section.3.7}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Peg in hole}{75}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textit  {Top-right:} The experimental setup. Blindfolded human teachers stand in the orange rectangle always facing the wall. Human teachers are informed of their starting location and told that they would always be facing the wall. \textit  {Top-left:} Three different power sockets. The plug is connected to a cylinder, to make it easy to hold for the human teachers. An ATI 6-axis force torque sensor (Nano25 Series) is between the cylinder and the plug. \textit  {Bottom-left} Human teacher performing the PiH search task. The human teachers wear goggles to remove sight and ear defenders to greatly diminish hearing.\textit  {Bottom-right:} The KUKA LWR robot equipped with the same force torque sensor and plug used by the human teachers.\relax }}{76}{figure.caption.53}}
\newlabel{fig:experiment_setup}{{4.1}{76}{\textit {Top-right:} The experimental setup. Blindfolded human teachers stand in the orange rectangle always facing the wall. Human teachers are informed of their starting location and told that they would always be facing the wall. \textit {Top-left:} Three different power sockets. The plug is connected to a cylinder, to make it easy to hold for the human teachers. An ATI 6-axis force torque sensor (Nano25 Series) is between the cylinder and the plug. \textit {Bottom-left} Human teacher performing the PiH search task. The human teachers wear goggles to remove sight and ear defenders to greatly diminish hearing.\textit {Bottom-right:} The KUKA LWR robot equipped with the same force torque sensor and plug used by the human teachers.\relax }{figure.caption.53}{}}
\citation{search_strategies_icra_2001}
\citation{online_gpr_icra_2014}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Background}{77}{section.4.1}}
\newlabel{ch4:background}{{4.1}{77}{Background}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Peg-in-hole}{77}{subsection.4.1.1}}
\@writefile{brf}{\backcite{search_strategies_icra_2001}{{77}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{online_gpr_icra_2014}{{77}{4.1.1}{subsection.4.1.1}}}
\citation{peg_personal_icra_2010}
\citation{hybrid_1992}
\citation{fast_peg_pbd_icmc_2014}
\citation{Schaal04learningmovement}
\citation{trans_workpiece_icra_2013}
\citation{sol_pdg_pbd_2014}
\citation{learn_force_c_icirs_2011}
\citation{learn_admittance_icra_1994}
\@writefile{brf}{\backcite{peg_personal_icra_2010}{{78}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{hybrid_1992}{{78}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{fast_peg_pbd_icmc_2014}{{78}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{Schaal04learningmovement}{{78}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{trans_workpiece_icra_2013}{{78}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{sol_pdg_pbd_2014}{{78}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{learn_force_c_icirs_2011}{{78}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{learn_admittance_icra_1994}{{78}{4.1.1}{subsection.4.1.1}}}
\citation{search_strategies_icra_2001}
\citation{peg_imcssd_2015}
\citation{intuitive_peg_isr_2013}
\citation{compliant_manip_icra_2008}
\citation{online_gpr_icra_2014}
\@writefile{brf}{\backcite{search_strategies_icra_2001}{{79}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{peg_imcssd_2015}{{79}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{intuitive_peg_isr_2013}{{79}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{compliant_manip_icra_2008}{{79}{4.1.1}{subsection.4.1.1}}}
\@writefile{brf}{\backcite{online_gpr_icra_2014}{{79}{4.1.1}{subsection.4.1.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Experiment}{79}{section.4.2}}
\newlabel{ch4:experiment}{{4.2}{79}{Experiment}{section.4.2}{}}
\citation{Bergman99recursivebayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textit  {Left}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated. \textit  {Right:} Time taken for the teachers to accomplish the PiH once the socket is localised. Group A and B are depicted in red and blue. The asterisk indicates that the group has changed sockets, so Group A\textsuperscript  {*} means that Group A is now performing the task with socket B and Group B\textsuperscript  {*} means that group B is now performing the task with socket A.\relax }}{81}{figure.caption.54}}
\newlabel{fig:experiment_setup_data}{{4.2}{81}{\textit {Left}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated. \textit {Right:} Time taken for the teachers to accomplish the PiH once the socket is localised. Group A and B are depicted in red and blue. The asterisk indicates that the group has changed sockets, so Group A\textsuperscript {*} means that Group A is now performing the task with socket B and Group B\textsuperscript {*} means that group B is now performing the task with socket A.\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Formulation}{81}{section.4.3}}
\newlabel{ch4:formulation}{{4.3}{81}{Formulation}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Belief probability density function}{81}{subsection.4.3.1}}
\@writefile{brf}{\backcite{Bergman99recursivebayesian}{{81}{4.3.1}{subsection.4.3.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textit  {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug, the orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit  {Right}: \textbf  {World model}: The plug is presented by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf  {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one. \relax }}{82}{figure.caption.55}}
\newlabel{fig:PMF}{{4.3}{82}{\textit {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug, the orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit {Right}: \textbf {World model}: The plug is presented by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one. \relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Belief compression}{82}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Learning Actor and Critic}{83}{section.4.4}}
\newlabel{sec:learning-value-actor}{{4.4}{83}{Learning Actor and Critic}{section.4.4}{}}
\newlabel{eq:value_function}{{4.4.1}{83}{Learning Actor and Critic}{equation.4.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Actor}{83}{subsection.4.4.1}}
\newlabel{eq:GMM}{{4.4.2}{83}{Actor}{equation.4.4.2}{}}
\citation{Atkeson97locallyweighted}
\citation{EGW05}
\citation{NIPS2008_3501,EGW05,Riedmiller05neuralfitted}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Critic}{84}{subsection.4.4.2}}
\@writefile{brf}{\backcite{Atkeson97locallyweighted}{{84}{4.4.2}{subsection.4.4.2}}}
\newlabel{eq:W}{{4.4.3}{84}{Critic}{equation.4.4.3}{}}
\newlabel{eq:lwr_predict}{{4.4.4}{84}{Critic}{equation.4.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fitted Policy Evaluation}{84}{section*.56}}
\@writefile{brf}{\backcite{EGW05}{{84}{4.4.2}{section*.56}}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces Fitted Policy Evaluation\relax }}{84}{algorithm.4.1}}
\newlabel{alg:fpe}{{4.1}{84}{Fitted Policy Evaluation\relax }{algorithm.4.1}{}}
\citation{sutton1998reinforcement}
\citation{DeisenrothNP2013}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces \textit  {Left:} LWR approximate value function $\mathaccentV {hat}05E{V}^{\pi }(b)$. \textit  {Right:} first five best and worst trajectories in terms of the accumulated value.\relax }}{85}{figure.caption.57}}
\newlabel{fig:Figure1}{{4.4}{85}{\textit {Left:} LWR approximate value function $\hat {V}^{\pi }(\B )$. \textit {Right:} first five best and worst trajectories in terms of the accumulated value.\relax }{figure.caption.57}{}}
\@writefile{brf}{\backcite{NIPS2008_3501}{{85}{4.4.2}{algorithm.4.1}}}
\@writefile{brf}{\backcite{EGW05}{{85}{4.4.2}{algorithm.4.1}}}
\@writefile{brf}{\backcite{Riedmiller05neuralfitted}{{85}{4.4.2}{algorithm.4.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Actor update}{85}{subsection.4.4.3}}
\@writefile{brf}{\backcite{sutton1998reinforcement}{{85}{4.4.3}{subsection.4.4.3}}}
\@writefile{brf}{\backcite{DeisenrothNP2013}{{85}{4.4.3}{subsection.4.4.3}}}
\citation{peter_nac_2008}
\newlabel{eq:disc_return}{{4.4.5}{86}{Actor update}{equation.4.4.5}{}}
\newlabel{eq:expected_reward}{{4.4.6}{86}{Actor update}{equation.4.4.6}{}}
\newlabel{eq:grad_log_cost}{{4.4.7}{86}{Actor update}{equation.4.4.7}{}}
\newlabel{eq:advantage_f}{{4.4.8}{86}{Actor update}{equation.4.4.8}{}}
\@writefile{brf}{\backcite{peter_nac_2008}{{86}{4.4.3}{equation.4.4.8}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Control architecture}{86}{section.4.5}}
\newlabel{seq:control_architecture}{{4.5}{86}{Control architecture}{section.4.5}{}}
\citation{gesture_calinon_2010}
\citation{gmr_2004}
\newlabel{eq:gmm_conditional}{{4.5.1}{87}{Control architecture}{equation.4.5.1}{}}
\@writefile{brf}{\backcite{gesture_calinon_2010}{{87}{4.5}{equation.4.5.1}}}
\@writefile{brf}{\backcite{gmr_2004}{{87}{4.5}{equation.4.5.1}}}
\newlabel{eq:alpha_eq}{{4.5.2}{87}{Control architecture}{equation.4.5.2}{}}
\newlabel{eq:alpha_expectation}{{4.5.3}{87}{Control architecture}{equation.4.5.3}{}}
\newlabel{eq:modulation}{{4.5.4}{87}{Control architecture}{equation.4.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Control architecture. The PMF (belief) received a measured velocity, $\mathaccentV {dot}05F{\mathaccentV {tilde}07E{x}}$, and sensor feature $\mathaccentV {tilde}07E{y}$ and gets updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller, Equation \ref  {eq:prop_speed}.\relax }}{88}{figure.caption.58}}
\newlabel{fig:control_flow}{{4.5}{88}{Control architecture. The PMF (belief) received a measured velocity, $\dot {\tilde {x}}$, and sensor feature $\tilde {y}$ and gets updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller, Equation \ref {eq:prop_speed}.\relax }{figure.caption.58}{}}
\newlabel{eq:prop_speed}{{4.5.5}{88}{Control architecture}{equation.4.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Q-EM and GMM policy vector fields. \textit  {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit  {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation.\relax }}{89}{figure.caption.59}}
\newlabel{fig:policy_vf}{{4.6}{89}{Q-EM and GMM policy vector fields. \textit {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation.\relax }{figure.caption.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Results}{90}{section.4.6}}
\newlabel{ch4:results}{{4.6}{90}{Results}{section.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Distance taken to reach the socket's edge (Qualitative)}{90}{subsection.4.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Three simulated search experiments. \textbf  {Experiment 1:} Three start positions are considered: \textit  {Left}, \textit  {Center} and \textit  {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. In the second row of Experiment 1, we illustrate the trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textbf  {Experiment 2:} Two cases are considered: \textit  {Case 1} blue, the initial belief state (circle) is fixed facing the left edge of the wall and the true location (diamond) is facing the socket. \textit  {Case 2} pink, the initial belief state (circle) is fixed to the right facing the edge of the wall and the true location is the left edge of the wall. In the second row, the trajectories are plotted for \textit  {Case 1}. \textbf  {Experiment 3:} A 150 start locations are deterministically generated from a grid in the start area. In the second row, we plot the distribution of the areas visited by the true position during the search.\relax }}{91}{figure.caption.60}}
\newlabel{fig:box_exp_sim}{{4.7}{91}{Three simulated search experiments. \textbf {Experiment 1:} Three start positions are considered: \textit {Left}, \textit {Center} and \textit {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. In the second row of Experiment 1, we illustrate the trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textbf {Experiment 2:} Two cases are considered: \textit {Case 1} blue, the initial belief state (circle) is fixed facing the left edge of the wall and the true location (diamond) is facing the socket. \textit {Case 2} pink, the initial belief state (circle) is fixed to the right facing the edge of the wall and the true location is the left edge of the wall. In the second row, the trajectories are plotted for \textit {Case 1}. \textbf {Experiment 3:} A 150 start locations are deterministically generated from a grid in the start area. In the second row, we plot the distribution of the areas visited by the true position during the search.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Distance taken to reach the socket's edge (Quantitative)}{92}{subsection.4.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces First contact with the wall, during experiment 1. (a) Contact distribution for initial condition ``Center'' . (b) Contact distribution for initial condition was ``Right''. The ellipses correspond to two standard deviations of a fitted Gaussian function.\relax }}{93}{figure.caption.61}}
\newlabel{fig:first_contact}{{4.8}{93}{First contact with the wall, during experiment 1. (a) Contact distribution for initial condition ``Center'' . (b) Contact distribution for initial condition was ``Right''. The ellipses correspond to two standard deviations of a fitted Gaussian function.\relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Distance travelled until the socket's edge is reached. (a) Three groups correspond to the initial conditions: Center, Left and Right depicted in Figure \ref  {fig:box_exp_sim}, \textit  {top left}. The Q-EM method is always better than the other methods, in terms of distance. (b) Results of the two initial conditions depicted in Figure \ref  {fig:box_exp_sim}, \textit  {top middle}, both the true position and most likely state are fixed. The Q-EM method always improves on the GMM. (c) Results corresponding to Experiment 3, Figure \ref  {fig:box_exp_sim}, \textit  {top right}. Again the Q-EM method is better, but at a less significant level.\relax }}{94}{figure.caption.62}}
\newlabel{fig:three_searches}{{4.9}{94}{Distance travelled until the socket's edge is reached. (a) Three groups correspond to the initial conditions: Center, Left and Right depicted in Figure \ref {fig:box_exp_sim}, \textit {top left}. The Q-EM method is always better than the other methods, in terms of distance. (b) Results of the two initial conditions depicted in Figure \ref {fig:box_exp_sim}, \textit {top middle}, both the true position and most likely state are fixed. The Q-EM method always improves on the GMM. (c) Results corresponding to Experiment 3, Figure \ref {fig:box_exp_sim}, \textit {top right}. Again the Q-EM method is better, but at a less significant level.\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{94}{figure.caption.62}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{94}{figure.caption.62}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{94}{figure.caption.62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Importance of data}{94}{subsection.4.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Original teacher \# 5 demonstrations. The teacher demonstrates a preference to go first to the top of the wall. He then leaves contact with the wall to position himself in front of the socket before trying to find it\relax }}{95}{figure.caption.63}}
\newlabel{fig:subj_5_traj}{{4.10}{95}{Original teacher \# 5 demonstrations. The teacher demonstrates a preference to go first to the top of the wall. He then leaves contact with the wall to position himself in front of the socket before trying to find it\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Value function learned from the 15 demonstrations of teacher \#5.\relax }}{95}{figure.caption.64}}
\newlabel{fig:value_function_subj_5}{{4.11}{95}{Value function learned from the 15 demonstrations of teacher \#5.\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Marginalised Gaussian Mixture parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. The illustrated transparency of the Gaussian functions is proportional to their weight. \textit  {Left column}: The Gaussian functions of the Q-EM have shifted from the left corner to the right. This is a result of the value function being higher in the top right corner region, see Figure \ref  {fig:value_function_subj_5}. \textit  {Center column}: The original data of the teacher went quite far back which results in a Gaussian function given a direction which moves away from the wall (green arrow), whilst in the case of the Q-EM parameters this effect is reduced and moved closer towards the wall. We can also see from the two plots of the Q-EM parameters that they then follow the paths encoded by the value function. \textit  {Right column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge.\relax }}{96}{figure.caption.65}}
\newlabel{fig:gmm_exp4}{{4.12}{96}{Marginalised Gaussian Mixture parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. The illustrated transparency of the Gaussian functions is proportional to their weight. \textit {Left column}: The Gaussian functions of the Q-EM have shifted from the left corner to the right. This is a result of the value function being higher in the top right corner region, see Figure \ref {fig:value_function_subj_5}. \textit {Center column}: The original data of the teacher went quite far back which results in a Gaussian function given a direction which moves away from the wall (green arrow), whilst in the case of the Q-EM parameters this effect is reduced and moved closer towards the wall. We can also see from the two plots of the Q-EM parameters that they then follow the paths encoded by the value function. \textit {Right column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge.\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.\relax }}{97}{figure.caption.66}}
\newlabel{fig:experiment4_stats}{{4.13}{97}{Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Generalisation}{97}{subsection.4.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit  {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. \relax }}{98}{figure.caption.67}}
\newlabel{fig:experiment5_traj}{{4.14}{98}{Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. \relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{98}{figure.caption.67}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Distance taken to reach the socket's edge. For the Fixed setup (see Figure \ref  {fig:experiment5_traj}) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. \relax }}{98}{figure.caption.68}}
\newlabel{fig:experiment5_stats}{{4.15}{98}{Distance taken to reach the socket's edge. For the Fixed setup (see Figure \ref {fig:experiment5_traj}) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. \relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{98}{figure.caption.68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.5}Distance taken to connect the plug to the socket}{99}{subsection.4.6.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces 25 search trajectories for each of the three search policies for socket A. \relax }}{100}{figure.caption.69}}
\newlabel{fig:real_policy}{{4.16}{100}{25 search trajectories for each of the three search policies for socket A. \relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{100}{figure.caption.69}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row are screen captures of the ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. (b) Same initial condition as in (a) but with socket C. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then makes a connection.\relax }}{101}{figure.caption.70}}
\newlabel{fig:real_pictures}{{4.17}{101}{KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row are screen captures of the ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. (b) Same initial condition as in (a) but with socket C. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then makes a connection.\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{101}{figure.caption.70}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{101}{figure.caption.70}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Distance taken to connect plug to socket once the socket is localised. (a) \textbf  {Socket A}. The human Group A are the set of teachers who first started with socket A. They had no previous training on another socket beforehand. Group B\textsuperscript  {*} first gave demonstrations on Socket B before giving demonstrations on Socket A. Group B\textsuperscript  {*} is better than group A at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the plug to the socket. (b) \textbf  {Socket B}. Both groups A\textsuperscript  {*} and B are similar in terms of the distance they took to insert the plug into the socket and as was the case for (a), the search policies travel less to accomplish the task. (c) Distance taken (measured from point of contact of plug with socket edge) to connect the plug to the socket. \relax }}{101}{figure.caption.71}}
\newlabel{fig:real_statistics}{{4.18}{101}{Distance taken to connect plug to socket once the socket is localised. (a) \textbf {Socket A}. The human Group A are the set of teachers who first started with socket A. They had no previous training on another socket beforehand. Group B\textsuperscript {*} first gave demonstrations on Socket B before giving demonstrations on Socket A. Group B\textsuperscript {*} is better than group A at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the plug to the socket. (b) \textbf {Socket B}. Both groups A\textsuperscript {*} and B are similar in terms of the distance they took to insert the plug into the socket and as was the case for (a), the search policies travel less to accomplish the task. (c) Distance taken (measured from point of contact of plug with socket edge) to connect the plug to the socket. \relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{101}{figure.caption.71}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{101}{figure.caption.71}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{101}{figure.caption.71}}
\citation{Chambrier2014}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Discussion \& Conclusion}{102}{section.4.7}}
\newlabel{ch4:conclusion}{{4.7}{102}{Discussion \& Conclusion}{section.4.7}{}}
\@writefile{brf}{\backcite{Chambrier2014}{{102}{4.7}{section.4.7}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Appendix}{103}{section.4.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}EM policy search}{103}{subsection.4.8.1}}
\newlabel{app:lb}{{4.8.1}{103}{EM policy search}{subsection.4.8.1}{}}
\citation{rl_gradient_survey_2013}
\newlabel{eq:lower_bound}{{4.8.2}{104}{EM policy search}{equation.4.8.2}{}}
\newlabel{eq:grad_log_cost_2}{{4.8.5}{104}{EM policy search}{equation.4.8.5}{}}
\@writefile{brf}{\backcite{rl_gradient_survey_2013}{{104}{4.8.1}{equation.4.8.5}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Q-EM for GMM derivation}{104}{subsection.4.8.2}}
\newlabel{app:grad}{{4.8.2}{104}{Q-EM for GMM derivation}{subsection.4.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Unbiased estimator}{105}{subsection.4.8.3}}
\newlabel{app:unbiased_delta}{{4.8.3}{105}{Unbiased estimator}{subsection.4.8.3}{}}
\citation{Thrun_Burgard_Fox_2005}
\citation{Thrun02particlefilters,negative_info_markov_localisation}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Non-parametric Bayesian State Space Estimator}{107}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Thrun_Burgard_Fox_2005}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces  \textit  {Table Environment} Table World (delimited by the black rectangle), viewed from above, and the agent's beliefs. There are three different probability density functions present on the table. The blue represents the believed location of the agent, the red and green probability distributions are associated with object 1 and 2. The white shapes in each figure represent the true location of each associated object or agent.\relax }}{108}{figure.caption.72}}
\newlabel{fig:Figure1}{{5.1}{108}{\textit {Table Environment} Table World (delimited by the black rectangle), viewed from above, and the agent's beliefs. There are three different probability density functions present on the table. The blue represents the believed location of the agent, the red and green probability distributions are associated with object 1 and 2. The white shapes in each figure represent the true location of each associated object or agent.\relax }{figure.caption.72}{}}
\@writefile{brf}{\backcite{Thrun_Burgard_Fox_2005}{{108}{5}{figure.caption.72}}}
\@writefile{brf}{\backcite{Thrun02particlefilters}{{108}{5}{figure.caption.72}}}
\@writefile{brf}{\backcite{negative_info_markov_localisation}{{108}{5}{figure.caption.72}}}
\@writefile{brf}{\backcite{Thrun_Burgard_Fox_2005}{{108}{5}{figure.caption.72}}}
\citation{NegInfoFurtherStudies}
\citation{Bake_Saxe_Tene_2011}
\citation{deChambrier2013}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Assumptions and attributes which have to be fulfilled by our Bayesian State Space Filter. \relax }}{109}{figure.caption.73}}
\newlabel{fig:ch5_assmuptions}{{5.2}{109}{Assumptions and attributes which have to be fulfilled by our Bayesian State Space Filter. \relax }{figure.caption.73}{}}
\@writefile{brf}{\backcite{NegInfoFurtherStudies}{{109}{5}{figure.caption.73}}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{110}{5}{figure.caption.73}}}
\@writefile{brf}{\backcite{deChambrier2013}{{110}{5}{figure.caption.73}}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Outline}{110}{section.5.1}}
\citation{Plagemann07gaussianbeam}
\citation{DataAssociation2003}
\citation{SLAM_part1}
\citation{TutGraphSLAM}
\citation{FastSLAM}
\citation{Thrun_Burgard_Fox_2005}
\citation{SLAM_HBR}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Background}{111}{section.5.2}}
\newlabel{ch5:background}{{5.2}{111}{Background}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}SLAM}{111}{subsection.5.2.1}}
\@writefile{brf}{\backcite{Plagemann07gaussianbeam}{{111}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{DataAssociation2003}{{111}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{SLAM_part1}{{111}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{TutGraphSLAM}{{111}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{FastSLAM}{{111}{5.2.1}{subsection.5.2.1}}}
\citation{Thrun_grid_based_1996}
\citation{Kollar_2008_Exploration_SLAM}
\citation{Navigation_strategires_for_exploring_indoor_environments}
\citation{PRM_1996}
\citation{RRT-SLAM}
\citation{ActivePosSLAM}
\citation{stachniss05robotics}
\citation{Ross08onlineplanning}
\citation{GeorgiosLidoris}
\citation{Active_SLAM_Uncertainty_compar,tovar_planning,KL_SLAM_exploration_PF}
\@writefile{brf}{\backcite{Thrun_Burgard_Fox_2005}{{112}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{SLAM_HBR}{{112}{5.2.1}{subsection.5.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Active-SLAM \& Exploration}{112}{subsection.5.2.2}}
\@writefile{brf}{\backcite{Thrun_grid_based_1996}{{112}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{Kollar_2008_Exploration_SLAM}{{112}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{Navigation_strategires_for_exploring_indoor_environments}{{112}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{PRM_1996}{{112}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{RRT-SLAM}{{112}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{ActivePosSLAM}{{112}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{stachniss05robotics}{{112}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{Ross08onlineplanning}{{112}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{GeorgiosLidoris}{{112}{5.2.2}{subsection.5.2.2}}}
\citation{BayesBall}
\citation{barberBRML2012}
\@writefile{brf}{\backcite{Active_SLAM_Uncertainty_compar}{{113}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{tovar_planning}{{113}{5.2.2}{subsection.5.2.2}}}
\@writefile{brf}{\backcite{KL_SLAM_exploration_PF}{{113}{5.2.2}{subsection.5.2.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Bayesian State Space Estimation}{113}{section.5.3}}
\newlabel{ch5:BSSE}{{5.3}{113}{Bayesian State Space Estimation}{section.5.3}{}}
\newlabel{eq:joint}{{5.3.1}{113}{Bayesian State Space Estimation}{equation.5.3.1}{}}
\@writefile{brf}{\backcite{BayesBall}{{113}{5.3}{figure.caption.74}}}
\@writefile{brf}{\backcite{barberBRML2012}{{113}{5.3}{figure.caption.74}}}
\@writefile{toc}{\contentsline {subsubsection}{EKF-SLAM}{113}{section*.76}}
\newlabel{sec:EKF-SLAM}{{5.3}{113}{EKF-SLAM}{section*.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Directed graphical model of dependencies between the agent(A), objects(O), sensing(Y) and action(u) random variables. Each object, $O^{(i)}$ is associated with one sensing random variable $Y^{(i)}$. The overall sensing random variable is $Y = \left [Y^{(1)},\dots  ,Y^{(M-1)}\right ]^{\mathrm  {T}}$, where $M$ is the total number of agent and object random variables in the filter. For readability we have left out the time index $t$ from $A$ and $Y$. Since the objects are static, they have no temporal process associated with them thus they will never have a time subscript. The two models necessary for filtering are the motion model $P(A_t|A_{t-1},u_t)$ (red) and measurement model $P(Y_t|A_t,O)$ (blue).\relax }}{114}{figure.caption.74}}
\newlabel{fig:bayesian_sse_dag}{{5.3}{114}{Directed graphical model of dependencies between the agent(A), objects(O), sensing(Y) and action(u) random variables. Each object, $O^{(i)}$ is associated with one sensing random variable $Y^{(i)}$. The overall sensing random variable is $Y = \left [Y^{(1)},\dots ,Y^{(M-1)}\right ]^{\mathrm {T}}$, where $M$ is the total number of agent and object random variables in the filter. For readability we have left out the time index $t$ from $A$ and $Y$. Since the objects are static, they have no temporal process associated with them thus they will never have a time subscript. The two models necessary for filtering are the motion model $P(A_t|A_{t-1},u_t)$ (red) and measurement model $P(Y_t|A_t,O)$ (blue).\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Dependence and independence relation between the random variables of the BN Figure \ref  {fig:bayesian_sse_dag}\relax }}{114}{figure.caption.75}}
\newlabel{fig:ch5_dseperation}{{5.4}{114}{Dependence and independence relation between the random variables of the BN Figure \ref {fig:bayesian_sse_dag}\relax }{figure.caption.75}{}}
\newlabel{eq:lik-measurement}{{5.3.3}{115}{EKF-SLAM}{equation.5.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Histogram SLAM}{115}{section*.78}}
\newlabel{sec:Discrete}{{5.3}{115}{Histogram SLAM}{section*.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Time slices of the evolution of the pdfs according to EKF-SLAM. The numbers in the top right corner of each plot indicate the temporal ordering. The blue pdf represents the agent's believed location and the circle with the dot in the middle is the true location of the agent. The same holds for the red distribution which represents the agent's belief of the location of an object. \textit  {Top:} evolution of the Kalman gain, $K$, parameter according to the measurement error $e$. $K_a$ is the gain update for the agent's Gaussian parameters and $K_o$ is the gain update for the object. \textit  {Middle:} evolution of the covariance components of $\Sigma $ over time. $\Sigma _a$ and $\Sigma _o$ are the variances of the agent and object positions and $\Sigma _{ao}$ is the cross covariance term. \textit  {Bottom:} true sensing $Y_t$ and the expected sensation $\mathaccentV {hat}05E{Y}_t$. 1D EKF-SLAM, the figures to the left show the evolution of the agent's belief of his location and that of an object which are represented by the blue and red Gaussian functions. The figures to the right show the evolution of the parameters of the EKF-SLAM.\relax }}{116}{figure.caption.77}}
\newlabel{fig:EKF-SLAM}{{5.5}{116}{Time slices of the evolution of the pdfs according to EKF-SLAM. The numbers in the top right corner of each plot indicate the temporal ordering. The blue pdf represents the agent's believed location and the circle with the dot in the middle is the true location of the agent. The same holds for the red distribution which represents the agent's belief of the location of an object. \textit {Top:} evolution of the Kalman gain, $K$, parameter according to the measurement error $e$. $K_a$ is the gain update for the agent's Gaussian parameters and $K_o$ is the gain update for the object. \textit {Middle:} evolution of the covariance components of $\Sigma $ over time. $\Sigma _a$ and $\Sigma _o$ are the variances of the agent and object positions and $\Sigma _{ao}$ is the cross covariance term. \textit {Bottom:} true sensing $Y_t$ and the expected sensation $\hat {Y}_t$. 1D EKF-SLAM, the figures to the left show the evolution of the agent's belief of his location and that of an object which are represented by the blue and red Gaussian functions. The figures to the right show the evolution of the parameters of the EKF-SLAM.\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces \textit  {Left:} Joint distribution of the agent and object, before any action or measurement is integrated. There are \textit  {Right:} Marginals of the agent and object, it tells use the probability of their location. The marginal of each random variable is obtained by marginalising the other out according to Equation \ref  {eq:agent_marginal}. \relax }}{117}{figure.caption.79}}
\newlabel{fig:histogram_joint}{{5.6}{117}{\textit {Left:} Joint distribution of the agent and object, before any action or measurement is integrated. There are \textit {Right:} Marginals of the agent and object, it tells use the probability of their location. The marginal of each random variable is obtained by marginalising the other out according to Equation \ref {eq:agent_marginal}. \relax }{figure.caption.79}{}}
\newlabel{eq:agent_marginal}{{5.3.4}{117}{Histogram SLAM}{equation.5.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces 1D world Likelihood $P(Y_t|A_t,O)$. \textit  {Left:} No contact detected with the object, the current measurement is $\mathaccentV {hat}05E{Y}_t = \leavevmode {\color  {red}0}$, both the agent and object cannot be in the same state. \textit  {Right:} The agent entered into contact with the object and got a haptic feedback $\mathaccentV {hat}05E{Y}_t = \leavevmode {\color  {red}1}$. There are only two measurements which the agent receives, contact or nothing.\relax }}{118}{figure.caption.80}}
\newlabel{fig:histogram_likelihood}{{5.7}{118}{1D world Likelihood $P(Y_t|A_t,O)$. \textit {Left:} No contact detected with the object, the current measurement is $\hat {Y}_t = \textcolor {red}0$, both the agent and object cannot be in the same state. \textit {Right:} The agent entered into contact with the object and got a haptic feedback $\hat {Y}_t = \textcolor {red}1$. There are only two measurements which the agent receives, contact or nothing.\relax }{figure.caption.80}{}}
\newlabel{eq:time-update}{{5.3.6}{118}{Histogram SLAM}{equation.5.3.6}{}}
\newlabel{eq:measurement-update}{{5.3.7}{118}{Histogram SLAM}{equation.5.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Histogram-SLAM, 4 time steps. \textbf  {1} Application of likelihood $P(Y_0=0|A_0,O)$ and the agent remains stationary, $u_0=0$, all states along the green line become zero. \textbf  {2} The agent moves to the right $u_1=1$, the motion $P(A_1|A_0,u_1)$, and likelihood models are applied consecutively. The right motion results in a shift (black arrow on the left) in the joint probability distribution towards the state $i=10$, all parameters on the pink line are zero. \textbf  {3} Same as two. \textbf  {4} The original result of the likelihood function, green line, has moved by the same amount as the agent's displacement. At each time step a new likelihood function (pink line) is applied to the joint distribution.\relax }}{119}{figure.caption.81}}
\newlabel{fig:discrete_example}{{5.8}{119}{Histogram-SLAM, 4 time steps. \textbf {1} Application of likelihood $P(Y_0=0|A_0,O)$ and the agent remains stationary, $u_0=0$, all states along the green line become zero. \textbf {2} The agent moves to the right $u_1=1$, the motion $P(A_1|A_0,u_1)$, and likelihood models are applied consecutively. The right motion results in a shift (black arrow on the left) in the joint probability distribution towards the state $i=10$, all parameters on the pink line are zero. \textbf {3} Same as two. \textbf {4} The original result of the likelihood function, green line, has moved by the same amount as the agent's displacement. At each time step a new likelihood function (pink line) is applied to the joint distribution.\relax }{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Histogram-SLAM contact. The agent has entered in contact with the object (measurement $\mathaccentV {hat}05E{Y}_t = 1$) and the likelihood function $P(Y_t=1|A_t,O)$ is applied to the joint distribution. Only parameters on the line $i=j$ will remain unchanged and parameters for which $i \not = j$ will be set to zero.\relax }}{120}{figure.caption.82}}
\newlabel{fig:discrete_example_contact}{{5.9}{120}{Histogram-SLAM contact. The agent has entered in contact with the object (measurement $\hat {Y}_t = 1$) and the likelihood function $P(Y_t=1|A_t,O)$ is applied to the joint distribution. Only parameters on the line $i=j$ will remain unchanged and parameters for which $i \not = j$ will be set to zero.\relax }{figure.caption.82}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Measurement Likelihood Memory Filter}{120}{section.5.4}}
\newlabel{ch5:MLMF}{{5.4}{120}{Measurement Likelihood Memory Filter}{section.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}New joint parametrisation \& intuition}{121}{subsection.5.4.1}}
\newlabel{eq:memory_function}{{5.4.1}{121}{New joint parametrisation \& intuition}{equation.5.4.1}{}}
\newlabel{eq:joint_filter_memory}{{5.4.2}{121}{New joint parametrisation \& intuition}{equation.5.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Time evolution of both the joint and marginal distributions of the agent (blue) and object (red) probability distributions for the case of a 1D search. The space is discretised to 100 bins and the agent is moving in the direction of the black arrow. The black line represents the measurement likelihood function. It is along this line $A=O$ that the joint distribution will be changed. On the \textit  {Left}, the measurement likelihood causes all the mass of joint distribution lying on the line $A=O$ to be removed. The reason for this is that the agent has not sensed the object (the red and blue circles on the marginals indicate the true position of both the agent and the object), and all the probability mass which lies in the area of influence of the measurement likelihood gets removed and redistributed to the other states in the joint distribution due to normalisation.\relax }}{122}{figure.caption.83}}
\newlabel{fig:margina_joint_example}{{5.10}{122}{Time evolution of both the joint and marginal distributions of the agent (blue) and object (red) probability distributions for the case of a 1D search. The space is discretised to 100 bins and the agent is moving in the direction of the black arrow. The black line represents the measurement likelihood function. It is along this line $A=O$ that the joint distribution will be changed. On the \textit {Left}, the measurement likelihood causes all the mass of joint distribution lying on the line $A=O$ to be removed. The reason for this is that the agent has not sensed the object (the red and blue circles on the marginals indicate the true position of both the agent and the object), and all the probability mass which lies in the area of influence of the measurement likelihood gets removed and redistributed to the other states in the joint distribution due to normalisation.\relax }{figure.caption.83}{}}
\newlabel{eq:time_update_memory}{{5.4.3}{122}{New joint parametrisation \& intuition}{equation.5.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Computation of evidence and marginals}{122}{subsection.5.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces  $P(Y_t|A_t,O)$ effect on the joint distribution for three different initialisations. The tube area depicts the regions where the measurement function influences the joint distribution. The tube's boundary is governed by the bandwidth of the radial basis function of the measurement likelihood function. The joint distribution can be decomposed into a dependent $A \cap O$ (the intersection of $A$ and $O$) and independent $A \ominus O$ term (the symmetric difference of $A$ and $O$, everything outside the area of intersection but still within the red and blue circles). In the three other figures any probability mass of the joint distribution which lies in the tube will be in the $\cap $, all probability mass outside the tube but still within the grey rectangle is located in $\ominus $ and the white space is in the domain but not parametrised and thus not considered. \textbf  {a)} The initial configuration, the agent and object pdfs are not overlapping and thus are completely independent. The joint distribution is not intersecting with the measurement function. \textbf  {b)} The marginals overlap one another resulting in the measurement likelihood function intersection with the joint distribution. The probability mass at the intersection gets removed and renormalised to other regions which is the result of applying Bayes integration. \textbf  {c)} The marginals of $A$ and $B$ are completely overlapping, however only a small fraction of the probability mass in the joint distribution is within the measurement function's tube.\relax }}{123}{figure.caption.84}}
\newlabel{fig:overlap_dependence_independence}{{5.11}{123}{$P(Y_t|A_t,O)$ effect on the joint distribution for three different initialisations. The tube area depicts the regions where the measurement function influences the joint distribution. The tube's boundary is governed by the bandwidth of the radial basis function of the measurement likelihood function. The joint distribution can be decomposed into a dependent $A \cap O$ (the intersection of $A$ and $O$) and independent $A \ominus O$ term (the symmetric difference of $A$ and $O$, everything outside the area of intersection but still within the red and blue circles). In the three other figures any probability mass of the joint distribution which lies in the tube will be in the $\cap $, all probability mass outside the tube but still within the grey rectangle is located in $\ominus $ and the white space is in the domain but not parametrised and thus not considered. \textbf {a)} The initial configuration, the agent and object pdfs are not overlapping and thus are completely independent. The joint distribution is not intersecting with the measurement function. \textbf {b)} The marginals overlap one another resulting in the measurement likelihood function intersection with the joint distribution. The probability mass at the intersection gets removed and renormalised to other regions which is the result of applying Bayes integration. \textbf {c)} The marginals of $A$ and $B$ are completely overlapping, however only a small fraction of the probability mass in the joint distribution is within the measurement function's tube.\relax }{figure.caption.84}{}}
\newlabel{eq:joint_independent_dependent}{{5.4.4}{123}{Computation of evidence and marginals}{equation.5.4.4}{}}
\newlabel{eq:I}{{5.4.5}{124}{Computation of evidence and marginals}{equation.5.4.5}{}}
\newlabel{eq:marignal_mrf}{{5.4.6}{124}{Computation of evidence and marginals}{equation.5.4.6}{}}
\newlabel{eq:marignal_mrf_2}{{5.4.7}{124}{Computation of evidence and marginals}{equation.5.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}MLMF-SLAM Algorithm}{124}{subsection.5.4.3}}
\citation{PF_tutorial_2002}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.1}{\ignorespaces MLMF-SLAM\relax }}{125}{algorithm.5.1}}
\newlabel{alg:mrf-slam}{{5.1}{125}{MLMF-SLAM\relax }{algorithm.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Space \& time complexity (MLMF)}{125}{section.5.5}}
\newlabel{ch5:space_time_complexity_MLMF}{{5.5}{125}{Space \& time complexity (MLMF)}{section.5.5}{}}
\@writefile{brf}{\backcite{PF_tutorial_2002}{{125}{1}{Hfootnote.9}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Space complexity}{126}{subsection.5.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Time complexity}{126}{subsection.5.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces \textit  {left:} Joint distribution $P(A,O^{(1)},O^{(2)})$ of the agent and two objects. Each measurement likelihood function, $P(Y|A,O^{(1)})$, $P(Y|A,O^{(2)})$ and $P(Y|O^{(1)},O^{(2)})$ corresponds to a hyperplane in the joint distribution. The measurement function $P(Y|O^{(1)},O^{(2)})$ stipulates that the objects cannot be in the same state. The marginal space is discretised to $N = 100$ states giving the total number of states in the joint distribution as $100^3$ $(M=3)$. \textit  {right:} Joint distribution of the agent and one object $P(A_t,O|Y_{0:t})$. Three measurement functions have been added to the memory term. At every time step when an action is taken all measurement functions are updated according to the action applied $u_t$. This means that the first function to be added to the memory will have had all actions applied to it. The number of the equations and their associated lines in the plot indicate the order in which they have been added to the memory. The three points are candidates at which we want to evaluate the joint distribution. First the offset $A=O+c$ is evaluated which corresponds to the dotted red lines. Then these are checked against all the offsets stored in memory. The cost of evaluating the joint distribution at the yellow point is $\BigO  {1}$ since we only have to check the first element in the memory. For the green and cyan points the cost is $\BigO  {\qopname  \relax o{log}(n)}$, where $n$ is the current size of the memory.\relax }}{127}{figure.caption.85}}
\newlabel{fig:3bel_lik_profile}{{5.12}{127}{\textit {left:} Joint distribution $P(A,O^{(1)},O^{(2)})$ of the agent and two objects. Each measurement likelihood function, $P(Y|A,O^{(1)})$, $P(Y|A,O^{(2)})$ and $P(Y|O^{(1)},O^{(2)})$ corresponds to a hyperplane in the joint distribution. The measurement function $P(Y|O^{(1)},O^{(2)})$ stipulates that the objects cannot be in the same state. The marginal space is discretised to $N = 100$ states giving the total number of states in the joint distribution as $100^3$ $(M=3)$. \textit {right:} Joint distribution of the agent and one object $P(A_t,O|Y_{0:t})$. Three measurement functions have been added to the memory term. At every time step when an action is taken all measurement functions are updated according to the action applied $u_t$. This means that the first function to be added to the memory will have had all actions applied to it. The number of the equations and their associated lines in the plot indicate the order in which they have been added to the memory. The three points are candidates at which we want to evaluate the joint distribution. First the offset $A=O+c$ is evaluated which corresponds to the dotted red lines. Then these are checked against all the offsets stored in memory. The cost of evaluating the joint distribution at the yellow point is $\BigO {1}$ since we only have to check the first element in the memory. For the green and cyan points the cost is $\BigO {\log (n)}$, where $n$ is the current size of the memory.\relax }{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces \textbf  {Scalable filter} Each agent-object joint distribution pair is modelled independently. For clarity we have left out the action random variable $u$ which is linked to every agent node. Two joint distributions $P(A^{(1)},O^{(1)}|Y^{(1)}_{0:t})$ and $P(A^{(2)},O^{(2)}|Y^{(2)}_{0:t})$ parametrise the graphical model. The dashed undirected lines represent a wanted dependency, if present $O^{(1)}$ and $O^{(2)}$ are to be dependent through $A$. In the standard setting there will be no exchange of information between the individual joints. However we demonstrate later on how we perform a one time transfer of information when one of the objects is sensed.\relax }}{128}{figure.caption.86}}
\newlabel{fig:scalable_mlmf_dae}{{5.13}{128}{\textbf {Scalable filter} Each agent-object joint distribution pair is modelled independently. For clarity we have left out the action random variable $u$ which is linked to every agent node. Two joint distributions $P(A^{(1)},O^{(1)}|Y^{(1)}_{0:t})$ and $P(A^{(2)},O^{(2)}|Y^{(2)}_{0:t})$ parametrise the graphical model. The dashed undirected lines represent a wanted dependency, if present $O^{(1)}$ and $O^{(2)}$ are to be dependent through $A$. In the standard setting there will be no exchange of information between the individual joints. However we demonstrate later on how we perform a one time transfer of information when one of the objects is sensed.\relax }{figure.caption.86}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Scalable extension to multiple objects}{128}{section.5.6}}
\newlabel{subsec:scalabe_extension}{{5.6}{128}{Scalable extension to multiple objects}{section.5.6}{}}
\newlabel{eq:pair_wise_joint}{{5.6.1}{128}{Scalable extension to multiple objects}{equation.5.6.1}{}}
\newlabel{eq:marg_indep}{{5.6.2}{129}{Scalable extension to multiple objects}{equation.5.6.2}{}}
\newlabel{eq:marg_indep_prod}{{5.6.4}{129}{Scalable extension to multiple objects}{equation.5.6.4}{}}
\newlabel{eq:marg_indep_sum}{{5.6.5}{129}{Scalable extension to multiple objects}{equation.5.6.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.2}{\ignorespaces Scalable-MLMF: Measurement Update\relax }}{129}{algorithm.5.2}}
\newlabel{alg:scalabe-mrf-slam}{{5.2}{129}{Scalable-MLMF: Measurement Update\relax }{algorithm.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces \textbf  {Transfer of information between joint distributions} \textit  {top left and right:} Joint distributions of $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$ and $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$ prior and post sensing. The red and blue lines correspond to the region in which the measurement functions $P(Y^{(1)}_{t}|A^{(1)}_t,O^{(1)})$ and $P(Y^{(2)}_{t}|A^{(2)}_t,O^{(2)})$ will change the joint distributions. The dotted black lines are for ease of comparing the joint distributions prior and post sensing. \textit  {bottom right:} After the agent has sensed $O^{(2)}$, all the probability mass which was not overlapping the blue line becomes an infeasible solution to the agent and object locations. \textit  {bottom left:} The constraint imposed by the measurement likelihood function of the second object (blue line) is transferred to the joint distribution of the first object according to Algorithm \ref  {alg:scalabe-mrf-slam}. The result is a change in the joint distribution $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$, which satisfies the constraints imposed by the agent's marginal from the joint distribution $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$.\relax }}{130}{figure.caption.87}}
\newlabel{fig:transfer_information}{{5.14}{130}{\textbf {Transfer of information between joint distributions} \textit {top left and right:} Joint distributions of $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$ and $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$ prior and post sensing. The red and blue lines correspond to the region in which the measurement functions $P(Y^{(1)}_{t}|A^{(1)}_t,O^{(1)})$ and $P(Y^{(2)}_{t}|A^{(2)}_t,O^{(2)})$ will change the joint distributions. The dotted black lines are for ease of comparing the joint distributions prior and post sensing. \textit {bottom right:} After the agent has sensed $O^{(2)}$, all the probability mass which was not overlapping the blue line becomes an infeasible solution to the agent and object locations. \textit {bottom left:} The constraint imposed by the measurement likelihood function of the second object (blue line) is transferred to the joint distribution of the first object according to Algorithm \ref {alg:scalabe-mrf-slam}. The result is a change in the joint distribution $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$, which satisfies the constraints imposed by the agent's marginal from the joint distribution $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$.\relax }{figure.caption.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces \textbf  {Independence \& Objects} \textit  {left:} resulting marginals after setting the agent marginals of each pair wise joint distribution equal $A^{(1)}_t = A^{(2)}_t$ according to Algorithm \ref  {alg:scalabe-mrf-slam}. The object marginal $P(O^{(2)}|Y_{0:t})$ is recomputed. \textit  {right:} resulting marginals in which the objects have no influence on one another. The difference between the two figures is that the object $O^{(1)}$ marginal changed in the case where we introduced the dependence \textit  {left plot}, and remained unchanged in the case where the objects are treated as being independent \textit  {right plot}.\relax }}{131}{figure.caption.88}}
\newlabel{fig:independence_object}{{5.15}{131}{\textbf {Independence \& Objects} \textit {left:} resulting marginals after setting the agent marginals of each pair wise joint distribution equal $A^{(1)}_t = A^{(2)}_t$ according to Algorithm \ref {alg:scalabe-mrf-slam}. The object marginal $P(O^{(2)}|Y_{0:t})$ is recomputed. \textit {right:} resulting marginals in which the objects have no influence on one another. The difference between the two figures is that the object $O^{(1)}$ marginal changed in the case where we introduced the dependence \textit {left plot}, and remained unchanged in the case where the objects are treated as being independent \textit {right plot}.\relax }{figure.caption.88}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces \textbf  {Time and space complexity summary} For both MLMF and scalabe-MLMF the worst case scenario is reported for the space complexity.\relax }}{131}{table.caption.89}}
\newlabel{tab:time_space_summary}{{5.1}{131}{\textbf {Time and space complexity summary} For both MLMF and scalabe-MLMF the worst case scenario is reported for the space complexity.\relax }{table.caption.89}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Evaluation}{131}{section.5.7}}
\newlabel{ch5:evaluation}{{5.7}{131}{Evaluation}{section.5.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Evaluation of time complexity}{131}{subsection.5.7.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces \textbf  {Time complexity:} \textit  {left:} mean time taken in Hz for a loop update (motion and measurement) as a function of the number of states in a marginal and the number of objects present. \textit  {right:} time taken for a loop update with respect to the number of states in the marginal. The colour coded lines are associated with the number of objects present. The computational cost is plotted on a log scale. As the number of states increases exponentially the computational cost matches it.\relax }}{132}{figure.caption.90}}
\newlabel{fig:time_complexity}{{5.16}{132}{\textbf {Time complexity:} \textit {left:} mean time taken in Hz for a loop update (motion and measurement) as a function of the number of states in a marginal and the number of objects present. \textit {right:} time taken for a loop update with respect to the number of states in the marginal. The colour coded lines are associated with the number of objects present. The computational cost is plotted on a log scale. As the number of states increases exponentially the computational cost matches it.\relax }{figure.caption.90}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Evaluation of independence assumption}{132}{subsection.5.7.2}}
\newlabel{subsec:eval_indep_assumptiom}{{5.7.2}{132}{Evaluation of independence assumption}{subsection.5.7.2}{}}
\newlabel{eq:hellinger}{{5.7.1}{133}{Evaluation of independence assumption}{equation.5.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}Evaluation of memory}{133}{subsection.5.7.3}}
\newlabel{eq:greedy_algorithm}{{5.7.2}{133}{Evaluation of memory}{equation.5.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces \textbf  {Comparison of scalable-MLMF and the histogram filter} A deterministic sweep policy was carried out for 100 different initialisations of the agent and object beliefs. \textit  {top left:} One particular Initialisation of the agent and object random variables. The true position of the agent and objects were sampled at random. The black arrow indicates the general policy which was followed for each of the 100 sweeps. These were performed for 1) scalable-MLMF with objects considered to be independent at all times (no Algorithm \ref  {alg:scalabe-mrf-slam}). 2) Agent marginal $P(A_t|Y_{0:t})$ is the product of marginals $P(A^{(i)}_t|Y^{(i)}_{0:t})$ (Equation \ref  {eq:marg_indep_prod}). 3) marginal $P(A_t|Y_t)$ is taken to be the average of all marginals $P(A^{(i)}_t|Y^{(i)}_{0:t})$ (Equation \ref  {eq:marg_indep_sum}). For each of these three experiment we report the kernel density estimation over the Hellinger distances taken at every time step between ground truth (from histogram filter) and scalable-MLMF.\relax }}{134}{figure.caption.91}}
\newlabel{fig:independence_assumption_test}{{5.17}{134}{\textbf {Comparison of scalable-MLMF and the histogram filter} A deterministic sweep policy was carried out for 100 different initialisations of the agent and object beliefs. \textit {top left:} One particular Initialisation of the agent and object random variables. The true position of the agent and objects were sampled at random. The black arrow indicates the general policy which was followed for each of the 100 sweeps. These were performed for 1) scalable-MLMF with objects considered to be independent at all times (no Algorithm \ref {alg:scalabe-mrf-slam}). 2) Agent marginal $P(A_t|Y_{0:t})$ is the product of marginals $P(A^{(i)}_t|Y^{(i)}_{0:t})$ (Equation \ref {eq:marg_indep_prod}). 3) marginal $P(A_t|Y_t)$ is taken to be the average of all marginals $P(A^{(i)}_t|Y^{(i)}_{0:t})$ (Equation \ref {eq:marg_indep_sum}). For each of these three experiment we report the kernel density estimation over the Hellinger distances taken at every time step between ground truth (from histogram filter) and scalable-MLMF.\relax }{figure.caption.91}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces \textbf  {Agent's prior beliefs.} Two types of environment, the first is a 2D world in which the agent lives in a square surrounded by a wall whilst the second is a 1D world. In the 2D figures the agent is illustrated by a circle with a bar to indicate its heading. The true location of the objects are represented by colour coded squares. \textit  {Top row} three different initialisations of the agent's location. \textit  {Bottom row} d) the agent's prior beliefs with respect to the location of the first object and e) belief of the second object's location. \textit  {bottom row} f) 1D world with one object.\relax }}{135}{figure.caption.92}}
\newlabel{fig:exploration_init}{{5.18}{135}{\textbf {Agent's prior beliefs.} Two types of environment, the first is a 2D world in which the agent lives in a square surrounded by a wall whilst the second is a 1D world. In the 2D figures the agent is illustrated by a circle with a bar to indicate its heading. The true location of the objects are represented by colour coded squares. \textit {Top row} three different initialisations of the agent's location. \textit {Bottom row} d) the agent's prior beliefs with respect to the location of the first object and e) belief of the second object's location. \textit {bottom row} f) 1D world with one object.\relax }{figure.caption.92}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces \textbf  {Memory size vs time to find objects} Results of the effect of the memory size on the decision process. The memory size is reported as the percentage of total number of states present in the marginal space. At 100\% the size of the memory is equal to that of the state space. \textit  {left:} results of the 1D search illustrated in Figure \ref  {fig:exploration_init} \textit  {f)}. \textit  {Middle \& right:} 2D search with initialisations accordingly depicted in Figure \ref  {fig:exploration_init}. \relax }}{136}{figure.caption.93}}
\newlabel{fig:time_to_reach_goal}{{5.19}{136}{\textbf {Memory size vs time to find objects} Results of the effect of the memory size on the decision process. The memory size is reported as the percentage of total number of states present in the marginal space. At 100\% the size of the memory is equal to that of the state space. \textit {left:} results of the 1D search illustrated in Figure \ref {fig:exploration_init} \textit {f)}. \textit {Middle \& right:} 2D search with initialisations accordingly depicted in Figure \ref {fig:exploration_init}. \relax }{figure.caption.93}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Conclusion}{136}{section.5.8}}
\newlabel{ch5:conclusion}{{5.8}{136}{Conclusion}{section.5.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Appendix}{137}{section.5.9}}
\newlabel{ch5:appendix}{{5.9}{137}{Appendix}{section.5.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.1}EKF-SLAM models}{138}{subsection.5.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.2}Bayesian filtering recursion}{138}{subsection.5.9.2}}
\newlabel{appendix:bayes_recursion}{{5.9.2}{138}{Bayesian filtering recursion}{subsection.5.9.2}{}}
\newlabel{eq:br_chain}{{5.9.6}{138}{Bayesian filtering recursion}{equation.5.9.6}{}}
\newlabel{eq:cancellations}{{5.9.7}{138}{Bayesian filtering recursion}{equation.5.9.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.3}Derivation of the evidence}{139}{subsection.5.9.3}}
\newlabel{appendix:evidence}{{5.9.3}{139}{Derivation of the evidence}{subsection.5.9.3}{}}
\newlabel{eq:marginal_likelihood_memory}{{5.9.11}{139}{Derivation of the evidence}{equation.5.9.11}{}}
\newlabel{eq:marginal_likelihood_memory_last}{{5.9.13}{139}{Derivation of the evidence}{equation.5.9.13}{}}
\newlabel{eq:marginal_likelihood_equal_I}{{5.9.17}{139}{Derivation of the evidence}{equation.5.9.17}{}}
\newlabel{eq:marginal_likelihood_equal_I2}{{5.9.21}{140}{Derivation of the evidence}{equation.5.9.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.4}Derivation of the marginal}{140}{subsection.5.9.4}}
\newlabel{appendix:marginal}{{5.9.4}{140}{Derivation of the marginal}{subsection.5.9.4}{}}
\newlabel{eq:dependence1}{{5.9.23}{140}{Derivation of the marginal}{equation.5.9.23}{}}
\newlabel{eq:dependence2}{{5.9.24}{140}{Derivation of the marginal}{equation.5.9.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.5}Space \& time complexity (scalable-MLMF)}{140}{subsection.5.9.5}}
\newlabel{appendix:space_time_scalable_mlmf}{{5.9.5}{140}{Space \& time complexity (scalable-MLMF)}{subsection.5.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.6}Space complexity}{140}{subsection.5.9.6}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces \textbf  {Memory list} After three updates the memory term $P(Y_{0:t}|A_t,O)$ contains three functions. The parametrisation of the functions is the same with the exception of actions $u_t$ which are added after each motion update step. As the result the first function added to the list will have all the actions applied to it from the first time step to the last.\relax }}{141}{table.caption.94}}
\newlabel{tab:memory_list}{{5.2}{141}{\textbf {Memory list} After three updates the memory term $P(Y_{0:t}|A_t,O)$ contains three functions. The parametrisation of the functions is the same with the exception of actions $u_t$ which are added after each motion update step. As the result the first function added to the list will have all the actions applied to it from the first time step to the last.\relax }{table.caption.94}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.7}Time complexity}{141}{subsection.5.9.7}}
\newlabel{appendix:memory_time_complexity}{{5.9.7}{141}{Time complexity}{subsection.5.9.7}{}}
\bibstyle{plainnat}
\bibdata{bib/MLMF.bib,bib/peg_hole.bib,bib/search_human_blind.bib,bib/RL.bib,bib/pomdp.bib,bib/cpomdp.bib,bib/citations.bib,bib/DT.bib,bib/ToM.bib,bib/spatial_navigation.bib,bib/ProspectTheory.bib,imitation_learning.bib,bib/ch3-citations.bib}
\bibcite{FIRM_2011}{{1}{2011}{{a.~Agha-mohammadi et~al.}}{{a.~Agha-mohammadi, Chakravorty, and Amato}}}
\bibcite{rob_online_bs_icra_2014}{{2}{2014}{{a.~Agha-mohammadi et~al.}}{{a.~Agha-mohammadi, Agarwal, Mahadevan, Chakravorty, Tomkins, Denny, and Amato}}}
\bibcite{sis_pomdp_2002}{{3}{2002}{{Aberdeen and Baxter}}{{}}}
\bibcite{sol_pdg_pbd_2014}{{4}{2014}{{Abu-Dakka et~al.}}{{Abu-Dakka, Nemec, Kramberger, Buch, Kr{\"u}ger, and Ude}}}
\bibcite{Arul_Mask_Clap_2002}{{5}{2002{a}}{{Arulampalam et~al.}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\bibcite{PF_tutorial_2002}{{6}{2002{b}}{{Arulampalam et~al.}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\bibcite{Atkeson97locallyweighted}{{7}{1997}{{Atkeson et~al.}}{{Atkeson, Moore, and Schaal}}}
\bibcite{Bake_Saxe_Tene_2011}{{8}{2011}{{Bake et~al.}}{{Bake, Tenenbaum, and Saxe}}}
\bibcite{Bake_Tene_Saxe_2006}{{9}{2006}{{Baker et~al.}}{{Baker, Tenenbaum, and Saxe}}}
\bibcite{barberBRML2012}{{10}{2012}{{Barber}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{143}{chapter*.95}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{Baron-Cohen}{{11}{1995}{{Baron-Cohen}}{{}}}
\bibcite{gpomdp_2000}{{12}{2000}{{Baxter and Bartlett}}{{}}}
\bibcite{Bergman99recursivebayesian}{{13}{1999}{{Bergman and Bergman}}{{}}}
\bibcite{Bernoulli1954}{{14}{1954}{{Bernoulli}}{{}}}
\bibcite{Billard_schol_2013}{{15}{2013}{{Billard and Grollman}}{{}}}
\bibcite{Billard08chapter}{{16}{2008{a}}{{Billard et~al.}}{{Billard, Calinon, Dillmann, and Schaal}}}
\bibcite{BillardCDS08}{{17}{2008{b}}{{Billard et~al.}}{{Billard, Calinon, Dillmann, and Schaal}}}
\bibcite{solving_continous_pomdps_2013}{{18}{2013}{{Brechtel et~al.}}{{Brechtel, Gindele, and Dillmann}}}
\bibcite{mc_update_ppomdps}{{19}{2011}{{Brooks and Williams}}{{}}}
\bibcite{spatial_memory_how_ego_allo_combine_2006}{{20}{2006}{{Burgess}}{{}}}
\bibcite{MRF_ToM}{{21}{2009}{{Butterfield et~al.}}{{Butterfield, Jenkins, Sobel, and Schwertfeger}}}
\bibcite{gesture_calinon_2010}{{22}{2010}{{Calinon et~al.}}{{Calinon, D'halluin, Sauser, Caldwell, and Billard}}}
\bibcite{KL_SLAM_exploration_PF}{{23}{2010}{{Carlone et~al.}}{{Carlone, Du, Ng, Bona, and Indri}}}
\bibcite{Active_SLAM_Uncertainty_compar}{{24}{2012}{{Carrillo et~al.}}{{Carrillo, Reid, and Castellanos}}}
\bibcite{ActingUncertainty_1996}{{25}{1996{a}}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{acting_uncer_1996}{{26}{1996{b}}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{Chambrier2014}{{27}{2014}{{Chambrier and Billard}}{{}}}
\bibcite{u_aware_grasp_ICRA_2015}{{28}{2015}{{Chen and von Wichert}}{{}}}
\bibcite{online_gpr_icra_2014}{{29}{2014}{{Cheng and Chen}}{{}}}
\bibcite{search_strategies_icra_2001}{{30}{2001}{{Chhatpar and Branicky}}{{}}}
\bibcite{deChambrier2013}{{31}{2013}{{de~Chambrier and Billard}}{{}}}
\bibcite{p_search_surv_2011}{{32}{2011}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{rl_gradient_survey_2013}{{33}{2013{a}}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{DeisenrothNP2013}{{34}{2013{b}}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{ToM_HRI_2106}{{35}{2016}{{Devin and Alami}}{{}}}
\bibcite{POMDP_approach_2010}{{36}{2010}{{Du et~al.}}{{Du, Hsu, Kurniawati, Lee, Ong, and Png}}}
\bibcite{SLAM_part1}{{37}{2006}{{Durrant-Whyte and Bailey}}{{}}}
\bibcite{Erez10ascalable}{{38}{2010}{{Erez and Smart}}{{}}}
\bibcite{EGW05}{{39}{2005{a}}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{Tree_batch_2005}{{40}{2005{b}}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{hybrid_1992}{{41}{1992}{{Fisher and Mujtaba}}{{}}}
\bibcite{Navigation_strategires_for_exploring_indoor_environments}{{42}{2002}{{Gonz{\'{a}}lez{-}Ba{\~{n}}os and Latombe}}{{}}}
\bibcite{TutGraphSLAM}{{43}{2010}{{Grisetti et~al.}}{{Grisetti, Kummerle, Stachniss, and Burgard}}}
\bibcite{ac_survey_2012}{{44}{2012}{{Grondman et~al.}}{{Grondman, Busoniu, Lopes, and Babuska}}}
\bibcite{learn_admittance_icra_1994}{{45}{1994}{{Gullapalli et~al.}}{{Gullapalli, Barto, and Grupen}}}
\bibcite{Hamner_2006_5810}{{46}{2006}{{Hamner et~al.}}{{Hamner, Singh, and Scherer}}}
\bibcite{Hauser_2011}{{47}{2011}{{Hauser}}{{}}}
\bibcite{DRQ_AAAI_2015}{{48}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{Quadrator_2008}{{49}{2008}{{He et~al.}}{{He, Prentice, and Roy}}}
\bibcite{next_best_touch}{{50}{2013}{{Hebert et~al.}}{{Hebert, Howard, Hudson, Ma, and Burdick}}}
\bibcite{negative_info_markov_localisation}{{51}{2005}{{Hoffman et~al.}}{{Hoffman, Spranger, Gohring, and Jungel}}}
\bibcite{NegInfoFurtherStudies}{{52}{2006}{{Hoffmann et~al.}}{{Hoffmann, Spranger, Gohring, Jungel, and Burkhard}}}
\bibcite{un_water_inspection_icra_2012}{{53}{2012}{{Hollinger et~al.}}{{Hollinger, Englot, Hover, Mitra, and Sukhatme}}}
\bibcite{Hsiao_RSS_10}{{54}{2010}{{Hsiao et~al.}}{{Hsiao, Kaelbling, and Lozano-Perez}}}
\bibcite{RRT-SLAM}{{55}{2008}{{Huang and Gupta}}{{}}}
\bibcite{DiffEntropyHuber2008}{{56}{2008}{{Huber et~al.}}{{Huber, Bailey, Durrant-Whyte, and Hanebeck}}}
\bibcite{Iachini2014}{{57}{2014}{{Iachini et~al.}}{{Iachini, Ruggiero, and Ruotolo}}}
\bibcite{Efficient_touch_2012}{{58}{2012}{{Javdani et~al.}}{{Javdani, Klingensmith, Bagnell, Pollard, and Srinivasa}}}
\bibcite{DARPA_2015}{{59}{2015}{{Johnson et~al.}}{{Johnson, Shrewsbury, Bertrand, Wu, Duran, Floyd, Abeles, Stephen, Mertins, Lesman, Carff, Rifenburgh, Kaveti, Straatman, Smith, Griffioen, Layton, de~Boer, Koolen, Neuhaus, and Pratt}}}
\bibcite{Kaelbling_1998}{{60}{1998}{{Kaelbling et~al.}}{{Kaelbling, Littman, and Cassandra}}}
\bibcite{learn_force_c_icirs_2011}{{61}{2011}{{Kalakrishnan et~al.}}{{Kalakrishnan, Righetti, Pastor, and Schaal}}}
\bibcite{Kasper2001153}{{62}{2001}{{Kasper et~al.}}{{Kasper, Fricke, Steuernagel, and von Puttkamer}}}
\bibcite{PRM_1996}{{63}{1996}{{Kavraki et~al.}}{{Kavraki, Svestka, Latombe, and Overmars}}}
\bibcite{heli_2004}{{64}{2004}{{Kim et~al.}}{{Kim, Jordan, Sastry, and Ng}}}
\bibcite{PoWER_2009}{{65}{2009}{{Kober and Peters}}{{}}}
\bibcite{RL_robots_surv_2013}{{66}{2013}{{Kober et~al.}}{{Kober, Bagnell, and Peters}}}
\bibcite{Kollar_2008_Exploration_SLAM}{{67}{2008}{{Kollar and Roy}}{{}}}
\bibcite{compliant_manip_icra_2008}{{68}{2008}{{kook Yun}}{{}}}
\bibcite{pancake_2010}{{69}{2010{a}}{{Kormushev et~al.}}{{Kormushev, Calinon, and Caldwell}}}
\bibcite{archery_2010}{{70}{2010{b}}{{Kormushev et~al.}}{{Kormushev, Calinon, Saegusa, and Metta}}}
\bibcite{SARSOP}{{71}{2008}{{Kurniawati et~al.}}{{Kurniawati, Hsu, and Lee}}}
\bibcite{human_stsm_2015}{{72}{}{{Lavenexa et~al.}}{{Lavenexa, Boujonb, Ndarugendamwob, and Lavenexa}}}
\bibcite{Leslie_TOMM}{{73}{1994}{{Leslie}}{{}}}
\bibcite{Li_2015}{{74}{2016}{{Li et~al.}}{{Li, Hang, Kragic, and Billard}}}
\bibcite{bs_compression_2010}{{75}{2010}{{Li et~al.}}{{Li, Cheung, and Liu}}}
\bibcite{GeorgiosLidoris}{{76}{2011}{{Lidoris}}{{}}}
\bibcite{qmdp_ijcnn_2014}{{77}{2014}{{Lin et~al.}}{{Lin, Lu, and Makedon}}}
\bibcite{Littman95}{{78}{1995}{{Littman et~al.}}{{Littman, Cassandra, and Kaelbling}}}
\bibcite{peg_imcssd_2015}{{79}{2015}{{M.~Bdiwi}}{{}}}
\bibcite{peg_personal_icra_2010}{{80}{2010}{{Meeussen et~al.}}{{Meeussen, Wise, Glaser, Chitta, McGann, Mihelich, Marder-Eppstein, Muja, Eruhimov, Foote, Hsu, Rusu, Marthi, Bradski, Konolige, Gerkey, and Berger}}}
\bibcite{cogprints730}{{81}{1956}{{Miller}}{{}}}
\bibcite{mnih-dqn-2015}{{82}{2015}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis}}}
\bibcite{DataAssociation2003}{{83}{2003}{{Montemerlo and Thrun}}{{}}}
\bibcite{FastSLAM}{{84}{2003}{{Montemerlo et~al.}}{{Montemerlo, Thrun, Koller, and Wegbreit}}}
\bibcite{trans_workpiece_icra_2013}{{85}{2013}{{Nemec et~al.}}{{Nemec, Abu-Dakka, Ridge, Ude, J\IeC {\o }rgensen, Savarimuthu, Jouffroy, Petersen, and Kr\IeC {\"u}ger}}}
\bibcite{NIPS2008_3501}{{86}{2009}{{Neumann and Peters}}{{}}}
\bibcite{Pegasus_2000}{{87}{2000}{{Ng and Jordan}}{{}}}
\bibcite{Nicolescu01learningand}{{88}{2001}{{Nicolescu and Mataric}}{{}}}
\bibcite{RL_book_sa}{{89}{2012}{{Now\'{e} et~al.}}{{Now\'{e}, Vrancx, and De~Hauwere}}}
\bibcite{where_look_2012}{{90}{2012}{{Nunez-Varela et~al.}}{{Nunez-Varela, Ravindran, and Wyatt}}}
\bibcite{intuitive_peg_isr_2013}{{91}{2013}{{Park et~al.}}{{Park, Bae, Park, Baeg, and Park}}}
\bibcite{Pasqualotto2013175}{{92}{2013}{{Pasqualotto et~al.}}{{Pasqualotto, Spiller, Jansari, and Proulx}}}
\bibcite{peter_nac_2008}{{93}{2008{a}}{{Peters and Schaal}}{{}}}
\bibcite{NAC_2008}{{94}{2008{b}}{{Peters and Schaal}}{{}}}
\bibcite{PBVI}{{95}{2003}{{Pineau et~al.}}{{Pineau, Gordon, and Thrun}}}
\bibcite{Plagemann07gaussianbeam}{{96}{2007}{{Plagemann et~al.}}{{Plagemann, Kersting, Pfaff, and Burgard}}}
\bibcite{non_gauss_bel_plan_2012}{{97}{2012}{{Platt et~al.}}{{Platt, Kaelbling, Lozano-Perez, and Tedrake}}}
\bibcite{bsp_rss_2010a}{{98}{2010}{{Platt et~al.}}{{Platt, Tedrake, Kaelbling, and Lozano-P\'{e}rez}}}
\bibcite{PBVI_C_2006}{{99}{2006}{{Porta et~al.}}{{Porta, Vlassis, Spaan, and Poupart}}}
\bibcite{BelRoadMap_2009}{{100}{2009}{{Prentice and Roy}}{{}}}
\bibcite{decision_un_2013}{{101}{2013}{{Preuschoff et~al.}}{{Preuschoff, Mohr, and Hsu}}}
\bibcite{rai2013learning}{{102}{2013}{{Rai et~al.}}{{Rai, De~Chambrier, and Billard}}}
\bibcite{Sondik_1973}{{103}{1973}{{Richard D.~Smallwood}}{{}}}
\bibcite{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}{{104}{2012}{{Richardson et~al.}}{{Richardson, Bake, Tenenbaum, and Saxe}}}
\bibcite{Riedmiller05neuralfitted}{{105}{2005{a}}{{Riedmiller}}{{}}}
\bibcite{neura_fqi_2005}{{106}{2005{b}}{{Riedmiller}}{{}}}
\bibcite{Ross08onlineplanning}{{107}{2008}{{Ross et~al.}}{{Ross, Pineau, Paquet, and Chaib-draa}}}
\bibcite{CostalNavigation1999}{{108}{1999}{{Roy et~al.}}{{Roy, Burgard, Fox, and Thrun}}}
\bibcite{belief_compression_2005}{{109}{2005}{{Roy}}{{}}}
\bibcite{EPCA_2003}{{110}{2003}{{Roy and Gordon}}{{}}}
\bibcite{Roy99coastalnavigation}{{111}{1999}{{Roy and Thrun}}{{}}}
\bibcite{ToM_humanoid}{{112}{2002}{{Scassellati}}{{}}}
\bibcite{Schaal04learningmovement}{{113}{2004}{{Schaal et~al.}}{{Schaal, Peters, Nakanishi, and Ijspeert}}}
\bibcite{Thrun_Burgard_Fox_2005}{{114}{2005}{{Sebastian~Thrun and Fox}}{{}}}
\bibcite{BayesBall}{{115}{1998}{{Shachter}}{{}}}
\bibcite{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}{{116}{2010}{{Silver et~al.}}{{Silver, Bagnell, and Stentz}}}
\bibcite{HSV}{{117}{2004}{{Smith and Simmons}}{{}}}
\bibcite{HSVI2}{{118}{2012}{{Smith and Simmons}}{{}}}
\bibcite{Towards_a_ToM_2010}{{119}{2010}{{Sodian and Kristen}}{{}}}
\bibcite{Spaan05icra}{{120}{2005}{{Spaan and Vlassis}}{{}}}
\bibcite{stachniss05robotics}{{121}{2005}{{Stachniss et~al.}}{{Stachniss, Grisetti, and Burgard}}}
\bibcite{stankiewicz2006lost}{{122}{2006}{{Stankiewicz et~al.}}{{Stankiewicz, Legge, Mansfield, and Schlicht}}}
\bibcite{dmp_iros_2011}{{123}{2011}{{Stulp et~al.}}{{Stulp, Theodorou, Kalakrishnan, Pastor, Righetti, and Schaal}}}
\bibcite{dmp_seq_2012}{{124}{2012}{{Stulp et~al.}}{{Stulp, Theodorou, and Schaal}}}
\bibcite{Needle_2014}{{125}{2014}{{Sun and Alterovitz}}{{}}}
\bibcite{gmr_2004}{{126}{2004}{{Sung}}{{}}}
\bibcite{sutton1998reinforcement}{{127}{1998}{{Sutton and Barto}}{{}}}
\bibcite{MC-POMDP}{{128}{2000}{{Thrun}}{{}}}
\bibcite{Thrun02particlefilters}{{129}{2002}{{Thrun}}{{}}}
\bibcite{Thrun_grid_based_1996}{{130}{1996}{{Thrun and B\"{u}}}{{}}}
\bibcite{SLAM_HBR}{{131}{2008}{{Thrun and Leonard}}{{}}}
\bibcite{Thrun_2005}{{132}{2005}{{Thrun et~al.}}{{Thrun, Burgard, and Fox}}}
\bibcite{tovar_planning}{{133}{2006}{{Tovar et~al.}}{{Tovar, Mu\IeC {\~n}oz-G\IeC {\'o}mez, Murrieta-Cid, Alencastre-Miranda, Monroy, and Hutchinson}}}
\bibcite{ActivePosSLAM}{{134}{2012}{{Valencia et~al.}}{{Valencia, Miro, Dissanayake, and Andrade-Cetto}}}
\bibcite{dense_entropy_icra_2014}{{135}{2014}{{Vallve and Andrade{-}Cetto}}{{}}}
\bibcite{LQG_MP_2011}{{136}{2011}{{Van Den~Berg et~al.}}{{Van Den~Berg, Abbeel, and Goldberg}}}
\bibcite{van_den_Berg_2012}{{137}{2012}{{van~den Berg et~al.}}{{van~den Berg, Patil, and Alterovitz}}}
\bibcite{FSVI}{{138}{2007}{{Veloso}}{{}}}
\bibcite{pomdp_iros_tous_2015}{{139}{2015}{{Vien and Toussaint}}{{}}}
\bibcite{eNAC_2003}{{140}{2003}{{Vijayakumar et~al.}}{{Vijayakumar, Shibata, and Schaal}}}
\bibcite{VonNeumann1944}{{141}{1990}{{Von~Neumann and Morgenstern}}{{}}}
\bibcite{Wang2016}{{142}{2016}{{Wang et~al.}}{{Wang, Uchibe, and Doya}}}
\bibcite{Wang_2007}{{143}{2007}{{Wang}}{{}}}
\bibcite{updating_egocentric_human_navigation_2000}{{144}{2000}{{Wang and Spelke}}{{}}}
\bibcite{reinforce_1992}{{145}{1992}{{Williams}}{{}}}
\bibcite{Biomechanics_2009}{{146}{2009}{{Winter}}{{}}}
\bibcite{what_det_our_nav_ability_2010}{{147}{2010}{{Wolbers and Hegarty}}{{}}}
\bibcite{spatial_updating_2008}{{148}{2008}{{Wolbers et~al.}}{{Wolbers, Hegarty, Buchel, and Loomis}}}
\bibcite{fast_peg_pbd_icmc_2014}{{149}{2014}{{Yang et~al.}}{{Yang, Lin, Song, Nemec, Ude, Buch, Kr\IeC {\"u}ger, and Savarimuthu}}}
\bibcite{seq_traj_replan_iros_2013}{{150}{2013}{{Zito et~al.}}{{Zito, Kopicki, Stolkin, Borst, Schmidt, Roa, and Wyatt}}}

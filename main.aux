\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{DARPA_2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}}
\citation{decision_un_2013}
\citation{ActingUncertainty_1996}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of the decision making under uncertainty in both robotics and everyday life situations. (a) European Space Agency (ESA), remote orbital peg in hole task. (b)-(c) ESA, simulated exploration of a cave on Mars in the dark. (d)-(e) MIT DAC team, Atlas robot doing valve task, \url  {http://drc.mit.edu/}. Other pictures include underwater exploration and industrial peg-in-hole assembly.\relax }}{2}{figure.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ch1-example}{{1.1}{2}{Examples of the decision making under uncertainty in both robotics and everyday life situations. (a) European Space Agency (ESA), remote orbital peg in hole task. (b)-(c) ESA, simulated exploration of a cave on Mars in the dark. (d)-(e) MIT DAC team, Atlas robot doing valve task, \url {http://drc.mit.edu/}. Other pictures include underwater exploration and industrial peg-in-hole assembly.\relax }{figure.caption.4}{}}
\citation{stankiewicz2006lost}
\citation{Billard08chapter}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contribution}{4}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Learning to reason with uncertainty as humans}{4}{subsection.1.2.1}}
\newlabel{sub:contr1}{{1.2.1}{4}{Learning to reason with uncertainty as humans}{subsection.1.2.1}{}}
\citation{rai2013learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Reinforcement learning in belief space}{5}{subsection.1.2.2}}
\newlabel{sub:contr2}{{1.2.2}{5}{Reinforcement learning in belief space}{subsection.1.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Non-parametric Bayesian state space filter}{6}{subsection.1.2.3}}
\newlabel{sub:contr3}{{1.2.3}{6}{Non-parametric Bayesian state space filter}{subsection.1.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Thesis outline}{6}{section.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Roadmap of the Thesis with key points. \relax }}{7}{figure.caption.5}}
\newlabel{fig:rmap_thesis}{{1.2}{7}{Roadmap of the Thesis with key points. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Chapter outline.\relax }}{9}{figure.caption.6}}
\newlabel{fig:ch2_outline}{{2.1}{9}{Chapter outline.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Decisions under uncertainty}{10}{section.2.1}}
\newlabel{sec:deci_un}{{2.1}{10}{Decisions under uncertainty}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Relation between beliefs, desires and actions and are all considered to be rational.\relax }}{11}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Decision theory}{11}{subsection.2.1.1}}
\newlabel{sec:ch2_DT}{{2.1.1}{11}{Decision theory}{subsection.2.1.1}{}}
\citation{Bernoulli1954}
\citation{VonNeumann1944}
\newlabel{eq:exp_utility}{{2.1.1}{12}{Decision theory}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sequential decision making}{13}{section.2.2}}
\newlabel{sec:sqp}{{2.2}{13}{Sequential decision making}{section.2.2}{}}
\newlabel{eq:joint_state_actions_util}{{2.2.1}{13}{Sequential decision making}{equation.2.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Definition of common variables used.\relax }}{14}{table.caption.8}}
\newlabel{tab:notation}{{2.1}{14}{Definition of common variables used.\relax }{table.caption.8}{}}
\newlabel{fig:mdp_off}{{2.3(a)}{15}{Subfigure 2 2.3(a)}{subfigure.2.3.1}{}}
\newlabel{sub@fig:mdp_off}{{(a)}{15}{Subfigure 2 2.3(a)\relax }{subfigure.2.3.1}{}}
\newlabel{fig:mdp_on}{{2.3(b)}{15}{Subfigure 2 2.3(b)}{subfigure.2.3.2}{}}
\newlabel{sub@fig:mdp_on}{{(b)}{15}{Subfigure 2 2.3(b)\relax }{subfigure.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf  {(a)} the decision nodes are not considered random variables whilst in \textbf  {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }}{15}{figure.caption.9}}
\newlabel{fig:mdp}{{2.3}{15}{Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf {(a)} the decision nodes are not considered random variables whilst in \textbf {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {off-policy}}}{15}{figure.caption.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {on-policy}}}{15}{figure.caption.9}}
\newlabel{eq:joint_state_actions}{{2.2.2}{16}{Sequential decision making}{equation.2.2.2}{}}
\newlabel{eq:temporal_expected_utility}{{2.2.4}{16}{Sequential decision making}{equation.2.2.4}{}}
\newlabel{eq:expansion}{{2.2.5}{16}{Sequential decision making}{equation.2.2.5}{}}
\newlabel{eq:bellman}{{2.2.6}{16}{Sequential decision making}{equation.2.2.6}{}}
\newlabel{eq:on_policy_bellman}{{2.2.7}{17}{Sequential decision making}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}POMDP}{17}{subsection.2.2.1}}
\newlabel{eq:sensor}{{2.2.8}{18}{POMDP}{equation.2.2.8}{}}
\newlabel{eq:likelihood}{{2.2.9}{18}{POMDP}{equation.2.2.9}{}}
\newlabel{fig:motion_update}{{2.4(a)}{19}{Subfigure 2 2.4(a)}{subfigure.2.4.1}{}}
\newlabel{sub@fig:motion_update}{{(a)}{19}{Subfigure 2 2.4(a)\relax }{subfigure.2.4.1}{}}
\newlabel{fig:measurement}{{2.4(b)}{19}{Subfigure 2 2.4(b)}{subfigure.2.4.2}{}}
\newlabel{sub@fig:measurement}{{(b)}{19}{Subfigure 2 2.4(b)\relax }{subfigure.2.4.2}{}}
\newlabel{fig:likelihood}{{2.4(c)}{19}{Subfigure 2 2.4(c)}{subfigure.2.4.3}{}}
\newlabel{sub@fig:likelihood}{{(c)}{19}{Subfigure 2 2.4(c)\relax }{subfigure.2.4.3}{}}
\newlabel{fig:measurement_update}{{2.4(d)}{19}{Subfigure 2 2.4(d)}{subfigure.2.4.4}{}}
\newlabel{sub@fig:measurement_update}{{(d)}{19}{Subfigure 2 2.4(d)\relax }{subfigure.2.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \textbf  {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf  {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf  {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf  {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }}{19}{figure.caption.10}}
\newlabel{fig:belief_update_example}{{2.4}{19}{\textbf {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{19}{figure.caption.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{19}{figure.caption.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{19}{figure.caption.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{19}{figure.caption.10}}
\newlabel{eq:motion_update}{{2.2.10}{20}{POMDP}{equation.2.2.10}{}}
\newlabel{eq:measurement_update}{{2.2.11}{20}{POMDP}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Bayesian state space filter.\relax }}{20}{figure.caption.11}}
\newlabel{fig:baysian_filter}{{2.5}{20}{Bayesian state space filter.\relax }{figure.caption.11}{}}
\newlabel{fig:sub_pomdp}{{2.6(a)}{21}{Subfigure 2 2.6(a)}{subfigure.2.6.1}{}}
\newlabel{sub@fig:sub_pomdp}{{(a)}{21}{Subfigure 2 2.6(a)\relax }{subfigure.2.6.1}{}}
\newlabel{fig:sub_bmdp}{{2.6(b)}{21}{Subfigure 2 2.6(b)}{subfigure.2.6.2}{}}
\newlabel{sub@fig:sub_bmdp}{{(b)}{21}{Subfigure 2 2.6(b)\relax }{subfigure.2.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \textbf  {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf  {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }}{21}{figure.caption.12}}
\newlabel{fig:pomdp}{{2.6}{21}{\textbf {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{21}{figure.caption.12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{21}{figure.caption.12}}
\newlabel{eq:belief_bellman}{{2.2.15}{21}{POMDP}{equation.2.2.15}{}}
\citation{Sondik_1973}
\citation{Thrun_2005}
\citation{Kaelbling_1998}
\newlabel{eq:belief_state_transformation}{{2.2.16}{22}{POMDP}{equation.2.2.16}{}}
\newlabel{eq:max_component}{{2.2.17}{22}{POMDP}{equation.2.2.17}{}}
\newlabel{eq:final_belief_bellman}{{2.2.18}{22}{POMDP}{equation.2.2.18}{}}
\citation{POMDP_approach_2010}
\citation{Thrun_2005}
\citation{PBVI}
\citation{HSV}
\citation{HSVI2}
\citation{FSVI}
\citation{SARSOP}
\citation{POMDP_approach_2010}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Literature review}{23}{section.2.3}}
\newlabel{sec:lit_rev}{{2.3}{23}{Literature review}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Value Iteration}{23}{subsection.2.3.1}}
\newlabel{lit:VI}{{2.3.1}{23}{Value Iteration}{subsection.2.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Mind-map of AI and robotic methods for acting under uncertainty.\relax }}{24}{figure.caption.13}}
\newlabel{fig:mindmap}{{2.7}{24}{Mind-map of AI and robotic methods for acting under uncertainty.\relax }{figure.caption.13}{}}
\citation{Spaan05icra}
\citation{PBVI_C_2006}
\citation{solving_continous_pomdps_2013}
\@writefile{toc}{\contentsline {subsubsection}{Point-base Value Iteration}{25}{section*.14}}
\citation{Ross08onlineplanning}
\@writefile{toc}{\contentsline {subsubsection}{Approximate Value Iteration}{26}{section*.15}}
\citation{MC-POMDP}
\citation{Tree_batch_2005}
\citation{mc_update_ppomdps}
\citation{neura_fqi_2005}
\citation{DRQ_AAAI_2015}
\citation{mnih-dqn-2015}
\citation{Roy99coastalnavigation}
\citation{belief_compression_2005}
\citation{EPCA_2003}
\citation{bs_compression_2010}
\@writefile{toc}{\contentsline {subsubsection}{Latent Value Iteration}{28}{section*.16}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: Value Iteration}{28}{section*.17}}
\citation{gpomdp_2000}
\citation{reinforce_1992}
\citation{gpomdp_2000}
\citation{sis_pomdp_2002}
\citation{Pegasus_2000}
\citation{heli_2004}
\citation{dmp_iros_2011}
\citation{dmp_seq_2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Policy search}{29}{subsection.2.3.2}}
\newlabel{lit:policy_search}{{2.3.2}{29}{Policy search}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient: policy search}{29}{section*.18}}
\citation{PoWER_2009}
\citation{archery_2010}
\citation{pancake_2010}
\citation{Wang2016}
\citation{p_search_surv_2011}
\citation{RL_robots_surv_2013}
\@writefile{toc}{\contentsline {subsubsection}{Expectation-Maximisation: policy search}{30}{section*.19}}
\citation{ac_survey_2012}
\citation{eNAC_2003}
\citation{NAC_2008}
\@writefile{toc}{\contentsline {subsubsection}{Actor-critic: policy search}{31}{section*.20}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: policy search}{31}{section*.21}}
\citation{BelRoadMap_2009}
\citation{Quadrator_2008}
\citation{FIRM_2011}
\citation{rob_online_bs_icra_2014}
\citation{bsp_rss_2010a}
\citation{LQG_MP_2011}
\citation{Erez10ascalable}
\citation{van_den_Berg_2012}
\citation{Needle_2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Planning}{32}{subsection.2.3.3}}
\newlabel{lit:Planning}{{2.3.3}{32}{Planning}{subsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Belief space road maps}{32}{section*.22}}
\@writefile{toc}{\contentsline {subsubsection}{Optimal control}{32}{section*.23}}
\citation{non_gauss_bel_plan_2012}
\citation{seq_traj_replan_iros_2013}
\@writefile{toc}{\contentsline {subsubsection}{Summary: planning}{33}{section*.24}}
\citation{un_water_inspection_icra_2012}
\citation{u_aware_grasp_ICRA_2015}
\citation{Li_2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Heuristics}{34}{subsection.2.3.4}}
\newlabel{lit:heuristics}{{2.3.4}{34}{Heuristics}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Myopic \& Q-MDP}{34}{section*.25}}
\citation{Littman95}
\citation{RL_book_sa}
\citation{Thrun_2005}
\citation{acting_uncer_1996}
\citation{qmdp_ijcnn_2014}
\citation{where_look_2012}
\citation{Hauser_2011}
\citation{pomdp_iros_tous_2015}
\citation{CostalNavigation1999}
\citation{stachniss05robotics}
\citation{dense_entropy_icra_2014}
\@writefile{toc}{\contentsline {subsubsection}{Information gain}{35}{section*.26}}
\citation{Hsiao_RSS_10}
\citation{Efficient_touch_2012}
\citation{next_best_touch}
\@writefile{toc}{\contentsline {subsubsection}{Summary: heuristic}{36}{section*.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Summary: literature}{37}{subsection.2.3.5}}
\citation{Billard08chapter}
\citation{Billard_schol_2013}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Approach}{38}{section.2.4}}
\newlabel{sec:approach}{{2.4}{38}{Approach}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }}{39}{figure.caption.28}}
\newlabel{fig:mind_summary}{{2.8}{39}{Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }}{40}{figure.caption.29}}
\newlabel{fig:belief-pipeline}{{2.9}{40}{Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \textbf  {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf  {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf  {Learn policy:} A generative policy, $\policy  (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }}{41}{figure.caption.30}}
\newlabel{fig:human_search}{{2.10}{41}{\textbf {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf {Learn policy:} A generative policy, $\policy (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }{figure.caption.30}{}}
\citation{Biomechanics_2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }}{42}{figure.caption.31}}
\newlabel{fig:control_architecture}{{2.11}{42}{Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning to reason with uncertainty as humans}{43}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf  {Blindfolded search task} \textit  {Left:} Search task, a human demonstrator searching for the green wooden block on the table given that both his hearing and vision senses have been impeded. He starts (hand) at the white spot near position (1). The the red and blue trajectories are examples of possible searches. \textit  {Middle:} Inferred belief the human might have with respect to his position. If the human always starts at (1) and his belief is known, all following beliefs (2) can be inferred from Bayes rule. \textit  {Right:} WAM Robot 7 DOF reproduces the search strategies demonstrated by humans to find the object.\relax }}{44}{figure.caption.32}}
\newlabel{fig:searching}{{3.1}{44}{\textbf {Blindfolded search task} \textit {Left:} Search task, a human demonstrator searching for the green wooden block on the table given that both his hearing and vision senses have been impeded. He starts (hand) at the white spot near position (1). The the red and blue trajectories are examples of possible searches. \textit {Middle:} Inferred belief the human might have with respect to his position. If the human always starts at (1) and his belief is known, all following beliefs (2) can be inferred from Bayes rule. \textit {Right:} WAM Robot 7 DOF reproduces the search strategies demonstrated by humans to find the object.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Outline}{45}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Background}{45}{section.3.2}}
\newlabel{ch3:background}{{3.2}{45}{Background}{section.3.2}{}}
\citation{Wang_2007}
\citation{what_det_our_nav_ability_2010}
\citation{spatial_updating_2008}
\citation{spatial_memory_how_ego_allo_combine_2006}
\citation{updating_egocentric_human_navigation_2000}
\citation{Pasqualotto2013175}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Spatial navigation}{46}{subsection.3.2.1}}
\citation{spatial_memory_how_ego_allo_combine_2006}
\citation{cogprints730}
\citation{human_stsm_2015}
\citation{Iachini2014}
\citation{stankiewicz2006lost}
\@writefile{toc}{\contentsline {subsubsection}{Spatial cognition and memory}{47}{section*.33}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: spatial cognition}{47}{section*.34}}
\citation{stankiewicz2006lost}
\citation{Towards_a_ToM_2010}
\citation{ToM_humanoid}
\citation{Leslie_TOMM}
\citation{Baron-Cohen}
\citation{MRF_ToM}
\citation{ToM_HRI_2106}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Human beliefs}{48}{subsection.3.2.2}}
\citation{Bake_Saxe_Tene_2011}
\citation{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}
\citation{Bake_Tene_Saxe_2006}
\citation{Bake_Saxe_Tene_2011}
\citation{Kasper2001153}
\citation{Hamner_2006_5810}
\citation{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Programming by demonstration \& uncertainty}{49}{subsection.3.2.3}}
\citation{online_pre_sensor_2011}
\citation{Klas_icra_2012}
\citation{MedinaSH13}
\citation{GeorgiosLidoris}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Experiment: table search}{50}{section.3.3}}
\newlabel{ch3:experiment}{{3.3}{50}{Experiment: table search}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Table search task. Blindfolded human subjects after a disorientation step are placed in one of the four starting locations. The heading of the subject is always kept the same. The human's objective is to locate the green block on the table. Throughout all experiments the green wooden block is kept in the same location.\relax }}{51}{figure.caption.35}}
\newlabel{fig:tab_search_task}{{3.2}{51}{Table search task. Blindfolded human subjects after a disorientation step are placed in one of the four starting locations. The heading of the subject is always kept the same. The human's objective is to locate the green block on the table. Throughout all experiments the green wooden block is kept in the same location.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \textit  {Top left}: A participant is trying to locate the green wooden block on the table given that both vision and hearing senses have been inhibited. The location of his hand is being tracked by the OptiTrack\textsuperscript  {\textregistered } system. \textit  {Top right:} Initial distribution of the uncertainty or belief we assume the human has with respect to his position. \textit  {Bottom left:} Set of recorded searches, the trajectories are with respect to the hand. \textit  {Bottom right:} Trajectories starting from same area but have different search patterns, the red trajectories all navigate to the goal via the top right corner as opposed to the blue which go by the bottom left and right corner. Among these two groups there are trajectories which seem to minimize the distance taken to reach the goal as opposed to some which seek to stay close to the edge and corners.\relax }}{52}{figure.caption.36}}
\newlabel{fig:experiment}{{3.3}{52}{\textit {Top left}: A participant is trying to locate the green wooden block on the table given that both vision and hearing senses have been inhibited. The location of his hand is being tracked by the OptiTrack\textsuperscript {\textregistered } system. \textit {Top right:} Initial distribution of the uncertainty or belief we assume the human has with respect to his position. \textit {Bottom left:} Set of recorded searches, the trajectories are with respect to the hand. \textit {Bottom right:} Trajectories starting from same area but have different search patterns, the red trajectories all navigate to the goal via the top right corner as opposed to the blue which go by the bottom left and right corner. Among these two groups there are trajectories which seem to minimize the distance taken to reach the goal as opposed to some which seek to stay close to the edge and corners.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Formulation}{53}{section.3.4}}
\newlabel{ch3:formulation}{{3.4}{53}{Formulation}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Belief model}{53}{section*.37}}
\citation{Arul_Mask_Clap_2002}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {subsubsection}{Sensing model}{54}{section*.38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Four different time frames of the evolution of the belief particle filter. \textit  {Top left}: Initial belief distribution; a lot of uncertainty. \textit  {Top right:} First contact is made with the table, the measurement likelihood restrains the samples to be on the table's surface. \textit  {Bottom right:} First contact is an edge. \textit  {Bottom left:} Gradual localisation.\relax }}{55}{figure.caption.39}}
\newlabel{fig:pf_example}{{3.4}{55}{Four different time frames of the evolution of the belief particle filter. \textit {Top left}: Initial belief distribution; a lot of uncertainty. \textit {Top right:} First contact is made with the table, the measurement likelihood restrains the samples to be on the table's surface. \textit {Bottom right:} First contact is an edge. \textit {Bottom left:} Gradual localisation.\relax }{figure.caption.39}{}}
\newlabel{eq:sensingfunction}{{3.4.3}{55}{Sensing model}{equation.3.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motion model}{55}{section*.40}}
\citation{DiffEntropyHuber2008}
\@writefile{toc}{\contentsline {subsubsection}{Uncertainty}{56}{section*.41}}
\newlabel{eq:gmm1}{{3.4.4}{56}{Uncertainty}{equation.3.4.4}{}}
\citation{BillardCDS08}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Representation of the estimated density function. \textit  {Top Left and Right:} Initial starting point, all Gaussian functions are uniformly distributed with uniform priors. The red cluster always has the highest likelihood which is taken to be the believed location of the robot's/human's end-effector. \textit  {Bottom Left:} Contact with the table has been established, the robot location differers from his belief. \textit  {Bottom Right:} Contact has been made with a corner, the clusters reflect that the robot could be at any corner (note that weights are not depicted, only cluster assignment).\relax }}{57}{figure.caption.42}}
\newlabel{fig:clustering}{{3.5}{57}{Representation of the estimated density function. \textit {Top Left and Right:} Initial starting point, all Gaussian functions are uniformly distributed with uniform priors. The red cluster always has the highest likelihood which is taken to be the believed location of the robot's/human's end-effector. \textit {Bottom Left:} Contact with the table has been established, the robot location differers from his belief. \textit {Bottom Right:} Contact has been made with a corner, the clusters reflect that the robot could be at any corner (note that weights are not depicted, only cluster assignment).\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Policies}{57}{section.3.5}}
\newlabel{chap3:policies}{{3.5}{57}{Policies}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Modelling human search strategies}{57}{subsection.3.5.1}}
\newlabel{chap3:GMM_policy}{{3.5.1}{57}{Modelling human search strategies}{subsection.3.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \textit  {Left: } Resulting search GMM, a total of 67 Gaussian mixture components are present. We note the many overlapping Gaussians: this results from the level of uncertainty over the different choices taken. For example humans follow along the edge of the table in different directions and might leave the edge once they are confident with respect to their location. \textit  {Right:} Information Gain map of the table environment, dark regions indicate high information gain as oppose to lighter ones. Not surprisingly, the highest are the corners, followed by the edges.\relax }}{58}{figure.caption.43}}
\newlabel{fig:gmm}{{3.6}{58}{\textit {Left: } Resulting search GMM, a total of 67 Gaussian mixture components are present. We note the many overlapping Gaussians: this results from the level of uncertainty over the different choices taken. For example humans follow along the edge of the table in different directions and might leave the edge once they are confident with respect to their location. \textit {Right:} Information Gain map of the table environment, dark regions indicate high information gain as oppose to lighter ones. Not surprisingly, the highest are the corners, followed by the edges.\relax }{figure.caption.43}{}}
\newlabel{eq:gmm2}{{3.5.1}{58}{Modelling human search strategies}{equation.3.5.1}{}}
\citation{CostalNavigation1999}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Coastal Navigation}{59}{subsection.3.5.2}}
\newlabel{chap3:costal_policy}{{3.5.2}{59}{Coastal Navigation}{subsection.3.5.2}{}}
\newlabel{eq:objective_function}{{3.5.2}{59}{Coastal Navigation}{equation.3.5.2}{}}
\newlabel{eq:IG}{{3.5.3}{59}{Coastal Navigation}{equation.3.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Control}{59}{subsection.3.5.3}}
\newlabel{eq:conditional}{{3.5.4}{60}{Control}{equation.3.5.4}{}}
\newlabel{eq:GMR}{{3.5.5}{60}{Control}{equation.3.5.5}{}}
\newlabel{eq:weight}{{3.5.6}{60}{Control}{equation.3.5.6}{}}
\newlabel{eq:w_expectation}{{3.5.7}{60}{Control}{equation.3.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Overview of the decision loop. At the top a strategy is chosen given an initial belief $p(x_{0}|y_{0})$ of the location of the end-effector (initially through sampling the conditional). A speed is applied to the given direction based on the believed distance to the goal. This velocity is passed onwards to a low level impedance controller which sends out the required torques. The resulting sensation, encoded through the Multinomial distribution over the environment features, and actual displacement are sent back to update the belief.\relax }}{61}{figure.caption.44}}
\newlabel{fig:flow_chart}{{3.7}{61}{Overview of the decision loop. At the top a strategy is chosen given an initial belief $p(x_{0}|y_{0})$ of the location of the end-effector (initially through sampling the conditional). A speed is applied to the given direction based on the believed distance to the goal. This velocity is passed onwards to a low level impedance controller which sends out the required torques. The resulting sensation, encoded through the Multinomial distribution over the environment features, and actual displacement are sent back to update the belief.\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Results and discussion}{61}{section.3.6}}
\newlabel{chap3:results}{{3.6}{61}{Results and discussion}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Search \& behaviour analysis}{62}{subsection.3.6.1}}
\newlabel{sub:search_behaviour}{{3.6.1}{62}{Search \& behaviour analysis}{subsection.3.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Expected sensation. Plots of the expected sensation of the edge and corner feature for all trajectories. The axes are associated with the sensor measurements, 0 means that the corresponding feature is not sensed and 1 the feature is fully sensed. A point in the plots summarises a whole trajectory by the mean and variance of the probability of sensing a corner or edge. The radius of the circles are proportional to the variance. The doted blue rectangle represents the decision boundary for classifying a trajectory as being either risk-prone or risk-averse. A point which lies inside the rectangle is risk-prone. \textit  {Left:} Human trajectories demonstrate a wide variety of behaviours ranging from those remaining close to features to those preferring more risk. \textit  {Right:} Red points show Greedy and blue points the GMM model. \textit  {Bottom:} Green circles are associated with the Hybrid method whilst orange are those of the Coastal navigation method. The Hybrid method is a skewed version of the GMM which tends towards risky behaviour and exhibits the same kind of behaviour as the Coastal algorithm.\relax }}{63}{figure.caption.45}}
\newlabel{fig:expectedfeatures}{{3.8}{63}{Expected sensation. Plots of the expected sensation of the edge and corner feature for all trajectories. The axes are associated with the sensor measurements, 0 means that the corresponding feature is not sensed and 1 the feature is fully sensed. A point in the plots summarises a whole trajectory by the mean and variance of the probability of sensing a corner or edge. The radius of the circles are proportional to the variance. The doted blue rectangle represents the decision boundary for classifying a trajectory as being either risk-prone or risk-averse. A point which lies inside the rectangle is risk-prone. \textit {Left:} Human trajectories demonstrate a wide variety of behaviours ranging from those remaining close to features to those preferring more risk. \textit {Right:} Red points show Greedy and blue points the GMM model. \textit {Bottom:} Green circles are associated with the Hybrid method whilst orange are those of the Coastal navigation method. The Hybrid method is a skewed version of the GMM which tends towards risky behaviour and exhibits the same kind of behaviour as the Coastal algorithm.\relax }{figure.caption.45}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Percentage of risk-prone trajectories based on two decision criteria, the feature (f) and the risk (r) (information gain) metrics discussed above.\relax }}{64}{table.caption.47}}
\newlabel{tab:percentage-risk-prone}{{3.1}{64}{Percentage of risk-prone trajectories based on two decision criteria, the feature (f) and the risk (r) (information gain) metrics discussed above.\relax }{table.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Risk of searches. Illustration of risk-prone and risk-averse searches in terms of a Risk factor (\textit  {left}) and expected sensation (\textit  {right}). \textit  {Left:} Each trajectory was reduced to a single scalar, which we call the Risk factor, quantifying the risk of a trajectory. The Risk factor is inversely proportional to the sum of the information gain of a particular trajectory. The colour paired dots (risk averse) and squares (risk prone) represent trajectories which are plotted in Figure \ref  {fig:risk_examples}, to illustrate that these correspond to risk averse and prone searches. \textit  {Right:} Corresponding trajectories chosen in the Risk factor space but represented in the feature space. As expected, trajectories with a high risk map to regions of low expected feature. However the transition from the Risk space to feature space is non-linear and will result in a different risk-level classification than the feature metric previously discussed.\relax }}{65}{figure.caption.46}}
\newlabel{fig:riskexamples}{{3.9}{65}{Risk of searches. Illustration of risk-prone and risk-averse searches in terms of a Risk factor (\textit {left}) and expected sensation (\textit {right}). \textit {Left:} Each trajectory was reduced to a single scalar, which we call the Risk factor, quantifying the risk of a trajectory. The Risk factor is inversely proportional to the sum of the information gain of a particular trajectory. The colour paired dots (risk averse) and squares (risk prone) represent trajectories which are plotted in Figure \ref {fig:risk_examples}, to illustrate that these correspond to risk averse and prone searches. \textit {Right:} Corresponding trajectories chosen in the Risk factor space but represented in the feature space. As expected, trajectories with a high risk map to regions of low expected feature. However the transition from the Risk space to feature space is non-linear and will result in a different risk-level classification than the feature metric previously discussed.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Risk prone \& averse searches (red \& green trajectories). \textit  {Top left:} Two human trajectories taken from data shown in Figure \ref  {fig:riskexamples}. \textit  {Top right:} Two Greedy trajectories. \textit  {Bottom left:} GMM trajectories, all starting from the same location, the colour coding is to illustrate the different policies which were encoded and emerge given the same initial conditions. \textit  {Bottom right:} Corresponding expected features of each trajectory, the colour coding matches the trajectories to the ``GMM risk types'' sub-figure. All the searches which were generated by the GMM for this initialisation produced risk-averse searches (based on the feature metric discussed previous).\relax }}{66}{figure.caption.48}}
\newlabel{fig:risk_examples}{{3.10}{66}{Risk prone \& averse searches (red \& green trajectories). \textit {Top left:} Two human trajectories taken from data shown in Figure \ref {fig:riskexamples}. \textit {Top right:} Two Greedy trajectories. \textit {Bottom left:} GMM trajectories, all starting from the same location, the colour coding is to illustrate the different policies which were encoded and emerge given the same initial conditions. \textit {Bottom right:} Corresponding expected features of each trajectory, the colour coding matches the trajectories to the ``GMM risk types'' sub-figure. All the searches which were generated by the GMM for this initialisation produced risk-averse searches (based on the feature metric discussed previous).\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}GMM \& Coastal Navigation policy analysis}{67}{subsection.3.6.2}}
\newlabel{sub:policy_analysis}{{3.6.2}{67}{GMM \& Coastal Navigation policy analysis}{subsection.3.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Illustration of three different types of modes present during the execution of the task where the robot is being controlled by the learned GMM model. The white ball represents the actual position of the robot's end-effector. The blue ball represents the believed position of the robot's end-effector and the robot is acting according to it. The blue ball arrows represent modes. Colours encode the mode's weights given by the priors $\pi _{k}$ after conditioning ( but not re-weighted as previously described). The spectrum ranges from red (high weight) to blue (low weight). \textit  {Top left:} Three modes are present, but two agree with each other. \textit  {Top right:} Three modes are again present indicating appropriate ways to reduce the uncertainty. \textit  {Lower left:} Two modes are in opposing directions. No flipping behaviour between modes occurs since preference is given to the modes pointing in the same direction as the robot's current trajectory. \textit  {Lower right:} GMM modes when conditioned on the state represented in the lower left figure. The two modes represent the possible directions (un-normalised).\relax }}{68}{figure.caption.49}}
\newlabel{fig:modes}{{3.11}{68}{Illustration of three different types of modes present during the execution of the task where the robot is being controlled by the learned GMM model. The white ball represents the actual position of the robot's end-effector. The blue ball represents the believed position of the robot's end-effector and the robot is acting according to it. The blue ball arrows represent modes. Colours encode the mode's weights given by the priors $\pi _{k}$ after conditioning ( but not re-weighted as previously described). The spectrum ranges from red (high weight) to blue (low weight). \textit {Top left:} Three modes are present, but two agree with each other. \textit {Top right:} Three modes are again present indicating appropriate ways to reduce the uncertainty. \textit {Lower left:} Two modes are in opposing directions. No flipping behaviour between modes occurs since preference is given to the modes pointing in the same direction as the robot's current trajectory. \textit {Lower right:} GMM modes when conditioned on the state represented in the lower left figure. The two modes represent the possible directions (un-normalised).\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Illustration of the vector field for the Coastal and GMM policy. \textit  {Top Left} Coastal policy, there is only one possible direction for every state at any time, the values of $\lambda _2$ in the cost function were set experimentally. \textit  {Others:} The GMM policy for three different levels of uncertainty. For each point multiple actions are possible which is reflected by the number of arrows (only the first three most likely actions). As the uncertainty decreases the policy becomes less multi-model, but remains around the edges and corners. Note that once certain of being close to an edge there is a possibility to go either straight to the goal or stay close to the edge and corners.\relax }}{69}{figure.caption.50}}
\newlabel{fig:vectorfield}{{3.12}{69}{Illustration of the vector field for the Coastal and GMM policy. \textit {Top Left} Coastal policy, there is only one possible direction for every state at any time, the values of $\lambda _2$ in the cost function were set experimentally. \textit {Others:} The GMM policy for three different levels of uncertainty. For each point multiple actions are possible which is reflected by the number of arrows (only the first three most likely actions). As the uncertainty decreases the policy becomes less multi-model, but remains around the edges and corners. Note that once certain of being close to an edge there is a possibility to go either straight to the goal or stay close to the edge and corners.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Distance efficiency \& Uncertainty}{70}{subsection.3.6.3}}
\newlabel{sub:time_uncertainty}{{3.6.3}{70}{Distance efficiency \& Uncertainty}{subsection.3.6.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Mean distance and (variance) taken to reach the goal for 3 methods in 5 experiments. The grey shaded entries correspond to the results of the search algorithm which obtained the fastest time to reach the goal in each type of experiment/search.\relax }}{70}{table.caption.52}}
\newlabel{tab:mean-var-distance}{{3.2}{70}{Mean distance and (variance) taken to reach the goal for 3 methods in 5 experiments. The grey shaded entries correspond to the results of the search algorithm which obtained the fastest time to reach the goal in each type of experiment/search.\relax }{table.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Four search initializations, from \textit  {top left} to \textit  {bottom right} we refer to them as \#1-4. The circle indicates the true starting point of the end-effector (eof), whilst the triangle is the initial believed location of the eof. The initialisation in \#1 was chosen such that the true and believed eof locations were at opposite sides of the table. This setting was selected to highlight the draw back in methods which do not take into account uncertainty. The second initialisation \#2, reflects the situation where once again there is a large distance between true and believed location of the eof. However this time both are above the table. The starting points in \#3 are a variant on \#1 with the difference being that the believed eof position is above the table whilst the true eof location is not. The last experiment \#4 was a setup which would be favourable to algorithms that are inclined to be greedy. Both true and believed eof locations are close to one another.\relax }}{71}{figure.caption.51}}
\newlabel{fig:four-initialisations}{{3.13}{71}{Four search initializations, from \textit {top left} to \textit {bottom right} we refer to them as \#1-4. The circle indicates the true starting point of the end-effector (eof), whilst the triangle is the initial believed location of the eof. The initialisation in \#1 was chosen such that the true and believed eof locations were at opposite sides of the table. This setting was selected to highlight the draw back in methods which do not take into account uncertainty. The second initialisation \#2, reflects the situation where once again there is a large distance between true and believed location of the eof. However this time both are above the table. The starting points in \#3 are a variant on \#1 with the difference being that the believed eof position is above the table whilst the true eof location is not. The last experiment \#4 was a setup which would be favourable to algorithms that are inclined to be greedy. Both true and believed eof locations are close to one another.\relax }{figure.caption.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces ANOVA tests the null hypothesis that all search experiments produced the same type of search with respect to the distance taken to reach the goal. All the p-values are extremely small which indicate that the null hypothesis can safely be rejected.\relax }}{72}{table.caption.53}}
\newlabel{tab:anova-1}{{3.3}{72}{ANOVA tests the null hypothesis that all search experiments produced the same type of search with respect to the distance taken to reach the goal. All the p-values are extremely small which indicate that the null hypothesis can safely be rejected.\relax }{table.caption.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces ANOVA between paired search methods. The first column gives an indication of the probability that both the Greedy and GMM searches are statistically the same (the null hypothesis). This was rejected with a tolerance of below \%1. In the second column, Greedy vs Coastal searches \#1 and \#4 are statistically closer than the rest with a p-value threshold of 10\% required to be able to reject the null hypothesis. In the third column the uniform and \#3 are not statistically different and would require a higher threshold on the p-value to be so.\relax }}{72}{table.caption.54}}
\newlabel{fig:anova-2}{{3.4}{72}{ANOVA between paired search methods. The first column gives an indication of the probability that both the Greedy and GMM searches are statistically the same (the null hypothesis). This was rejected with a tolerance of below \%1. In the second column, Greedy vs Coastal searches \#1 and \#4 are statistically closer than the rest with a p-value threshold of 10\% required to be able to reject the null hypothesis. In the third column the uniform and \#3 are not statistically different and would require a higher threshold on the p-value to be so.\relax }{table.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Reduction of the uncertainty for the Uniform, \#1, \#2 and \#4 experiment, the expected value is reported \textit  {Top left}: Uniform initialisation, expected uncertainty for the Greedy (red), GMM (blue), Hybrid (green) \& Coastal (orange) search strategies. \textit  {Top right:} Experiment \#1. \textit  {Bottom left:} Experiment \#2. \textit  {Bottom right:} Experiment \#4.\relax }}{73}{figure.caption.55}}
\newlabel{fig:uncertainty}{{3.14}{73}{Reduction of the uncertainty for the Uniform, \#1, \#2 and \#4 experiment, the expected value is reported \textit {Top left}: Uniform initialisation, expected uncertainty for the Greedy (red), GMM (blue), Hybrid (green) \& Coastal (orange) search strategies. \textit {Top right:} Experiment \#1. \textit {Bottom left:} Experiment \#2. \textit {Bottom right:} Experiment \#4.\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Conclusions}{73}{section.3.7}}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Peg in hole}{75}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Peg-in-hole (PiH) search task. \textit  {Left:} A teacher is wearing ear defenders (to impede any hearing) and a blindfold. He first searches for the socket's location and then attempts to establish a connection. Force and torque information is obtained from an ATI 6-axis force torque sensor at the end-effector of the tool held by the teacher. \textit  {Right:} The KUKA LWR4 robot equipped with the same force torque sensor and plug reproducing the teacher's demonstrated behaviour.\relax }}{76}{figure.caption.56}}
\newlabel{fig:experiment_setup}{{4.1}{76}{Peg-in-hole (PiH) search task. \textit {Left:} A teacher is wearing ear defenders (to impede any hearing) and a blindfold. He first searches for the socket's location and then attempts to establish a connection. Force and torque information is obtained from an ATI 6-axis force torque sensor at the end-effector of the tool held by the teacher. \textit {Right:} The KUKA LWR4 robot equipped with the same force torque sensor and plug reproducing the teacher's demonstrated behaviour.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Outline}{76}{section.4.1}}
\citation{search_strategies_icra_2001}
\citation{online_gpr_icra_2014}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Background}{77}{section.4.2}}
\newlabel{ch4:background}{{4.2}{77}{Background}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Peg-in-hole}{77}{subsection.4.2.1}}
\citation{peg_personal_icra_2010}
\citation{peg_personal_icra_2010}
\citation{fast_peg_pbd_icmc_2014}
\citation{Schaal04learningmovement}
\citation{trans_workpiece_icra_2013}
\citation{sol_pdg_pbd_2014}
\citation{Kronander2015}
\citation{hybrid_1992}
\citation{learn_force_c_icirs_2011}
\citation{learn_admittance_icra_1994}
\citation{search_strategies_icra_2001}
\citation{peg_imcssd_2015}
\citation{intuitive_peg_isr_2013}
\citation{compliant_manip_icra_2008}
\citation{online_gpr_icra_2014}
\citation{ACML_variance_2015}
\citation{rl_ac_surv_2012}
\citation{sutton98a}
\citation{Sutton00policygradient}
\citation{Safe_val_function_1995}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Actor-Critic \& Fitted Reinforcement Learning}{80}{subsection.4.2.2}}
\citation{stable_FA_gordon_1995}
\citation{stable_FA_gordon_1995}
\citation{kernel_rl_ormoneit_2002}
\citation{fvi_uav_2010}
\citation{EGW05}
\citation{fqi_nips_peter_2009}
\citation{Riedmiller2005}
\citation{NAC_2008}
\citation{rl_gmm_2010}
\citation{Lange_riedmiller_2010}
\citation{mnih-dqn-2015}
\citation{DRQ_AAAI_2015}
\citation{approx_rl_overview_2011}
\citation{RL_state_art_2012}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The experimental setup. \textit  {Top-left:} A participant (human teacher) is blindfolded and placed within the orange rectangular area always facing the wall. \textit  {Top-right:} Dimensions of the the wall and socket. \textit  {Bottom:} Three different power sockets, only socket A and B are used for data collection, socket C is purely used for evaluating the generalisation of the learned policy.\relax }}{82}{figure.caption.57}}
\newlabel{fig:search_task_setup}{{4.2}{82}{The experimental setup. \textit {Top-left:} A participant (human teacher) is blindfolded and placed within the orange rectangular area always facing the wall. \textit {Top-right:} Dimensions of the the wall and socket. \textit {Bottom:} Three different power sockets, only socket A and B are used for data collection, socket C is purely used for evaluating the generalisation of the learned policy.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Experiment methods}{82}{section.4.3}}
\newlabel{ch4:experiment}{{4.3}{82}{Experiment methods}{section.4.3}{}}
\citation{Bergman99recursivebayesian}
\citation{NIPS2002_2319}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Human holding the cylinder plug holder, which is equipped with OptiTrack markers.\relax }}{83}{figure.caption.58}}
\newlabel{fig:plug_cylinder}{{4.3}{83}{Human holding the cylinder plug holder, which is equipped with OptiTrack markers.\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {subsubsection}{Belief state}{83}{section*.59}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces \textit  {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug. The orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit  {Right}: \textbf  {World model}: The plug is modelled by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf  {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one. \relax }}{84}{figure.caption.60}}
\newlabel{fig:PMF}{{4.4}{84}{\textit {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug. The orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit {Right}: \textbf {World model}: The plug is modelled by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one. \relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Participants and experiment protocol}{85}{subsection.4.3.1}}
\citation{Kronander2015}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Experiment protocol. The participants are divided in two groups of 5, Group A begins with socket A and after a short break repeats the task with socket B. The same logic holds for Group B. For each socket 15 executions of the task are recorded.\relax }}{86}{figure.caption.61}}
\newlabel{fig:experiment_design}{{4.5}{86}{Experiment protocol. The participants are divided in two groups of 5, Group A begins with socket A and after a short break repeats the task with socket B. The same logic holds for Group B. For each socket 15 executions of the task are recorded.\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Learning Actor and Critic}{86}{section.4.4}}
\newlabel{ch4:learning-value-actor}{{4.4}{86}{Learning Actor and Critic}{section.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces \textit  {Top}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated. \textit  {Bottom:} Time taken for the teachers to accomplish the PiH once the socket is localised. Group A and B are depicted in red and blue. The second later indicates which socket is used, see Figure \ref  {fig:experiment_design}.\relax }}{87}{figure.caption.62}}
\newlabel{fig:experiment_setup_data}{{4.6}{87}{\textit {Top}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated. \textit {Bottom:} Time taken for the teachers to accomplish the PiH once the socket is localised. Group A and B are depicted in red and blue. The second later indicates which socket is used, see Figure \ref {fig:experiment_design}.\relax }{figure.caption.62}{}}
\citation{Atkeson97locallyweighted}
\newlabel{eq:value_function}{{4.4.1}{88}{Learning Actor and Critic}{equation.4.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Actor \& Critic}{88}{subsection.4.4.1}}
\newlabel{eq:GMM}{{4.4.2}{88}{Actor \& Critic}{equation.4.4.2}{}}
\citation{EGW05}
\citation{NIPS2008_3501,EGW05,Riedmiller2005}
\newlabel{eq:W}{{4.4.3}{89}{Actor \& Critic}{equation.4.4.3}{}}
\newlabel{eq:lwr_predict}{{4.4.4}{89}{Actor \& Critic}{equation.4.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Fitted policy evaluation and improvement}{89}{subsection.4.4.2}}
\newlabel{sec:fpe}{{4.4.2}{89}{Fitted policy evaluation and improvement}{subsection.4.4.2}{}}
\newlabel{alg:fpe}{{1}{89}{Fitted policy evaluation and improvement}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Fitted Policy Evaluation\relax }}{89}{algocf.1}}
\citation{sutton1998reinforcement}
\citation{p_search_surv_2011}
\citation{peter_nac_2008}
\newlabel{eq:disc_return}{{4.4.5}{90}{Fitted policy evaluation and improvement}{equation.4.4.5}{}}
\newlabel{eq:expected_reward}{{4.4.6}{90}{Fitted policy evaluation and improvement}{equation.4.4.6}{}}
\newlabel{eq:grad_log_cost}{{4.4.7}{90}{Fitted policy evaluation and improvement}{equation.4.4.7}{}}
\newlabel{eq:advantage_f}{{4.4.8}{90}{Fitted policy evaluation and improvement}{equation.4.4.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Fitted policy evaluation \& improvement example. \textit  {Top-left:} The goal of the task is to reach the goal state. The first teacher (blue) demonstrates five trajectories which contours the obstacle in front of the goal. The second teacher (red) demonstrates 5 trajectories which initially deviate from the goal before passing between the two obstacles. \textit  {Bottom-left:} The EM algorithm is used to fit a GMM to the teachers' original data. The marginal $\pi _{\bm  {\theta }}(x)$ is plotted in blue and trajectories generated by the policy $\mathbb  {E}\{\pi _{\bm  {\theta }}(\mathaccentV {dot}05F{x}|x)\}$ in orange. \textit  {Top-right} \textit  {Policy Evaluation:}. Value function after fitted policy evaluation terminated, the reward function is binary, $r=1$ at the goal and zero otherwise, and a discount factor $\gamma = 0.99$ is used. \textit  {Bottom-right} \textit  {Policy Improvement:} the GMM is learned with the Q-EM algorithm in which each data point's weight proportional to the advantage function. \relax }}{92}{figure.caption.63}}
\newlabel{fig:fpe_example}{{4.7}{92}{Fitted policy evaluation \& improvement example. \textit {Top-left:} The goal of the task is to reach the goal state. The first teacher (blue) demonstrates five trajectories which contours the obstacle in front of the goal. The second teacher (red) demonstrates 5 trajectories which initially deviate from the goal before passing between the two obstacles. \textit {Bottom-left:} The EM algorithm is used to fit a GMM to the teachers' original data. The marginal $\pi _{\Param }(\X )$ is plotted in blue and trajectories generated by the policy $\mathbb {E}\{\pi _{\Param }(\U |\X )\}$ in orange. \textit {Top-right} \textit {Policy Evaluation:}. Value function after fitted policy evaluation terminated, the reward function is binary, $r=1$ at the goal and zero otherwise, and a discount factor $\gamma = 0.99$ is used. \textit {Bottom-right} \textit {Policy Improvement:} the GMM is learned with the Q-EM algorithm in which each data point's weight proportional to the advantage function. \relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces LWR value function approximate $\mathaccentV {hat}05E{V}^{\pi }(\mathaccentV {hat}05E{x})$ for the most likely state $\mathaccentV {hat}05E{x}$. The red plane is to help visualise where the value function is above and below the axis $z=0$. Only states with values above $0.25$ are plotted. The red arrow indicates the heading of the human teacher when performing the search task. The discount factor was $\gamma =0.99$ and the variance of the kernel variance of 1 [cm], which was set experimentally. \relax }}{93}{figure.caption.64}}
\newlabel{fig:ch4:Figure1}{{4.8}{93}{LWR value function approximate $\hat {V}^{\pi }(\hat {x})$ for the most likely state $\hat {x}$. The red plane is to help visualise where the value function is above and below the axis $z=0$. Only states with values above $0.25$ are plotted. The red arrow indicates the heading of the human teacher when performing the search task. The discount factor was $\gamma =0.99$ and the variance of the kernel variance of 1 [cm], which was set experimentally. \relax }{figure.caption.64}{}}
\citation{gesture_calinon_2010}
\citation{gmr_2004}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Best and worst trajectories. The red demonstrated trajectories are the best in terms of the amount of value function gain whilst the blue are the worst. The red arrow indicates the teacher's heading. The blue trajectories tend towards the sides of the wall as the initial starting position is on the boarders of the wall. The red trajectories are centred along the y-axis of socket and tend to move in a straight line towards the wall whilst aligning themselves with the axis $z=0$.\relax }}{94}{figure.caption.65}}
\newlabel{fig:best_worst_traj}{{4.9}{94}{Best and worst trajectories. The red demonstrated trajectories are the best in terms of the amount of value function gain whilst the blue are the worst. The red arrow indicates the teacher's heading. The blue trajectories tend towards the sides of the wall as the initial starting position is on the boarders of the wall. The red trajectories are centred along the y-axis of socket and tend to move in a straight line towards the wall whilst aligning themselves with the axis $z=0$.\relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Control architecture}{94}{section.4.5}}
\newlabel{ch4:control_architecture}{{4.5}{94}{Control architecture}{section.4.5}{}}
\newlabel{eq:gmm_conditional}{{4.5.1}{94}{Control architecture}{equation.4.5.1}{}}
\newlabel{eq:alpha_eq}{{4.5.2}{95}{Control architecture}{equation.4.5.2}{}}
\newlabel{eq:alpha_expectation}{{4.5.3}{95}{Control architecture}{equation.4.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Robot Implementation}{95}{subsection.4.5.1}}
\newlabel{eq:modulation}{{4.5.4}{95}{Robot Implementation}{equation.4.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Q-EM and GMM policy vector fields. \textit  {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit  {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation.\relax }}{96}{figure.caption.66}}
\newlabel{fig:policy_vf}{{4.10}{96}{Q-EM and GMM policy vector fields. \textit {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation.\relax }{figure.caption.66}{}}
\newlabel{eq:prop_speed}{{4.5.5}{97}{Robot Implementation}{equation.4.5.5}{}}
\newlabel{eq:torque_control}{{4.5.6}{97}{Robot Implementation}{equation.4.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces The KUKA LWR is a 7 Degree Of Freedom (DoF) robot, we illustrate in red each joint, which is controlled at a rate of 1kHz via an ethernet cable. The KUKA API provides a command interface to the stiffness, damping, position and torque variables of each joint.\relax }}{98}{figure.caption.67}}
\newlabel{fig:kuka}{{4.11}{98}{The KUKA LWR is a 7 Degree Of Freedom (DoF) robot, we illustrate in red each joint, which is controlled at a rate of 1kHz via an ethernet cable. The KUKA API provides a command interface to the stiffness, damping, position and torque variables of each joint.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Control architecture. The PMF (belief) receives a measured velocity, $\mathaccentV {dot}05F{\mathaccentV {tilde}07E{x}}$, and a sensor measurement $\mathaccentV {tilde}07E{y}$ and is updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller, Equation \ref  {eq:prop_speed}.\relax }}{98}{figure.caption.68}}
\newlabel{fig:control_flow}{{4.12}{98}{Control architecture. The PMF (belief) receives a measured velocity, $\dot {\tilde {x}}$, and a sensor measurement $\tilde {y}$ and is updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller, Equation \ref {eq:prop_speed}.\relax }{figure.caption.68}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Results}{99}{section.4.6}}
\newlabel{ch4:results}{{4.6}{99}{Results}{section.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Distance taken to reach the socket's edge (Qualitative)}{99}{subsection.4.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Three simulated search experiments. \textbf  {Experiment 1:} Three start positions are considered: \textit  {Left}, \textit  {Center} and \textit  {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. In the second row of Experiment 1, we illustrate the trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textbf  {Experiment 2:} Two cases are considered: \textit  {Case 1} blue, the initial belief state (circle) is fixed facing the left edge of the wall and the true location (diamond) is facing the socket. \textit  {Case 2} pink, the initial belief state (circle) is fixed to the right facing the edge of the wall and the true location is the left edge of the wall. In the second row, the trajectories are plotted for \textit  {Case 1}. \textbf  {Experiment 3:} A 150 start locations are deterministically generated from a grid in the start area. In the second row, we plot the distribution of the areas visited by the true position during the search.\relax }}{101}{figure.caption.69}}
\newlabel{fig:box_exp_sim}{{4.13}{101}{Three simulated search experiments. \textbf {Experiment 1:} Three start positions are considered: \textit {Left}, \textit {Center} and \textit {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. In the second row of Experiment 1, we illustrate the trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textbf {Experiment 2:} Two cases are considered: \textit {Case 1} blue, the initial belief state (circle) is fixed facing the left edge of the wall and the true location (diamond) is facing the socket. \textit {Case 2} pink, the initial belief state (circle) is fixed to the right facing the edge of the wall and the true location is the left edge of the wall. In the second row, the trajectories are plotted for \textit {Case 1}. \textbf {Experiment 3:} A 150 start locations are deterministically generated from a grid in the start area. In the second row, we plot the distribution of the areas visited by the true position during the search.\relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces First contact with the wall, during experiment 1. (a) Contact distribution for initial condition ``Center'' . (b) Contact distribution for initial condition was ``Right''. The ellipses correspond to two standard deviations of a fitted Gaussian function.\relax }}{102}{figure.caption.70}}
\newlabel{fig:first_contact}{{4.14}{102}{First contact with the wall, during experiment 1. (a) Contact distribution for initial condition ``Center'' . (b) Contact distribution for initial condition was ``Right''. The ellipses correspond to two standard deviations of a fitted Gaussian function.\relax }{figure.caption.70}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Distance taken to reach the socket's edge (Quantitative)}{102}{subsection.4.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Distance travelled until the socket's edge is reached. a) Three groups correspond to the initial conditions: Center, Left and Right depicted in Figure \ref  {fig:box_exp_sim}, \textit  {top left}. The Q-EM method is always better than the other methods, in terms of distance. b) Results of the two initial conditions depicted in Figure \ref  {fig:box_exp_sim}, \textit  {top middle}, both the true position and most likely state are fixed. The Q-EM method always improves on the GMM. \relax }}{103}{figure.caption.71}}
\newlabel{fig:three_searches}{{4.15}{103}{Distance travelled until the socket's edge is reached. a) Three groups correspond to the initial conditions: Center, Left and Right depicted in Figure \ref {fig:box_exp_sim}, \textit {top left}. The Q-EM method is always better than the other methods, in terms of distance. b) Results of the two initial conditions depicted in Figure \ref {fig:box_exp_sim}, \textit {top middle}, both the true position and most likely state are fixed. The Q-EM method always improves on the GMM. \relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Distance travelled until the socket's edge is reached. Results corresponding to Experiment 3, Figure \ref  {fig:box_exp_sim}, \textit  {top right}. Again the Q-EM method is better, but at a less significant level.\relax }}{103}{figure.caption.72}}
\newlabel{fig:three_searches_exp3}{{4.16}{103}{Distance travelled until the socket's edge is reached. Results corresponding to Experiment 3, Figure \ref {fig:box_exp_sim}, \textit {top right}. Again the Q-EM method is better, but at a less significant level.\relax }{figure.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Demonstrations of teacher \# 5. The teacher demonstrates a preference\relax }}{104}{figure.caption.73}}
\newlabel{fig:subj_5_traj}{{4.17}{104}{Demonstrations of teacher \# 5. The teacher demonstrates a preference\relax }{figure.caption.73}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Importance of data}{104}{subsection.4.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Value function learned from the 15 demonstrations of teacher \#5. The value of the most likely state is plotted.\relax }}{105}{figure.caption.74}}
\newlabel{fig:value_function_subj_5}{{4.18}{105}{Value function learned from the 15 demonstrations of teacher \#5. The value of the most likely state is plotted.\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Marginalised Gaussian Mixture parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. The illustrated transparency of the Gaussian functions is proportional to their weight. \textit  {Left column}: The Gaussian functions of the Q-EM have shifted from the left corner to the right. This is a result of the value function being higher in the top right corner region, see Figure \ref  {fig:value_function_subj_5}. \textit  {Center column}: The original data of the teacher went quite far back which results in a Gaussian function given a direction which moves away from the wall (green arrow), whilst in the case of the Q-EM parameters this effect is reduced and moved closer towards the wall. We can also see from the two plots of the Q-EM parameters that they then follow the paths encoded by the value function. \textit  {Right column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge.\relax }}{106}{figure.caption.75}}
\newlabel{fig:gmm_exp4}{{4.19}{106}{Marginalised Gaussian Mixture parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. The illustrated transparency of the Gaussian functions is proportional to their weight. \textit {Left column}: The Gaussian functions of the Q-EM have shifted from the left corner to the right. This is a result of the value function being higher in the top right corner region, see Figure \ref {fig:value_function_subj_5}. \textit {Center column}: The original data of the teacher went quite far back which results in a Gaussian function given a direction which moves away from the wall (green arrow), whilst in the case of the Q-EM parameters this effect is reduced and moved closer towards the wall. We can also see from the two plots of the Q-EM parameters that they then follow the paths encoded by the value function. \textit {Right column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge.\relax }{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.\relax }}{107}{figure.caption.76}}
\newlabel{fig:experiment4_stats}{{4.20}{107}{Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.\relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Generalisation}{107}{subsection.4.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit  {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. \relax }}{108}{figure.caption.77}}
\newlabel{fig:experiment5_traj}{{4.21}{108}{Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. \relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{108}{figure.caption.77}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Distance taken to reach the socket's edge. For the Fixed setup (see Figure \ref  {fig:experiment5_traj}) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. \relax }}{108}{figure.caption.78}}
\newlabel{fig:experiment5_stats}{{4.22}{108}{Distance taken to reach the socket's edge. For the Fixed setup (see Figure \ref {fig:experiment5_traj}) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. \relax }{figure.caption.78}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{108}{figure.caption.78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.5}Distance taken to connect the plug to the socket}{109}{subsection.4.6.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces 25 search trajectories for each of the three search policies for socket A. \relax }}{110}{figure.caption.79}}
\newlabel{fig:real_policy}{{4.23}{110}{25 search trajectories for each of the three search policies for socket A. \relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{110}{figure.caption.79}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row shows screen captures taken of ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. (b) Same initial condition as in (a) but with socket C. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then making a connection.\relax }}{111}{figure.caption.80}}
\newlabel{fig:real_pictures}{{4.24}{111}{KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row shows screen captures taken of ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. (b) Same initial condition as in (a) but with socket C. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then making a connection.\relax }{figure.caption.80}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{111}{figure.caption.80}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{111}{figure.caption.80}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Distance taken to connect plug to socket once the socket is localised. (a) \textbf  {Socket A}. The human Group A are the set of teachers who first started with socket A. They had no previous training on another socket beforehand. Group BA first gave demonstrations on Socket B before giving demonstrations on Socket A. Group BA is better than Group AA at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the plug to the socket. (b) \textbf  {Socket B}. Both Groups AB and BB are similar in terms of the distance they took to insert the plug into the socket and as was the case for (a), the search policies travel less to accomplish the task. \relax }}{111}{figure.caption.81}}
\newlabel{fig:real_statistics}{{4.25}{111}{Distance taken to connect plug to socket once the socket is localised. (a) \textbf {Socket A}. The human Group A are the set of teachers who first started with socket A. They had no previous training on another socket beforehand. Group BA first gave demonstrations on Socket B before giving demonstrations on Socket A. Group BA is better than Group AA at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the plug to the socket. (b) \textbf {Socket B}. Both Groups AB and BB are similar in terms of the distance they took to insert the plug into the socket and as was the case for (a), the search policies travel less to accomplish the task. \relax }{figure.caption.81}{}}
\citation{Chambrier2014}
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces Distance taken (measured from point of contact of plug with socket edge) to connect the plug to the socket.\relax }}{112}{figure.caption.82}}
\newlabel{fig:real_statistics2}{{4.26}{112}{Distance taken (measured from point of contact of plug with socket edge) to connect the plug to the socket.\relax }{figure.caption.82}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Discussion \& Conclusion}{112}{section.4.7}}
\newlabel{ch4:conclusion}{{4.7}{112}{Discussion \& Conclusion}{section.4.7}{}}
\citation{Thrun_Burgard_Fox_2005}
\citation{Thrun02particlefilters,negative_info_markov_localisation}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Non-parametric Bayesian State Space Estimator}{115}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{Thrun_Burgard_Fox_2005}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces  \textit  {Table Environment} Table World (delimited by the black rectangle), viewed from above, and the agent's beliefs. There are three different probability density functions present on the table. The blue represents the believed location of the agent, the red and green probability distributions are associated with object 1 and 2. The white shapes in each figure represent the true location of each associated object or agent.\relax }}{116}{figure.caption.83}}
\newlabel{fig:Figure1}{{5.1}{116}{\textit {Table Environment} Table World (delimited by the black rectangle), viewed from above, and the agent's beliefs. There are three different probability density functions present on the table. The blue represents the believed location of the agent, the red and green probability distributions are associated with object 1 and 2. The white shapes in each figure represent the true location of each associated object or agent.\relax }{figure.caption.83}{}}
\citation{NegInfoFurtherStudies}
\citation{Bake_Saxe_Tene_2011}
\citation{deChambrier2013}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Assumptions and attributes which have to be fulfilled by our Bayesian State Space Filter. \relax }}{117}{figure.caption.84}}
\newlabel{fig:ch5_assmuptions}{{5.2}{117}{Assumptions and attributes which have to be fulfilled by our Bayesian State Space Filter. \relax }{figure.caption.84}{}}
\citation{Plagemann07gaussianbeam}
\citation{DataAssociation2003}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Outline}{118}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Background}{118}{section.5.2}}
\newlabel{ch5:background}{{5.2}{118}{Background}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}SLAM}{118}{subsection.5.2.1}}
\citation{SLAM_part1}
\citation{TutGraphSLAM}
\citation{FastSLAM}
\citation{Thrun_Burgard_Fox_2005}
\citation{SLAM_HBR}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Active-SLAM \& Exploration}{119}{subsection.5.2.2}}
\citation{Thrun_grid_based_1996}
\citation{Kollar_2008_Exploration_SLAM}
\citation{Navigation_strategires_for_exploring_indoor_environments}
\citation{PRM_1996}
\citation{RRT-SLAM}
\citation{ActivePosSLAM}
\citation{stachniss05robotics}
\citation{Ross08onlineplanning}
\citation{GeorgiosLidoris}
\citation{Active_SLAM_Uncertainty_compar,KL_SLAM_exploration_PF}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Bayesian State Space Estimation}{120}{section.5.3}}
\newlabel{ch5:BSSE}{{5.3}{120}{Bayesian State Space Estimation}{section.5.3}{}}
\citation{BayesBall}
\citation{barberBRML2012}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Directed graphical model of dependencies between the agent(A) and object(O)'s estimated location. Each object, $O^{(i)}$ is associated with one sensing random variable $Y^{(i)}$. The overall sensing random variable is $Y = \left [Y^{(1)},\dots  ,Y^{(M-1)}\right ]^{\mathrm  {T}}$, where $M$ is the total number of agent and object random variables in the filter. For readability we have left out the time index $t$ from $A$ and $Y$. Since the objects are static, they have no temporal process associated with them thus they will never have a time subscript. The two models necessary for filtering are the motion model $P(A_t|A_{t-1},u_t)$ (red) and measurement model $P(Y_t|A_t,O)$ (blue).\relax }}{121}{figure.caption.85}}
\newlabel{fig:bayesian_sse_dag}{{5.3}{121}{Directed graphical model of dependencies between the agent(A) and object(O)'s estimated location. Each object, $O^{(i)}$ is associated with one sensing random variable $Y^{(i)}$. The overall sensing random variable is $Y = \left [Y^{(1)},\dots ,Y^{(M-1)}\right ]^{\mathrm {T}}$, where $M$ is the total number of agent and object random variables in the filter. For readability we have left out the time index $t$ from $A$ and $Y$. Since the objects are static, they have no temporal process associated with them thus they will never have a time subscript. The two models necessary for filtering are the motion model $P(A_t|A_{t-1},u_t)$ (red) and measurement model $P(Y_t|A_t,O)$ (blue).\relax }{figure.caption.85}{}}
\newlabel{eq:joint}{{5.3.1}{121}{Bayesian State Space Estimation}{equation.5.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Dependence and independence relation between the random variables of the BN Figure \ref  {fig:bayesian_sse_dag}\relax }}{122}{figure.caption.86}}
\newlabel{fig:ch5_dseperation}{{5.4}{122}{Dependence and independence relation between the random variables of the BN Figure \ref {fig:bayesian_sse_dag}\relax }{figure.caption.86}{}}
\@writefile{toc}{\contentsline {subsubsection}{EKF-SLAM}{122}{section*.87}}
\newlabel{sec:EKF-SLAM}{{5.3}{122}{EKF-SLAM}{section*.87}{}}
\citation{SLAM_part1}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces \textbf  {a)} EKF-SLAM time slice evolutions of the pdfs. The temporal ordering is given by the numbers in the top right corner of each plot. The blue pdf represents the agent's believed location and the circle is the agent's true location. The same holds for the red distribution which represents the agent's belief of the location of an object. \textbf  {b)} Evolution of the covariance components of $\Sigma $ over time and true $Y_t$ and expected measurements, $\mathaccentV {hat}05E{Y}_t$. $\Sigma _a$ and $\Sigma _o$ are the variances of the agent and object positions and $\Sigma _{ao}$ is the cross-covariance term.\relax }}{123}{figure.caption.88}}
\newlabel{fig:EKF-SLAM}{{5.5}{123}{\textbf {a)} EKF-SLAM time slice evolutions of the pdfs. The temporal ordering is given by the numbers in the top right corner of each plot. The blue pdf represents the agent's believed location and the circle is the agent's true location. The same holds for the red distribution which represents the agent's belief of the location of an object. \textbf {b)} Evolution of the covariance components of $\Sigma $ over time and true $Y_t$ and expected measurements, $\hat {Y}_t$. $\Sigma _a$ and $\Sigma _o$ are the variances of the agent and object positions and $\Sigma _{ao}$ is the cross-covariance term.\relax }{figure.caption.88}{}}
\newlabel{eq:lik-measurement}{{5.3.3}{123}{EKF-SLAM}{equation.5.3.3}{}}
\newlabel{eq:ch5:measurement_ekf}{{5.3.4}{123}{EKF-SLAM}{equation.5.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Histogram-SLAM}{124}{section*.89}}
\newlabel{sec:Discrete}{{5.3}{124}{Histogram-SLAM}{section*.89}{}}
\newlabel{eq:agent_marginal}{{5.3.5}{124}{Histogram-SLAM}{equation.5.3.5}{}}
\newlabel{eq:ch5:discrete_likelihoood}{{5.3.6}{124}{Histogram-SLAM}{equation.5.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces \textbf  {Top}: \textit  {Left:} Initialisation of the agent and object joint distribution. \textit  {Right:} Marginals of the agent and object parameterised by $\bm  {\theta }_a$ and $\bm  {\theta }_o$, giving the probability of their location. The marginal of each random variable is obtained from Equation \ref  {eq:agent_marginal}. The probability of the agent and object being in state $s=6$ is given by summing the blue and red highlighted parameters in the joint distribution. \textbf  {Bottom}: 1D world Likelihood $P(Y_t|A_t,O)$, the white regions $A \cap O$ will leave the joint distribution unchanged whilst the black regions will evaluate the joint distribution to zero. \textit  {Left:} No contact detected with the object, the current measurement is $Y_t = \leavevmode {\color  {red}0}$, both the agent and object cannot be in the same state. \textit  {Right:} The agent entered into contact with the object and received a haptic feedback $Y_t = \leavevmode {\color  {red}1}$. The agent receives only two measurement possibilities, contact or no contact. \relax }}{125}{figure.caption.90}}
\newlabel{fig:histogram_joint}{{5.6}{125}{\textbf {Top}: \textit {Left:} Initialisation of the agent and object joint distribution. \textit {Right:} Marginals of the agent and object parameterised by $\ThA $ and $\ThO $, giving the probability of their location. The marginal of each random variable is obtained from Equation \ref {eq:agent_marginal}. The probability of the agent and object being in state $s=6$ is given by summing the blue and red highlighted parameters in the joint distribution. \textbf {Bottom}: 1D world Likelihood $P(Y_t|A_t,O)$, the white regions $A \cap O$ will leave the joint distribution unchanged whilst the black regions will evaluate the joint distribution to zero. \textit {Left:} No contact detected with the object, the current measurement is $Y_t = \textcolor {red}0$, both the agent and object cannot be in the same state. \textit {Right:} The agent entered into contact with the object and received a haptic feedback $Y_t = \textcolor {red}1$. The agent receives only two measurement possibilities, contact or no contact. \relax }{figure.caption.90}{}}
\newlabel{eq:ch5:disc_prod_AO}{{5.3.7}{126}{Histogram-SLAM}{equation.5.3.7}{}}
\newlabel{eq:ch5:disc_motion}{{5.3.8}{126}{Histogram-SLAM}{equation.5.3.8}{}}
\newlabel{eq:ch5:disc_measurement}{{5.3.9}{126}{Histogram-SLAM}{equation.5.3.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Histogram-SLAM, 4 time steps. \textbf  {1} Application of likelihood $P(Y_0=0|A_0,O)$ and the agent remains stationary, $u_1=0$, all states along the green line become zero. \textbf  {2} The agent moves to the right $u_1=1$, the motion $P(A_1|A_0,u_1)$, and likelihood models are applied consecutively. The right motion results in a shift (black arrow on the left) in the joint probability distribution towards the state $i=10$. All parameters on the pink line are zero. \textbf  {3} Same as two. \textbf  {4} The original result of the likelihood function, green line, has moved by the same amount as the agent's displacement. At each time step a new likelihood function (pink line) is applied to the joint distribution.\relax }}{127}{figure.caption.91}}
\newlabel{fig:discrete_example}{{5.7}{127}{Histogram-SLAM, 4 time steps. \textbf {1} Application of likelihood $P(Y_0=0|A_0,O)$ and the agent remains stationary, $u_1=0$, all states along the green line become zero. \textbf {2} The agent moves to the right $u_1=1$, the motion $P(A_1|A_0,u_1)$, and likelihood models are applied consecutively. The right motion results in a shift (black arrow on the left) in the joint probability distribution towards the state $i=10$. All parameters on the pink line are zero. \textbf {3} Same as two. \textbf {4} The original result of the likelihood function, green line, has moved by the same amount as the agent's displacement. At each time step a new likelihood function (pink line) is applied to the joint distribution.\relax }{figure.caption.91}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Histogram-SLAM contact. The agent has entered in contact with the object (measurement $Y_t = 1$) and the likelihood function $P(Y_t=1|A_t,O)$ is applied to the joint distribution. Only parameters on the line $i=j$ will remain unchanged and parameters for which $i \not = j$ will be set to zero.\relax }}{128}{figure.caption.92}}
\newlabel{fig:discrete_example_contact}{{5.8}{128}{Histogram-SLAM contact. The agent has entered in contact with the object (measurement $Y_t = 1$) and the likelihood function $P(Y_t=1|A_t,O)$ is applied to the joint distribution. Only parameters on the line $i=j$ will remain unchanged and parameters for which $i \not = j$ will be set to zero.\relax }{figure.caption.92}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Measurement Likelihood Memory Filter}{129}{section.5.4}}
\newlabel{ch5:MLMF}{{5.4}{129}{Measurement Likelihood Memory Filter}{section.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}MLMF parametrisation}{129}{subsection.5.4.1}}
\newlabel{eq:ch5:prod_AO}{{5.4.1}{130}{MLMF parametrisation}{equation.5.4.1}{}}
\newlabel{eq:ch5:mlmf_motion_update}{{5.4.2}{130}{MLMF parametrisation}{equation.5.4.2}{}}
\newlabel{eq:ch5:mlmf_measurement_update}{{5.4.3}{130}{MLMF parametrisation}{equation.5.4.3}{}}
\newlabel{eq:ch5:mlmf_filter_conditional}{{5.4.4}{130}{MLMF parametrisation}{equation.5.4.4}{}}
\newlabel{eq:marignal_mrf}{{5.4.5}{130}{MLMF parametrisation}{equation.5.4.5}{}}
\newlabel{eq:memory}{{5.4.7}{130}{MLMF parametrisation}{equation.5.4.7}{}}
\newlabel{eq:ch5:liklihood_v2}{{5.4.8}{130}{MLMF parametrisation}{equation.5.4.8}{}}
\newlabel{eq:ch5:offset}{{5.4.9}{130}{MLMF parametrisation}{equation.5.4.9}{}}
\newlabel{alg:memory-motion}{{2}{131}{MLMF parametrisation}{algocfline.2}{}}
\newlabel{alg:ch5:motion_memory}{{2}{131}{MLMF parametrisation}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Memory Likelihood update\relax }}{131}{algocf.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Un-normalised MLMF joint distribution, numerator of Equation \ref  {eq:ch5:mlmf_filter_conditional}, at time $t=3$. Three measurements (all $Y=0$) and two actions (both $u=1$) have been integrated into the joint distribution, for simplicity we do not consider any motion noise. \textit  {Left column:} The first plot illustrates the likelihood of the first measurement $Y_0$. We highlight the contour in light-green to indicate that it was the first applied likelihood function (see the correspondence with Figure \ref  {fig:discrete_example}). The first likelihood function has been moved by the 2 actions, the second likelihood function has been moved by one action (the last one, $u_2=1$) and the third likelihood has had no action applied to it yet. The last applied likelihood function is highlighted in pink and the product of all the likelihoods since $t=0$ until $t=3$ is depicted at the bottom of the figure which is $P(Y_{0:2}|A_2,O,u_{1:2})$. \textit  {Right column:} the top figure illustrates the original marginal of the object $P(O;\bm  {\theta }^*_o)$, which remains unchanged, and the agent's marginal $P(A_2|u_{1:2};\bm  {\theta }^*_a)$ which has moved in accordance to the motion update function. Their product would results in the joint distribution $P(A_2,O|u_{1:2};\bm  {\theta }^*_a,\bm  {\theta }^*_o)$ illustrated in the middle figure if evaluated at each state $(i,j)$. The bottom figure is the result of multiplying $P(A_2,O|u_{1:2};\bm  {\theta }^*_a,\bm  {\theta }^*_o)$ with $P(Y_{0:2}|A_2,O,u_{1:2};\bm  {\Psi _{0:2}})$ giving the filtered joint distribution, Equation \ref  {eq:ch5:mlmf_filter_conditional}. \relax }}{132}{figure.caption.93}}
\newlabel{fig:maringal_joint_example_v2}{{5.9}{132}{Un-normalised MLMF joint distribution, numerator of Equation \ref {eq:ch5:mlmf_filter_conditional}, at time $t=3$. Three measurements (all $Y=0$) and two actions (both $u=1$) have been integrated into the joint distribution, for simplicity we do not consider any motion noise. \textit {Left column:} The first plot illustrates the likelihood of the first measurement $Y_0$. We highlight the contour in light-green to indicate that it was the first applied likelihood function (see the correspondence with Figure \ref {fig:discrete_example}). The first likelihood function has been moved by the 2 actions, the second likelihood function has been moved by one action (the last one, $u_2=1$) and the third likelihood has had no action applied to it yet. The last applied likelihood function is highlighted in pink and the product of all the likelihoods since $t=0$ until $t=3$ is depicted at the bottom of the figure which is $P(Y_{0:2}|A_2,O,u_{1:2})$. \textit {Right column:} the top figure illustrates the original marginal of the object $P(O;\ThOs )$, which remains unchanged, and the agent's marginal $P(A_2|u_{1:2};\ThAs )$ which has moved in accordance to the motion update function. Their product would results in the joint distribution $P(A_2,O|u_{1:2};\ThAs ,\ThOs )$ illustrated in the middle figure if evaluated at each state $(i,j)$. The bottom figure is the result of multiplying $P(A_2,O|u_{1:2};\ThAs ,\ThOs )$ with $P(Y_{0:2}|A_2,O,u_{1:2};\boldsymbol {\Psi _{0:2}})$ giving the filtered joint distribution, Equation \ref {eq:ch5:mlmf_filter_conditional}. \relax }{figure.caption.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Evidence and marginals}{133}{subsection.5.4.2}}
\newlabel{eq:joint_independent_dependent}{{5.4.10}{133}{Evidence and marginals}{equation.5.4.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Evidence}{133}{section*.95}}
\newlabel{eq:I_v1}{{5.4.11}{133}{Evidence}{equation.5.4.11}{}}
\newlabel{eq:I_v2}{{5.4.12}{133}{Evidence}{equation.5.4.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces  \textbf  {a)} Likelihood $P(Y_t=0|A_t,O)$, the blue area depicts the regions in which the likelihood is $<1$ and the red area is where the likelihood is $=1$. If the probability mass of the joint distribution is in the blue region, then the parameters of the random variables in these areas are dependent, $A \cap O$. Otherwise they are independent, $A \ominus O$. \textbf  {b)} The agent and object marginals are not overlapping and are thus completely independent. The joint distribution, $P(A_t,O|Y_{0:t})$ the black rectangle, is not intersecting with the measurement function. As a result $P_{\cap }(A_t,O|Y_{0:t})$ is empty. \textbf  {c)} The marginals overlap resulting in the measurement likelihood function intersecting with the joint distribution. The joint distribution is composed of the blue and red areas, Equation \ref  {eq:joint_independent_dependent}. The probability mass at the intersection is removed and re-normalised to other regions which is the result of applying Bayes integration. \textbf  {d)} The marginals of $A$ and $O$ are completely overlapping, however only a small fraction of the probability mass in the joint distribution is within the measurement function's tube.\relax }}{134}{figure.caption.94}}
\newlabel{fig:overlap_dependence_independence}{{5.10}{134}{\textbf {a)} Likelihood $P(Y_t=0|A_t,O)$, the blue area depicts the regions in which the likelihood is $<1$ and the red area is where the likelihood is $=1$. If the probability mass of the joint distribution is in the blue region, then the parameters of the random variables in these areas are dependent, $A \cap O$. Otherwise they are independent, $A \ominus O$. \textbf {b)} The agent and object marginals are not overlapping and are thus completely independent. The joint distribution, $P(A_t,O|Y_{0:t})$ the black rectangle, is not intersecting with the measurement function. As a result $P_{\cap }(A_t,O|Y_{0:t})$ is empty. \textbf {c)} The marginals overlap resulting in the measurement likelihood function intersecting with the joint distribution. The joint distribution is composed of the blue and red areas, Equation \ref {eq:joint_independent_dependent}. The probability mass at the intersection is removed and re-normalised to other regions which is the result of applying Bayes integration. \textbf {d)} The marginals of $A$ and $O$ are completely overlapping, however only a small fraction of the probability mass in the joint distribution is within the measurement function's tube.\relax }{figure.caption.94}{}}
\@writefile{toc}{\contentsline {subsubsection}{Marginals}{135}{section*.96}}
\newlabel{eq:marignal_mrf}{{5.4.13}{135}{Marginals}{equation.5.4.13}{}}
\newlabel{eq:marignal_mrf_2}{{5.4.14}{135}{Marginals}{equation.5.4.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Filtered Marginals. Illustration of the agent and object marginal update, Equation \ref  {eq:marignal_mrf}. The joint distribution parameters which are independent $A \ominus O$ are pale and the dependent areas $A \cap O$, where $P(Y_t<1|A_t,O)$, are bright. MLMF only evaluates the joint distribution in dependent states. For each state $s$ of the marginals $1,\dots  ,10$ the difference of the marginals inside the dependent area, before and after the measurement likelihood is applied, is evaluated and removed from the marginals $P(A_t|Y_{0:t-1})$, $P(O|Y_{0:t-1})$ leading to $P(A_t|Y_{0:t})$, $P(O|Y_{0:t})$ (we did not show $u_{1:t}$ for ease of notation). \relax }}{136}{figure.caption.97}}
\newlabel{fig:ch5:marginal_update}{{5.11}{136}{Filtered Marginals. Illustration of the agent and object marginal update, Equation \ref {eq:marignal_mrf}. The joint distribution parameters which are independent $A \ominus O$ are pale and the dependent areas $A \cap O$, where $P(Y_t<1|A_t,O)$, are bright. MLMF only evaluates the joint distribution in dependent states. For each state $s$ of the marginals $1,\dots ,10$ the difference of the marginals inside the dependent area, before and after the measurement likelihood is applied, is evaluated and removed from the marginals $P(A_t|Y_{0:t-1})$, $P(O|Y_{0:t-1})$ leading to $P(A_t|Y_{0:t})$, $P(O|Y_{0:t})$ (we did not show $u_{1:t}$ for ease of notation). \relax }{figure.caption.97}{}}
\citation{PF_tutorial_2002}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces MLMF functions with associated parameters. The marginal parameters are the discretisation of the state space $\bm  {\theta } \in \mathbb  {R}^N$, $\bm  {\theta }^{(s)}$ correspond to the probability being in state $s$.\relax }}{137}{table.caption.98}}
\newlabel{tab:mlmf_parameters}{{5.1}{137}{MLMF functions with associated parameters. The marginal parameters are the discretisation of the state space $\boldsymbol {\theta } \in \mathbb {R}^N$, $\boldsymbol {\theta }^{(s)}$ correspond to the probability being in state $s$.\relax }{table.caption.98}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}MLMF-SLAM Algorithm}{137}{subsection.5.4.3}}
\newlabel{alg:mrf-slam}{{3}{139}{MLMF-SLAM Algorithm}{algocfline.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces MLMF-SLAM\relax }}{139}{algocf.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Space \& time complexity}{140}{subsection.5.4.4}}
\newlabel{ch5:space_time_complexity_MLMF}{{5.4.4}{140}{Space \& time complexity}{subsection.5.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Space complexity}{140}{section*.99}}
\@writefile{toc}{\contentsline {subsubsection}{Time complexity}{140}{section*.101}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces \textit  {Left:} Joint distribution $P(A,O^{(1)},O^{(2)})$ of the agent and two objects. Each measurement likelihood function, $P(Y|A,O^{(1)})$, $P(Y|A,O^{(2)})$ corresponds to a hyperplane in the joint distribution The state space is discretised to $N$ bins giving the total number of parameters in the joint distribution of $N^3$. \textit  {Right:} \textbf  {Scalable-MLMF} Each agent-object joint distribution pair is modelled independently. For clarity we have left out the action random variable $u$ which is linked to every agent node. Two joint distributions $P(A^{(1)},O^{(1)}|Y^{(1)}_{0:t})$ and $P(A^{(2)},O^{(2)}|Y^{(2)}_{0:t})$ parametrise the graphical model. The dashed undirected lines represent a wanted dependency, if present $O^{(1)}$ and $O^{(2)}$ are to be dependent through $A$. In the standard setting there will be no exchange of information between the individual joint distributions. However we demonstrate later on how we perform a one time transfer of information when one of the objects is sensed.\relax }}{141}{figure.caption.100}}
\newlabel{fig:3bel_lik_profile}{{5.12}{141}{\textit {Left:} Joint distribution $P(A,O^{(1)},O^{(2)})$ of the agent and two objects. Each measurement likelihood function, $P(Y|A,O^{(1)})$, $P(Y|A,O^{(2)})$ corresponds to a hyperplane in the joint distribution The state space is discretised to $N$ bins giving the total number of parameters in the joint distribution of $N^3$. \textit {Right:} \textbf {Scalable-MLMF} Each agent-object joint distribution pair is modelled independently. For clarity we have left out the action random variable $u$ which is linked to every agent node. Two joint distributions $P(A^{(1)},O^{(1)}|Y^{(1)}_{0:t})$ and $P(A^{(2)},O^{(2)}|Y^{(2)}_{0:t})$ parametrise the graphical model. The dashed undirected lines represent a wanted dependency, if present $O^{(1)}$ and $O^{(2)}$ are to be dependent through $A$. In the standard setting there will be no exchange of information between the individual joint distributions. However we demonstrate later on how we perform a one time transfer of information when one of the objects is sensed.\relax }{figure.caption.100}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.5}Scalable extension to multiple objects}{142}{subsection.5.4.5}}
\newlabel{subsec:scalabe_extension}{{5.4.5}{142}{Scalable extension to multiple objects}{subsection.5.4.5}{}}
\newlabel{eq:pair_wise_joint}{{5.4.15}{142}{Scalable extension to multiple objects}{equation.5.4.15}{}}
\newlabel{eq:marg_indep}{{5.4.16}{142}{Scalable extension to multiple objects}{equation.5.4.16}{}}
\newlabel{eq:marg_indep_prod}{{5.4.17}{142}{Scalable extension to multiple objects}{equation.5.4.17}{}}
\newlabel{eq:marg_indep_sum}{{5.4.18}{143}{Scalable extension to multiple objects}{equation.5.4.18}{}}
\newlabel{alg:scalabe-mrf-slam}{{4}{143}{Scalable extension to multiple objects}{algocfline.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Scalable-MLMF: Measurement Update\relax }}{143}{algocf.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces \textbf  {Transfer of information (joint distributions)} \textit  {Top:} Joint distributions of $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$ and $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$ prior sensing, $Y_t^{(2)}=1$, see Figure \ref  {fig:independence_object} (\textit  {Top right}) for the corresponding marginals. The red and green lines across the joint distributions correspond to the region in which the likelihood functions $P(Y^{(1)}_{t}|A^{(1)}_t,O^{(1)})$ and $P(Y^{(2)}_{t}|A^{(2)}_t,O^{(2)})$ will change the joint distributions. The dotted blue lines are to ease the comparison ofthe joint distributions prior and post sensing. \textit  {Bottom right:} After the agent has sensed $O^{(2)}$, all the probability mass which was not overlapping the green line becomes an infeasible solution to the agent and object locations. At this point the marginals $P(A^{(1)}_t|u_{1:t}) \not = P(A^{(2)}_t|u_{1:t})$ are no longer equal (see the blue marginals \textit  {Top}). \textit  {Bottom left:} The constraint imposed by the likelihood function of the second object (green line) is transferred to the joint distribution of the first object according to Algorithm \ref  {alg:scalabe-mrf-slam}. This results in a change in the joint distribution $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$, which satisfies the constraints imposed by the agent's marginal from the joint distribution $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$.\relax }}{144}{figure.caption.102}}
\newlabel{fig:transfer_information}{{5.13}{144}{\textbf {Transfer of information (joint distributions)} \textit {Top:} Joint distributions of $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$ and $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$ prior sensing, $Y_t^{(2)}=1$, see Figure \ref {fig:independence_object} (\textit {Top right}) for the corresponding marginals. The red and green lines across the joint distributions correspond to the region in which the likelihood functions $P(Y^{(1)}_{t}|A^{(1)}_t,O^{(1)})$ and $P(Y^{(2)}_{t}|A^{(2)}_t,O^{(2)})$ will change the joint distributions. The dotted blue lines are to ease the comparison ofthe joint distributions prior and post sensing. \textit {Bottom right:} After the agent has sensed $O^{(2)}$, all the probability mass which was not overlapping the green line becomes an infeasible solution to the agent and object locations. At this point the marginals $P(A^{(1)}_t|u_{1:t}) \not = P(A^{(2)}_t|u_{1:t})$ are no longer equal (see the blue marginals \textit {Top}). \textit {Bottom left:} The constraint imposed by the likelihood function of the second object (green line) is transferred to the joint distribution of the first object according to Algorithm \ref {alg:scalabe-mrf-slam}. This results in a change in the joint distribution $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$, which satisfies the constraints imposed by the agent's marginal from the joint distribution $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$.\relax }{figure.caption.102}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces \textbf  {Transfer of information (marginals)} \textit  {Top left:} Initial beliefs of the agent and object's location. The agent moves to the left until it senses object $O^{(2)}$. \textit  {Top right:} Marginals prior the agent entering in contact with the green object, see Figure \ref  {fig:transfer_information} (\textit  {Top}) for an illustrate of the joint distributions. \textit  {Bottom left:} resulting marginals after setting the agent marginals of each joint distribution equal $A^{(1)}_t = A^{(2)}_t$ according to Algorithm \ref  {alg:scalabe-mrf-slam}. The object marginal $P(O^{(2)}|Y_{0:t})$ is recomputed. \textit  {Bottom Right:} resulting marginals in which the objects have no influence on one another. Note that a transfer of information has caused a change in the marginal $O^{(1)}$.\relax }}{145}{figure.caption.103}}
\newlabel{fig:independence_object}{{5.14}{145}{\textbf {Transfer of information (marginals)} \textit {Top left:} Initial beliefs of the agent and object's location. The agent moves to the left until it senses object $O^{(2)}$. \textit {Top right:} Marginals prior the agent entering in contact with the green object, see Figure \ref {fig:transfer_information} (\textit {Top}) for an illustrate of the joint distributions. \textit {Bottom left:} resulting marginals after setting the agent marginals of each joint distribution equal $A^{(1)}_t = A^{(2)}_t$ according to Algorithm \ref {alg:scalabe-mrf-slam}. The object marginal $P(O^{(2)}|Y_{0:t})$ is recomputed. \textit {Bottom Right:} resulting marginals in which the objects have no influence on one another. Note that a transfer of information has caused a change in the marginal $O^{(1)}$.\relax }{figure.caption.103}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces \textbf  {Time and space complexity summary} For both MLMF and scalabe-MLMF the worst case scenario is reported for the space complexity.\relax }}{145}{table.caption.104}}
\newlabel{tab:time_space_summary}{{5.2}{145}{\textbf {Time and space complexity summary} For both MLMF and scalabe-MLMF the worst case scenario is reported for the space complexity.\relax }{table.caption.104}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Evaluation}{145}{section.5.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces \textbf  {Time complexity:} \textit  {left:} mean time taken for a loop update (motion and measurement) as a function of the number of states in a marginal and the number of objects present. \textit  {right:} time taken for a loop update with respect to the number of states in the marginal. The colour coded lines are associated with the number of objects present. The computational cost is plotted on a log scale. As the number of states increases exponentially the computational cost matches it.\relax }}{146}{figure.caption.105}}
\newlabel{fig:time_complexity}{{5.15}{146}{\textbf {Time complexity:} \textit {left:} mean time taken for a loop update (motion and measurement) as a function of the number of states in a marginal and the number of objects present. \textit {right:} time taken for a loop update with respect to the number of states in the marginal. The colour coded lines are associated with the number of objects present. The computational cost is plotted on a log scale. As the number of states increases exponentially the computational cost matches it.\relax }{figure.caption.105}{}}
\newlabel{ch5:evaluation}{{5.5}{146}{Evaluation}{section.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Evaluation of time complexity}{146}{subsection.5.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Evaluation of the independence assumption}{146}{subsection.5.5.2}}
\newlabel{subsec:eval_indep_assumptiom}{{5.5.2}{147}{Evaluation of the independence assumption}{subsection.5.5.2}{}}
\newlabel{eq:hellinger}{{5.5.1}{147}{Evaluation of the independence assumption}{equation.5.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces \textbf  {Comparison of scalable-MLMF and the histogram filter} A deterministic sweep policy was carried out for 100 different initialisations of the agent and object beliefs. \textit  {Top left:} One particular Initialisation of the agent and object random variables. The true position of the agent and objects were sampled at random. The black arrow indicates the general policy which was followed for each of the 100 sweeps. These were performed for \textbf  {1)} scalable-MLMF with objects considered to be independent at all times (no Algorithm \ref  {alg:scalabe-mrf-slam}). \textbf  {2)} Agent marginal $P(A_t|Y_{0:t},u_{1:t})$ is the product of marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref  {eq:marg_indep_prod}. \textbf  {3)} marginal $P(A_t|Y_{0:t},u_{1:t})$ is taken to be the average of all marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref  {eq:marg_indep_sum}. For each of these three experiment we report the kernel density estimation over the Hellinger distances taken at every time step between ground truth (from histogram filter) and scalable-MLMF.\relax }}{148}{figure.caption.106}}
\newlabel{fig:independence_assumption_test}{{5.16}{148}{\textbf {Comparison of scalable-MLMF and the histogram filter} A deterministic sweep policy was carried out for 100 different initialisations of the agent and object beliefs. \textit {Top left:} One particular Initialisation of the agent and object random variables. The true position of the agent and objects were sampled at random. The black arrow indicates the general policy which was followed for each of the 100 sweeps. These were performed for \textbf {1)} scalable-MLMF with objects considered to be independent at all times (no Algorithm \ref {alg:scalabe-mrf-slam}). \textbf {2)} Agent marginal $P(A_t|Y_{0:t},u_{1:t})$ is the product of marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref {eq:marg_indep_prod}. \textbf {3)} marginal $P(A_t|Y_{0:t},u_{1:t})$ is taken to be the average of all marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref {eq:marg_indep_sum}. For each of these three experiment we report the kernel density estimation over the Hellinger distances taken at every time step between ground truth (from histogram filter) and scalable-MLMF.\relax }{figure.caption.106}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Evaluation of memory}{148}{subsection.5.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces \textbf  {Agent's prior beliefs.} Two types of environment, the first is a 2D world where the agent lives in a square surrounded by a wall whilst the second is a 1D world. In the 2D figures the agent is illustrated by a circle with a bar to indicate its heading. The true location of the objects are represented by colour coded squares. \textit  {Top row} three different initialisations of the agent's location. \textit  {Bottom row} d) the agent's prior beliefs with respect to the location of the first object and e) belief of the second object's location. \textit  {bottom row} f) 1D world with one object.\relax }}{149}{figure.caption.107}}
\newlabel{fig:exploration_init}{{5.17}{149}{\textbf {Agent's prior beliefs.} Two types of environment, the first is a 2D world where the agent lives in a square surrounded by a wall whilst the second is a 1D world. In the 2D figures the agent is illustrated by a circle with a bar to indicate its heading. The true location of the objects are represented by colour coded squares. \textit {Top row} three different initialisations of the agent's location. \textit {Bottom row} d) the agent's prior beliefs with respect to the location of the first object and e) belief of the second object's location. \textit {bottom row} f) 1D world with one object.\relax }{figure.caption.107}{}}
\newlabel{eq:greedy_algorithm}{{5.5.2}{149}{Evaluation of memory}{equation.5.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces \textbf  {Memory size vs time to find object in 1D} Results of the effect of the memory size on the decision process for the 1D search illustrated in Figure \ref  {fig:exploration_init} \textit  {f)}. The memory size is reported as the percentage of total number of states present in the marginal space. At 100\% the size of the memory is equal to that of the state space, $N=100$ in this case. A total sweep of the entire state space would result in a total of 200 steps, the dotted grey line in the above figure. When no restrictions are placed on the memory size the policy following the greedy approach takes around 180 steps. This result converges when the number of parameters $|\Psi _{0:t}|$ of the memory likelihood function is greater than 50\% of the original state space. \relax }}{151}{figure.caption.108}}
\newlabel{fig:time_to_reach_goal_1D}{{5.18}{151}{\textbf {Memory size vs time to find object in 1D} Results of the effect of the memory size on the decision process for the 1D search illustrated in Figure \ref {fig:exploration_init} \textit {f)}. The memory size is reported as the percentage of total number of states present in the marginal space. At 100\% the size of the memory is equal to that of the state space, $N=100$ in this case. A total sweep of the entire state space would result in a total of 200 steps, the dotted grey line in the above figure. When no restrictions are placed on the memory size the policy following the greedy approach takes around 180 steps. This result converges when the number of parameters $|\Psi _{0:t}|$ of the memory likelihood function is greater than 50\% of the original state space. \relax }{figure.caption.108}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces \textbf  {Memory size vs time to find objects in 2D}. The initial beliefs correspond to those of Figure \ref  {fig:exploration_init}, a) for Gaussian (green line), b) 4 Gaussians (red line) and c) Uniform (blue line), both objects are initialised according to d) and e).\relax }}{151}{figure.caption.109}}
\newlabel{fig:time_to_reach_goal_2D}{{5.19}{151}{\textbf {Memory size vs time to find objects in 2D}. The initial beliefs correspond to those of Figure \ref {fig:exploration_init}, a) for Gaussian (green line), b) 4 Gaussians (red line) and c) Uniform (blue line), both objects are initialised according to d) and e).\relax }{figure.caption.109}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Conclusion}{152}{section.5.6}}
\newlabel{ch5:conclusion}{{5.6}{152}{Conclusion}{section.5.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and summary}{155}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Main Contributions}{155}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Limitations and Future Work}{156}{section.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Mixture Controller}{156}{section*.110}}
\citation{pomdp_toussain_iros_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces GMM policy drawback. \textit  {Top:} Two demonstrations, blue trajectories, follow a path which when fitted with a GMM (one Gaussian component is drawn in green) results in a vector which keeps the plug in contact with the edge of the wall. \textit  {Bottom:} A third demonstration, red line, did not result in a contact with the edge of the wall. The Gaussian of the GMM is shifted to the mean point of the data. The GMM policy no longer keeps the plug in contact with the edge. \relax }}{157}{figure.caption.111}}
\newlabel{fig:ch6:gmm_traj}{{6.1}{157}{GMM policy drawback. \textit {Top:} Two demonstrations, blue trajectories, follow a path which when fitted with a GMM (one Gaussian component is drawn in green) results in a vector which keeps the plug in contact with the edge of the wall. \textit {Bottom:} A third demonstration, red line, did not result in a contact with the edge of the wall. The Gaussian of the GMM is shifted to the mean point of the data. The GMM policy no longer keeps the plug in contact with the edge. \relax }{figure.caption.111}{}}
\citation{NIPS2002_2319}
\@writefile{toc}{\contentsline {subsubsection}{Belief compression}{158}{section*.112}}
\@writefile{toc}{\contentsline {subsubsection}{Policy representation}{158}{section*.113}}
\citation{Billard08chapter}
\@writefile{toc}{\contentsline {subsubsection}{MLMF}{159}{section*.114}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Final Words}{159}{section.6.3}}
\citation{RL_book_2010}
\@writefile{toc}{\contentsline {chapter}{Appendices}{161}{section*.115}}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {A}Peg in hole}{163}{Appendix.1.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Time to connect socket (ANOVA)}{163}{section.1.A.1}}
\newlabel{app:anova_socket}{{A.1}{163}{Time to connect socket (ANOVA)}{section.1.A.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces One way anova of the time taken to connect two sockets A and B. There is a significant difference.\relax }}{163}{table.caption.119}}
\newlabel{tab:ch4:anova_socket}{{A.1}{163}{One way anova of the time taken to connect two sockets A and B. There is a significant difference.\relax }{table.caption.119}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Time taken to find and connect the plug to the power socket. \textbf  {Top:} Time taken to connect the plug to the socket once the socket is localised. For most subjects the median value of the taken time is higher for socket A when compared with socket B. \textbf  {Bottom}: time taken to localise the socket. For the second search, AB and BA, it seams that the subjects are faster indicating learning during the experiment. \relax }}{164}{figure.caption.116}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces \relax }}{164}{figure.caption.117}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces \relax }}{165}{figure.caption.118}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}EM policy search}{165}{section.1.A.2}}
\newlabel{app:lb}{{A.2}{165}{EM policy search}{section.1.A.2}{}}
\newlabel{eq:ch4:app:J}{{A.2.1}{165}{EM policy search}{equation.1.A.2.1}{}}
\newlabel{eq:ch4:argmax}{{A.2.4}{166}{EM policy search}{equation.1.A.2.4}{}}
\newlabel{eq:lower_bound}{{A.2.5}{166}{EM policy search}{equation.1.A.2.5}{}}
\citation{rl_gradient_survey_2013}
\citation{matrix_ckb}
\citation{Bishop_2006}
\newlabel{eq:ch4:app:expec_Q}{{A.2.7}{167}{EM policy search}{equation.1.A.2.7}{}}
\newlabel{eq:ch4:app:logsum}{{A.2.8}{167}{EM policy search}{equation.1.A.2.8}{}}
\newlabel{eq:grad_log_cost_2}{{A.2.9}{167}{EM policy search}{equation.1.A.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Q-EM for GMM}{167}{section.1.A.3}}
\newlabel{app:grad}{{A.3}{167}{Q-EM for GMM}{section.1.A.3}{}}
\newlabel{eq:ch5:app:gmm}{{A.3.1}{167}{Q-EM for GMM}{equation.1.A.3.1}{}}
\newlabel{eq:ch4:app_div_Q}{{A.3.2}{167}{Q-EM for GMM}{equation.1.A.3.2}{}}
\newlabel{eq:ch4:app_div_Q_simple}{{A.3.3}{167}{Q-EM for GMM}{equation.1.A.3.3}{}}
\citation{Bishop_2006}
\newlabel{eq:ch4:app:mu_max}{{A.3.4}{168}{Q-EM for GMM}{equation.1.A.3.4}{}}
\newlabel{eq:ch4:app_new_mu}{{A.3.5}{168}{Q-EM for GMM}{equation.1.A.3.5}{}}
\newlabel{eq:ch4:app_new_cov}{{A.3.6}{168}{Q-EM for GMM}{equation.1.A.3.6}{}}
\newlabel{eq:ch4:app_new_pi}{{A.3.7}{168}{Q-EM for GMM}{equation.1.A.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Unbiased estimator}{168}{section.1.A.4}}
\newlabel{app:unbiased_delta}{{A.4}{168}{Unbiased estimator}{section.1.A.4}{}}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {B}Non-parametric Bayesian State Space Estimator}{171}{Appendix.1.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch5:appendix}{{B}{171}{Non-parametric Bayesian State Space Estimator}{Appendix.1.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Probabilities}{171}{section.1.B.1}}
\newlabel{eq:ch5:chain_rule}{{B.1.1}{171}{Probabilities}{equation.1.B.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Bayesian filtering recursion}{171}{section.1.B.2}}
\newlabel{appendix:bayes_recursion}{{B.2}{171}{Bayesian filtering recursion}{section.1.B.2}{}}
\newlabel{eq:ch5:app:joint}{{B.2.1}{171}{Bayesian filtering recursion}{equation.1.B.2.1}{}}
\newlabel{eq:ch5:app:joint_2}{{B.2.2}{171}{Bayesian filtering recursion}{equation.1.B.2.2}{}}
\newlabel{eq:ch5:joint_final}{{B.2.3}{171}{Bayesian filtering recursion}{equation.1.B.2.3}{}}
\newlabel{eq:ch5:chain_example}{{B.2.4}{172}{Bayesian filtering recursion}{equation.1.B.2.4}{}}
\newlabel{eq:ch5:cancel_actions}{{B.2.5}{172}{Bayesian filtering recursion}{equation.1.B.2.5}{}}
\newlabel{eq:br_chain}{{B.2.12}{173}{Bayesian filtering recursion}{equation.1.B.2.12}{}}
\newlabel{eq:ch5_measurement_update}{{B.2.16}{173}{Bayesian filtering recursion}{equation.1.B.2.16}{}}
\newlabel{eq:ch5:app:bayes_filter}{{B.2.17}{173}{Bayesian filtering recursion}{equation.1.B.2.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Recursion example}{174}{section.1.B.3}}
\newlabel{appendix:recursion_example}{{B.3}{174}{Recursion example}{section.1.B.3}{}}
\newlabel{eq:ch5:rec_ex1}{{B.3.4}{174}{Recursion example}{equation.1.B.3.4}{}}
\newlabel{eq:ch5:rec_ex2}{{B.3.5}{174}{Recursion example}{equation.1.B.3.5}{}}
\newlabel{eq:ch5:rec_ex3}{{B.3.9}{174}{Recursion example}{equation.1.B.3.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Derivation of the evidence}{175}{section.1.B.4}}
\newlabel{appendix:evidence}{{B.4}{175}{Derivation of the evidence}{section.1.B.4}{}}
\newlabel{eq:ch5:numerator}{{B.4.1}{175}{Derivation of the evidence}{equation.1.B.4.1}{}}
\newlabel{eq:ch5:gradient_alpha}{{B.4.5}{176}{Derivation of the evidence}{equation.1.B.4.5}{}}
\newlabel{eq:ch5:evidence_yo}{{B.4.8}{176}{Derivation of the evidence}{equation.1.B.4.8}{}}
\newlabel{eq:ch5:evidence_y}{{B.4.9}{176}{Derivation of the evidence}{equation.1.B.4.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Derivation of the marginal}{176}{section.1.B.5}}
\newlabel{appendix:marginal}{{B.5}{176}{Derivation of the marginal}{section.1.B.5}{}}
\newlabel{eq:ch5:app:dummy}{{B.5.1}{177}{Derivation of the marginal}{equation.1.B.5.1}{}}
\newlabel{eq:dependence1}{{B.5.2}{177}{Derivation of the marginal}{equation.1.B.5.2}{}}
\newlabel{eq:dependence2}{{B.5.3}{177}{Derivation of the marginal}{equation.1.B.5.3}{}}
\bibstyle{plainnat}
\bibdata{bib/MLMF.bib,bib/peg_hole.bib,bib/RL.bib,bib/pomdp.bib,bib/citations.bib,bib/DT.bib,bib/spatial_navigation.bib,bib/ch3-citations.bib}
\bibcite{sis_pomdp_2002}{{1}{2002}{{Aberdeen and Baxter}}{{}}}
\bibcite{sol_pdg_pbd_2014}{{2}{2014}{{Abu-Dakka et~al.}}{{Abu-Dakka, Nemec, Kramberger, Buch, Kr{\"u}ger, and Ude}}}
\bibcite{rl_gmm_2010}{{3}{2010}{{Agostini and Celaya}}{{}}}
\bibcite{FIRM_2011}{{4}{2011}{{akbar Agha-mohammadi et~al.}}{{akbar Agha-mohammadi, Chakravorty, and Amato}}}
\bibcite{rob_online_bs_icra_2014}{{5}{2014}{{akbar Agha-mohammadi et~al.}}{{akbar Agha-mohammadi, Agarwal, Mahadevan, Chakravorty, Tomkins, Denny, and Amato}}}
\bibcite{Arul_Mask_Clap_2002}{{6}{2002{a}}{{Arulampalam et~al.}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\bibcite{PF_tutorial_2002}{{7}{2002{b}}{{Arulampalam et~al.}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\bibcite{Atkeson97locallyweighted}{{8}{1997}{{Atkeson et~al.}}{{Atkeson, Moore, and Schaal}}}
\bibcite{Bake_Tene_Saxe_2006}{{9}{2006}{{Bake et~al.}}{{Bake, Tenenbaum, and Saxe}}}
\bibcite{Bake_Saxe_Tene_2011}{{10}{2011}{{Bake et~al.}}{{Bake, Tenenbaum, and Saxe}}}
\bibcite{barberBRML2012}{{11}{2012}{{Barber}}{{}}}
\bibcite{Baron-Cohen}{{12}{1995}{{Baron-Cohen}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{179}{chapter*.121}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{gpomdp_2000}{{13}{2000}{{Baxter and Bartlett}}{{}}}
\bibcite{peg_imcssd_2015}{{14}{2015}{{Bdiwi et~al.}}{{Bdiwi, Winkler, Jokesch, and Suchy}}}
\bibcite{Bergman99recursivebayesian}{{15}{1999}{{Bergman and Bergman}}{{}}}
\bibcite{Bernoulli1954}{{16}{1954}{{Bernoulli}}{{}}}
\bibcite{Billard_schol_2013}{{17}{2013}{{Billard and Grollman}}{{}}}
\bibcite{Billard08chapter}{{18}{2008}{{Billard et~al.}}{{Billard, Calinon, Dillmann, and Schaal}}}
\bibcite{Bishop_2006}{{19}{2006}{{Bishop}}{{}}}
\bibcite{fvi_uav_2010}{{20}{2010}{{Bou-Ammar et~al.}}{{Bou-Ammar, Voos, and Ertel}}}
\bibcite{Safe_val_function_1995}{{21}{1995}{{Boyan and Moore}}{{}}}
\bibcite{solving_continous_pomdps_2013}{{22}{2013}{{Brechtel et~al.}}{{Brechtel, Gindele, and Dillmann}}}
\bibcite{mc_update_ppomdps}{{23}{2011}{{Brooks and Williams}}{{}}}
\bibcite{spatial_memory_how_ego_allo_combine_2006}{{24}{2006}{{Burgess}}{{}}}
\bibcite{approx_rl_overview_2011}{{25}{2011}{{Busoniu et~al.}}{{Busoniu, Ernst, Schutter, and Babuska}}}
\bibcite{MRF_ToM}{{26}{2009}{{Butterfield et~al.}}{{Butterfield, Jenkins, Sobel, and Schwertfeger}}}
\bibcite{gesture_calinon_2010}{{27}{2010}{{Calinon et~al.}}{{Calinon, D'halluin, Sauser, Caldwell, and Billard}}}
\bibcite{KL_SLAM_exploration_PF}{{28}{2010}{{Carlone et~al.}}{{Carlone, Du, Ng, Bona, and Indri}}}
\bibcite{Active_SLAM_Uncertainty_compar}{{29}{2012}{{Carrillo et~al.}}{{Carrillo, Reid, and Castellanos}}}
\bibcite{acting_uncer_1996}{{30}{1996}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{Chambrier2014}{{31}{2014}{{Chambrier and Billard}}{{}}}
\bibcite{u_aware_grasp_ICRA_2015}{{32}{2015}{{Chen and von Wichert}}{{}}}
\bibcite{online_gpr_icra_2014}{{33}{2014}{{Cheng and Chen}}{{}}}
\bibcite{search_strategies_icra_2001}{{34}{2001}{{Chhatpar and Branicky}}{{}}}
\bibcite{deChambrier2013}{{35}{2013}{{de~Chambrier and Billard}}{{}}}
\bibcite{p_search_surv_2011}{{36}{2011}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{rl_gradient_survey_2013}{{37}{2013}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{ToM_HRI_2106}{{38}{2016}{{Devin and Alami}}{{}}}
\bibcite{POMDP_approach_2010}{{39}{2010}{{Du et~al.}}{{Du, Hsu, Kurniawati, Lee, Ong, and Png}}}
\bibcite{SLAM_part1}{{40}{2006}{{Durrant-Whyte and Bailey}}{{}}}
\bibcite{Erez10ascalable}{{41}{2010}{{Erez and Smart}}{{}}}
\bibcite{EGW05}{{42}{2005{a}}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{Tree_batch_2005}{{43}{2005{b}}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{hybrid_1992}{{44}{1992}{{Fisher and Mujtaba}}{{}}}
\bibcite{Navigation_strategires_for_exploring_indoor_environments}{{45}{2002}{{Gonz{\'{a}}lez{-}Ba{\~{n}}os and Latombe}}{{}}}
\bibcite{stable_FA_gordon_1995}{{46}{1995}{{Gordon}}{{}}}
\bibcite{TutGraphSLAM}{{47}{2010}{{Grisetti et~al.}}{{Grisetti, Kummerle, Stachniss, and Burgard}}}
\bibcite{ac_survey_2012}{{48}{2012{a}}{{Grondman et~al.}}{{Grondman, Busoniu, Lopes, and Babuska}}}
\bibcite{rl_ac_surv_2012}{{49}{2012{b}}{{Grondman et~al.}}{{Grondman, Busoniu, Lopes, and Babuska}}}
\bibcite{learn_admittance_icra_1994}{{50}{1994}{{Gullapalli et~al.}}{{Gullapalli, Barto, and Grupen}}}
\bibcite{Hamner_2006_5810}{{51}{2006}{{Hamner et~al.}}{{Hamner, Singh, and Scherer}}}
\bibcite{Hauser_2011}{{52}{2011}{{Hauser}}{{}}}
\bibcite{DRQ_AAAI_2015}{{53}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{Quadrator_2008}{{54}{2008}{{He et~al.}}{{He, Prentice, and Roy}}}
\bibcite{next_best_touch}{{55}{2013}{{Hebert et~al.}}{{Hebert, Howard, Hudson, Ma, and Burdick}}}
\bibcite{negative_info_markov_localisation}{{56}{2005}{{Hoffman et~al.}}{{Hoffman, Spranger, Gohring, and Jungel}}}
\bibcite{NegInfoFurtherStudies}{{57}{2006}{{Hoffmann et~al.}}{{Hoffmann, Spranger, Gohring, Jungel, and Burkhard}}}
\bibcite{un_water_inspection_icra_2012}{{58}{2012}{{Hollinger et~al.}}{{Hollinger, Englot, Hover, Mitra, and Sukhatme}}}
\bibcite{Hsiao_RSS_10}{{59}{2010}{{Hsiao et~al.}}{{Hsiao, Kaelbling, and Lozano-Perez}}}
\bibcite{RRT-SLAM}{{60}{2008}{{Huang and Gupta}}{{}}}
\bibcite{DiffEntropyHuber2008}{{61}{2008}{{Huber et~al.}}{{Huber, Bailey, Durrant-Whyte, and Hanebeck}}}
\bibcite{Iachini2014}{{62}{2014}{{Iachini et~al.}}{{Iachini, Ruggiero, and Ruotolo}}}
\bibcite{Efficient_touch_2012}{{63}{2012}{{Javdani et~al.}}{{Javdani, Klingensmith, Bagnell, Pollard, and Srinivasa}}}
\bibcite{DARPA_2015}{{64}{2015}{{Johnson and et. al}}{{}}}
\bibcite{Kaelbling_1998}{{65}{1998}{{Kaelbling et~al.}}{{Kaelbling, Littman, and Cassandra}}}
\bibcite{learn_force_c_icirs_2011}{{66}{2011}{{Kalakrishnan et~al.}}{{Kalakrishnan, Righetti, Pastor, and Schaal}}}
\bibcite{Kasper2001153}{{67}{2001}{{Kasper et~al.}}{{Kasper, Fricke, Steuernagel, and von Puttkamer}}}
\bibcite{PRM_1996}{{68}{1996}{{Kavraki et~al.}}{{Kavraki, Svestka, Latombe, and Overmars}}}
\bibcite{heli_2004}{{69}{2004}{{Kim et~al.}}{{Kim, Jordan, Sastry, and Ng}}}
\bibcite{PoWER_2009}{{70}{2009}{{Kober and Peters}}{{}}}
\bibcite{RL_robots_surv_2013}{{71}{2013}{{Kober et~al.}}{{Kober, Bagnell, and Peters}}}
\bibcite{Kollar_2008_Exploration_SLAM}{{72}{2008}{{Kollar and Roy}}{{}}}
\bibcite{compliant_manip_icra_2008}{{73}{2008}{{kook Yun}}{{}}}
\bibcite{pancake_2010}{{74}{2010{a}}{{Kormushev et~al.}}{{Kormushev, Calinon, and Caldwell}}}
\bibcite{archery_2010}{{75}{2010{b}}{{Kormushev et~al.}}{{Kormushev, Calinon, Saegusa, and Metta}}}
\bibcite{Kronander2015}{{76}{2015}{{Kronander}}{{}}}
\bibcite{Klas_icra_2012}{{77}{2012}{{Kronander and Billard}}{{}}}
\bibcite{SARSOP}{{78}{2008}{{Kurniawati et~al.}}{{Kurniawati, Hsu, and Lee}}}
\bibcite{Lange_riedmiller_2010}{{79}{2010}{{Lange and Riedmiller}}{{}}}
\bibcite{human_stsm_2015}{{80}{2015}{{Lavenexa et~al.}}{{Lavenexa, Boujonb, Ndarugendamwob, and Lavenexa}}}
\bibcite{Leslie_TOMM}{{81}{1994}{{Leslie}}{{}}}
\bibcite{Li_2015}{{82}{2016}{{Li et~al.}}{{Li, Hang, Kragic, and Billard}}}
\bibcite{bs_compression_2010}{{83}{2010}{{Li et~al.}}{{Li, Cheung, and Liu}}}
\bibcite{GeorgiosLidoris}{{84}{2011}{{Lidoris}}{{}}}
\bibcite{qmdp_ijcnn_2014}{{85}{2014}{{Lin et~al.}}{{Lin, Lu, and Makedon}}}
\bibcite{Littman95}{{86}{1995}{{Littman et~al.}}{{Littman, Cassandra, and Kaelbling}}}
\bibcite{MedinaSH13}{{87}{2013}{{Medina et~al.}}{{Medina, Sieber, and Hirche}}}
\bibcite{peg_personal_icra_2010}{{88}{2010}{{Meeussen et~al.}}{{Meeussen, Wise, Glaser, Chitta, McGann, Mihelich, Marder-Eppstein, Muja, Eruhimov, Foote, Hsu, Rusu, Marthi, Bradski, Konolige, Gerkey, and Berger}}}
\bibcite{cogprints730}{{89}{1956}{{Miller}}{{}}}
\bibcite{mnih-dqn-2015}{{90}{2015}{{Mnih}}{{}}}
\bibcite{DataAssociation2003}{{91}{2003}{{Montemerlo and Thrun}}{{}}}
\bibcite{FastSLAM}{{92}{2003}{{Montemerlo et~al.}}{{Montemerlo, Thrun, Koller, and Wegbreit}}}
\bibcite{trans_workpiece_icra_2013}{{93}{2013}{{Nemec et~al.}}{{Nemec, Abu-Dakka, Ridge, Ude, Jorgensen, Savarimuthu, Jouffroy, Petersen, and Kr\"uger}}}
\bibcite{fqi_nips_peter_2009}{{94}{2009{a}}{{Neumann and Peters}}{{}}}
\bibcite{NIPS2008_3501}{{95}{2009{b}}{{Neumann and Peters}}{{}}}
\bibcite{Pegasus_2000}{{96}{2000}{{Ng and Jordan}}{{}}}
\bibcite{RL_book_sa}{{97}{2012}{{Now\'{e} et~al.}}{{Now\'{e}, Vrancx, and De~Hauwere}}}
\bibcite{where_look_2012}{{98}{2012}{{Nunez-Varela et~al.}}{{Nunez-Varela, Ravindran, and Wyatt}}}
\bibcite{kernel_rl_ormoneit_2002}{{99}{2002}{{Ormoneit and Glynn}}{{}}}
\bibcite{intuitive_peg_isr_2013}{{100}{2013}{{Park et~al.}}{{Park, Bae, Park, Baeg, and Park}}}
\bibcite{Pasqualotto2013175}{{101}{2013}{{Pasqualotto et~al.}}{{Pasqualotto, Spiller, Jansari, and Proulx}}}
\bibcite{online_pre_sensor_2011}{{102}{2011}{{Pastor et~al.}}{{Pastor, Righetti, Kalakrishnan, and Schaal}}}
\bibcite{peter_nac_2008}{{103}{2008{a}}{{Peters and Schaal}}{{}}}
\bibcite{NAC_2008}{{104}{2008{b}}{{Peters and Schaal}}{{}}}
\bibcite{matrix_ckb}{{105}{2012}{{Petersen and Pedersen}}{{}}}
\bibcite{PBVI}{{106}{2003}{{Pineau et~al.}}{{Pineau, Gordon, and Thrun}}}
\bibcite{Plagemann07gaussianbeam}{{107}{2007}{{Plagemann et~al.}}{{Plagemann, Kersting, Pfaff, and Burgard}}}
\bibcite{non_gauss_bel_plan_2012}{{108}{2012}{{Platt et~al.}}{{Platt, Kaelbling, Lozano-Perez, and Tedrake}}}
\bibcite{bsp_rss_2010a}{{109}{2010}{{Platt et~al.}}{{Platt, Tedrake, Kaelbling, and Lozano-P\'{e}rez}}}
\bibcite{PBVI_C_2006}{{110}{2006}{{Porta et~al.}}{{Porta, Vlassis, Spaan, and Poupart}}}
\bibcite{BelRoadMap_2009}{{111}{2009}{{Prentice and Roy}}{{}}}
\bibcite{decision_un_2013}{{112}{2013}{{Preuschoff et~al.}}{{Preuschoff, Mohr, and Hsu}}}
\bibcite{rai2013learning}{{113}{2013}{{Rai et~al.}}{{Rai, De~Chambrier, and Billard}}}
\bibcite{Sondik_1973}{{114}{1973}{{Richard D.~Smallwood}}{{}}}
\bibcite{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}{{115}{2012}{{Richardson et~al.}}{{Richardson, Bake, Tenenbaum, and Saxe}}}
\bibcite{Riedmiller2005}{{116}{2005}{{Riedmiller}}{{}}}
\bibcite{Ross08onlineplanning}{{117}{2008}{{Ross et~al.}}{{Ross, Pineau, Paquet, and Chaib-draa}}}
\bibcite{CostalNavigation1999}{{118}{1999}{{Roy et~al.}}{{Roy, Burgard, Fox, and Thrun}}}
\bibcite{belief_compression_2005}{{119}{2005}{{Roy}}{{}}}
\bibcite{EPCA_2003}{{120}{}{{Roy and Gordon}}{{}}}
\bibcite{NIPS2002_2319}{{121}{2003}{{Roy and Gordon}}{{}}}
\bibcite{Roy99coastalnavigation}{{122}{1999}{{Roy and Thrun}}{{}}}
\bibcite{ToM_humanoid}{{123}{2002}{{Scassellati}}{{}}}
\bibcite{Schaal04learningmovement}{{124}{2004}{{Schaal et~al.}}{{Schaal, Peters, Nakanishi, and Ijspeert}}}
\bibcite{BayesBall}{{125}{1998}{{Shachter}}{{}}}
\bibcite{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}{{126}{2010}{{Silver et~al.}}{{Silver, Bagnell, and Stentz}}}
\bibcite{HSV}{{127}{2004}{{Smith and Simmons}}{{}}}
\bibcite{HSVI2}{{128}{2012}{{Smith and Simmons}}{{}}}
\bibcite{Towards_a_ToM_2010}{{129}{2010}{{Sodian and Kristen}}{{}}}
\bibcite{Spaan05icra}{{130}{2005}{{Spaan and Vlassis}}{{}}}
\bibcite{stachniss05robotics}{{131}{2005}{{Stachniss et~al.}}{{Stachniss, Grisetti, and Burgard}}}
\bibcite{stankiewicz2006lost}{{132}{2006}{{Stankiewicz et~al.}}{{Stankiewicz, Legge, Mansfield, and Schlicht}}}
\bibcite{dmp_iros_2011}{{133}{2011}{{Stulp et~al.}}{{Stulp, Theodorou, Kalakrishnan, Pastor, Righetti, and Schaal}}}
\bibcite{dmp_seq_2012}{{134}{2012}{{Stulp et~al.}}{{Stulp, Theodorou, and Schaal}}}
\bibcite{Needle_2014}{{135}{2014}{{Sun and Alterovitz}}{{}}}
\bibcite{gmr_2004}{{136}{2004}{{Sung}}{{}}}
\bibcite{sutton98a}{{137}{1998{a}}{{Sutton and Barto}}{{}}}
\bibcite{Sutton00policygradient}{{138}{}{{Sutton et~al.}}{{Sutton, Mcallester, Singh, and Mansour}}}
\bibcite{sutton1998reinforcement}{{139}{1998{b}}{{Sutton and Barto}}{{}}}
\bibcite{RL_book_2010}{{140}{2010}{{Szepesv\'ari}}{{}}}
\bibcite{MC-POMDP}{{141}{2000}{{Thrun}}{{}}}
\bibcite{Thrun02particlefilters}{{142}{2002}{{Thrun}}{{}}}
\bibcite{Thrun_grid_based_1996}{{143}{1996}{{Thrun and B\"{u}}}{{}}}
\bibcite{SLAM_HBR}{{144}{2008}{{Thrun and Leonard}}{{}}}
\bibcite{Thrun_2005}{{145}{2005{a}}{{Thrun et~al.}}{{Thrun, Burgard, and Fox}}}
\bibcite{Thrun_Burgard_Fox_2005}{{146}{2005{b}}{{Thrun et~al.}}{{Thrun, Burgard, and Fox}}}
\bibcite{ActivePosSLAM}{{147}{2012}{{Valencia et~al.}}{{Valencia, Miro, Dissanayake, and Andrade-Cetto}}}
\bibcite{dense_entropy_icra_2014}{{148}{2014}{{Vallve and Andrade{-}Cetto}}{{}}}
\bibcite{LQG_MP_2011}{{149}{2011}{{Van Den~Berg et~al.}}{{Van Den~Berg, Abbeel, and Goldberg}}}
\bibcite{van_den_Berg_2012}{{150}{2012}{{van~den Berg et~al.}}{{van~den Berg, Patil, and Alterovitz}}}
\bibcite{FSVI}{{151}{2007}{{Veloso}}{{}}}
\bibcite{pomdp_iros_tous_2015}{{152}{2015{a}}{{Vien and Toussaint}}{{}}}
\bibcite{pomdp_toussain_iros_2015}{{153}{2015{b}}{{Vien and Toussaint}}{{}}}
\bibcite{eNAC_2003}{{154}{2003}{{Vijayakumar et~al.}}{{Vijayakumar, Shibata, and Schaal}}}
\bibcite{VonNeumann1944}{{155}{1990}{{Von~Neumann and Morgenstern}}{{}}}
\bibcite{Wang2016}{{156}{2016}{{Wang et~al.}}{{Wang, Uchibe, and Doya}}}
\bibcite{Wang_2007}{{157}{2007}{{Wang}}{{}}}
\bibcite{updating_egocentric_human_navigation_2000}{{158}{2000}{{Wang and Spelke}}{{}}}
\bibcite{RL_state_art_2012}{{159}{2012}{{Wiering and van Otterio}}{{}}}
\bibcite{reinforce_1992}{{160}{1992}{{Williams}}{{}}}
\bibcite{Biomechanics_2009}{{161}{2009}{{Winter}}{{}}}
\bibcite{what_det_our_nav_ability_2010}{{162}{2010}{{Wolbers and Hegarty}}{{}}}
\bibcite{spatial_updating_2008}{{163}{2008}{{Wolbers et~al.}}{{Wolbers, Hegarty, Buchel, and Loomis}}}
\bibcite{fast_peg_pbd_icmc_2014}{{164}{2014}{{Yang et~al.}}{{Yang, Lin, Song, Nemec, Ude, Buch, Kruger, and Savarimuthu}}}
\bibcite{ACML_variance_2015}{{165}{2015}{{Zhao et~al.}}{{Zhao, Niu, Xie, Yang, and Sugiyama}}}
\bibcite{seq_traj_replan_iros_2013}{{166}{2013}{{Zito et~al.}}{{Zito, Kopicki, Stolkin, Borst, Schmidt, Roa, and Wyatt}}}

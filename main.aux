\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{decision_un_2013}
\citation{Bernoulli1954}
\citation{VonNeumann1944}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}}
\@writefile{brf}{\backcite{decision_un_2013}{{1}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{Bernoulli1954}{{1}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{VonNeumann1944}{{1}{1.1}{section.1.1}}}
\citation{ActingUncertainty_1996}
\citation{stankiewicz2006lost}
\citation{Billard08chapter}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of the decision making under uncertainty in both robotics and everyday life situations. Images taken from the public domain.\relax }}{2}{figure.caption.1}}
\@writefile{brf}{\backcite{ActingUncertainty_1996}{{2}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{2}{1.1}{figure.caption.1}}}
\@writefile{brf}{\backcite{Billard08chapter}{{2}{1.1}{figure.caption.1}}}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contribution}{3}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Learning to reason with uncertainty as humans}{3}{subsection.1.2.1}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{3}{1.2.1}{subsection.1.2.1}}}
\citation{rai2013learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Non-parametric Bayesian state space filter}{4}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Reinforcement learning in belief space}{4}{subsection.1.2.3}}
\@writefile{brf}{\backcite{rai2013learning}{{4}{1.2.3}{subsection.1.2.3}}}
\citation{Chambrier2014}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Thesis outline}{5}{section.1.3}}
\@writefile{brf}{\backcite{Chambrier2014}{{5}{1.3}{section.1.3}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Chapter outline.\relax }}{7}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ch2_outline}{{2.1}{7}{Chapter outline.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Decisions under uncertainty}{8}{section.2.1}}
\newlabel{sec:deci_un}{{2.1}{8}{Decisions under uncertainty}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Relation between beliefs, desires and actions and are all considered to be rational.\relax }}{9}{figure.caption.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Decision theory}{9}{subsection.2.1.1}}
\newlabel{sec:ch2_DT}{{2.1.1}{9}{Decision theory}{subsection.2.1.1}{}}
\citation{Bernoulli1954}
\citation{VonNeumann1944}
\@writefile{brf}{\backcite{Bernoulli1954}{{10}{2.1.1}{figure.caption.3}}}
\newlabel{eq:exp_utility}{{2.1.1}{10}{Decision theory}{figure.caption.3}{}}
\@writefile{brf}{\backcite{VonNeumann1944}{{10}{2.1.1}{figure.caption.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sequential decision making}{11}{section.2.2}}
\newlabel{sec:sqp}{{2.2}{11}{Sequential decision making}{section.2.2}{}}
\newlabel{eq:joint_state_actions_util}{{2.1}{11}{Sequential decision making}{equation.2.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Definition of common variables used.\relax }}{12}{table.caption.4}}
\newlabel{tab:notation}{{2.1}{12}{Definition of common variables used.\relax }{table.caption.4}{}}
\newlabel{fig:mdp_off}{{2.3(a)}{13}{Subfigure 2 2.3(a)}{subfigure.2.3.1}{}}
\newlabel{sub@fig:mdp_off}{{(a)}{13}{Subfigure 2 2.3(a)\relax }{subfigure.2.3.1}{}}
\newlabel{fig:mdp_on}{{2.3(b)}{13}{Subfigure 2 2.3(b)}{subfigure.2.3.2}{}}
\newlabel{sub@fig:mdp_on}{{(b)}{13}{Subfigure 2 2.3(b)\relax }{subfigure.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf  {(a)} the decision nodes are not considered random variables whilst in \textbf  {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }}{13}{figure.caption.5}}
\newlabel{fig:mdp}{{2.3}{13}{Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf {(a)} the decision nodes are not considered random variables whilst in \textbf {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {off-policy}}}{13}{figure.caption.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {on-policy}}}{13}{figure.caption.5}}
\newlabel{eq:joint_state_actions}{{2.2}{14}{Sequential decision making}{equation.2.2.2}{}}
\newlabel{eq:temporal_expected_utility}{{2.4}{14}{Sequential decision making}{equation.2.2.4}{}}
\newlabel{eq:expansion}{{2.5}{14}{Sequential decision making}{equation.2.2.5}{}}
\newlabel{eq:bellman}{{2.6}{14}{Sequential decision making}{equation.2.2.6}{}}
\newlabel{eq:on_policy_bellman}{{2.7}{15}{Sequential decision making}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}POMDP}{15}{subsection.2.2.1}}
\newlabel{eq:sensor}{{2.8}{16}{POMDP}{equation.2.2.8}{}}
\newlabel{eq:likelihood}{{2.9}{16}{POMDP}{equation.2.2.9}{}}
\newlabel{fig:motion_update}{{2.4(a)}{17}{Subfigure 2 2.4(a)}{subfigure.2.4.1}{}}
\newlabel{sub@fig:motion_update}{{(a)}{17}{Subfigure 2 2.4(a)\relax }{subfigure.2.4.1}{}}
\newlabel{fig:measurement}{{2.4(b)}{17}{Subfigure 2 2.4(b)}{subfigure.2.4.2}{}}
\newlabel{sub@fig:measurement}{{(b)}{17}{Subfigure 2 2.4(b)\relax }{subfigure.2.4.2}{}}
\newlabel{fig:likelihood}{{2.4(c)}{17}{Subfigure 2 2.4(c)}{subfigure.2.4.3}{}}
\newlabel{sub@fig:likelihood}{{(c)}{17}{Subfigure 2 2.4(c)\relax }{subfigure.2.4.3}{}}
\newlabel{fig:measurement_update}{{2.4(d)}{17}{Subfigure 2 2.4(d)}{subfigure.2.4.4}{}}
\newlabel{sub@fig:measurement_update}{{(d)}{17}{Subfigure 2 2.4(d)\relax }{subfigure.2.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \textbf  {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf  {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf  {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf  {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }}{17}{figure.caption.6}}
\newlabel{fig:belief_update_example}{{2.4}{17}{\textbf {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{17}{figure.caption.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{17}{figure.caption.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{17}{figure.caption.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{17}{figure.caption.6}}
\newlabel{eq:motion_update}{{2.10}{18}{POMDP}{equation.2.2.10}{}}
\newlabel{eq:measurement_update}{{2.11}{18}{POMDP}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Bayesian state space filter.\relax }}{18}{figure.caption.7}}
\newlabel{fig:sub_pomdp}{{2.6(a)}{19}{Subfigure 2 2.6(a)}{subfigure.2.6.1}{}}
\newlabel{sub@fig:sub_pomdp}{{(a)}{19}{Subfigure 2 2.6(a)\relax }{subfigure.2.6.1}{}}
\newlabel{fig:sub_bmdp}{{2.6(b)}{19}{Subfigure 2 2.6(b)}{subfigure.2.6.2}{}}
\newlabel{sub@fig:sub_bmdp}{{(b)}{19}{Subfigure 2 2.6(b)\relax }{subfigure.2.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \textbf  {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf  {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }}{19}{figure.caption.8}}
\newlabel{fig:pomdp}{{2.6}{19}{\textbf {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{19}{figure.caption.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{19}{figure.caption.8}}
\newlabel{eq:belief_bellman}{{2.15}{19}{POMDP}{equation.2.2.15}{}}
\citation{Sondik_1973}
\citation{Thrun_2005}
\citation{Kaelbling_1998}
\newlabel{eq:belief_state_transformation}{{2.16}{20}{POMDP}{equation.2.2.16}{}}
\newlabel{eq:max_component}{{2.17}{20}{POMDP}{equation.2.2.17}{}}
\newlabel{eq:final_belief_bellman}{{2.18}{20}{POMDP}{equation.2.2.18}{}}
\@writefile{brf}{\backcite{Sondik_1973}{{20}{2.2.1}{equation.2.2.18}}}
\@writefile{brf}{\backcite{Thrun_2005}{{20}{2.2.1}{equation.2.2.18}}}
\@writefile{brf}{\backcite{Kaelbling_1998}{{20}{2.2.1}{equation.2.2.18}}}
\citation{POMDP_approach_2010}
\citation{Thrun_2005}
\citation{PBVI}
\citation{HSV}
\citation{HSVI2}
\citation{FSVI}
\citation{SARSOP}
\citation{POMDP_approach_2010}
\@writefile{brf}{\backcite{POMDP_approach_2010}{{21}{2.2.1}{equation.2.2.18}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Literature review}{21}{section.2.3}}
\newlabel{sec:lit_rev}{{2.3}{21}{Literature review}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Value Iteration}{21}{subsection.2.3.1}}
\newlabel{lit:VI}{{2.3.1}{21}{Value Iteration}{subsection.2.3.1}{}}
\@writefile{brf}{\backcite{Thrun_2005}{{21}{2.3.1}{subsection.2.3.1}}}
\@writefile{toc}{\contentsline {subsubsection}{Point-base Value Iteration}{21}{section*.10}}
\@writefile{brf}{\backcite{PBVI}{{21}{2.3.1}{section*.10}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Mind-map of AI and robotic methods for acting under uncertainty.\relax }}{22}{figure.caption.9}}
\newlabel{fig:mindmap}{{2.7}{22}{Mind-map of AI and robotic methods for acting under uncertainty.\relax }{figure.caption.9}{}}
\citation{Spaan05icra}
\citation{PBVI_C_2006}
\citation{solving_continous_pomdps_2013}
\@writefile{brf}{\backcite{HSV}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{HSVI2}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{FSVI}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{SARSOP}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{POMDP_approach_2010}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{Spaan05icra}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{PBVI_C_2006}{{23}{2.3.1}{section*.10}}}
\citation{Ross08onlineplanning}
\citation{MC-POMDP}
\citation{Tree_batch_2005}
\@writefile{brf}{\backcite{solving_continous_pomdps_2013}{{24}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{Ross08onlineplanning}{{24}{2.3.1}{section*.10}}}
\@writefile{toc}{\contentsline {subsubsection}{Approximate Value Iteration}{24}{section*.11}}
\@writefile{brf}{\backcite{MC-POMDP}{{24}{2.3.1}{section*.11}}}
\citation{mc_update_ppomdps}
\citation{neura_fqi_2005}
\citation{DRQ_AAAI_2015}
\citation{mnih-dqn-2015}
\citation{Roy99coastalnavigation}
\@writefile{brf}{\backcite{Tree_batch_2005}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{mc_update_ppomdps}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{neura_fqi_2005}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{DRQ_AAAI_2015}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{mnih-dqn-2015}{{25}{2.3.1}{section*.11}}}
\@writefile{toc}{\contentsline {subsubsection}{Latent Value Iteration}{25}{section*.12}}
\citation{belief_compression_2005}
\citation{EPCA_2003}
\citation{bs_compression_2010}
\@writefile{brf}{\backcite{Roy99coastalnavigation}{{26}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{belief_compression_2005}{{26}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{EPCA_2003}{{26}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{bs_compression_2010}{{26}{2.3.1}{section*.12}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: Value Iteration}{26}{section*.13}}
\citation{gpomdp_2000}
\citation{reinforce_1992}
\citation{gpomdp_2000}
\citation{sis_pomdp_2002}
\citation{Pegasus_2000}
\citation{heli_2004}
\citation{dmp_iros_2011}
\citation{dmp_seq_2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Policy search}{27}{subsection.2.3.2}}
\newlabel{lit:policy_search}{{2.3.2}{27}{Policy search}{subsection.2.3.2}{}}
\@writefile{brf}{\backcite{gpomdp_2000}{{27}{2.3.2}{subsection.2.3.2}}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient: policy search}{27}{section*.14}}
\@writefile{brf}{\backcite{reinforce_1992}{{27}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{gpomdp_2000}{{27}{2.3.2}{section*.14}}}
\citation{PoWER_2009}
\citation{archery_2010}
\citation{pancake_2010}
\citation{Wang2016}
\citation{p_search_surv_2011}
\citation{RL_robots_surv_2013}
\citation{ac_survey_2012}
\citation{eNAC_2003}
\citation{NAC_2008}
\@writefile{brf}{\backcite{sis_pomdp_2002}{{28}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{Pegasus_2000}{{28}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{heli_2004}{{28}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{dmp_iros_2011}{{28}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{dmp_seq_2012}{{28}{2.3.2}{section*.14}}}
\@writefile{toc}{\contentsline {subsubsection}{Expectation-Maximisation: policy search}{28}{section*.15}}
\@writefile{brf}{\backcite{PoWER_2009}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{archery_2010}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{pancake_2010}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{Wang2016}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{p_search_surv_2011}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{RL_robots_surv_2013}{{28}{2.3.2}{section*.15}}}
\@writefile{toc}{\contentsline {subsubsection}{Actor-critic: policy search}{28}{section*.16}}
\@writefile{brf}{\backcite{ac_survey_2012}{{29}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{eNAC_2003}{{29}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{NAC_2008}{{29}{2.3.2}{section*.16}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: policy search}{29}{section*.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Planning}{29}{subsection.2.3.3}}
\newlabel{lit:Planning}{{2.3.3}{29}{Planning}{subsection.2.3.3}{}}
\citation{BelRoadMap_2009}
\citation{Quadrator_2008}
\citation{FIRM_2011}
\citation{rob_online_bs_icra_2014}
\citation{bsp_rss_2010a}
\citation{LQG_MP_2011}
\citation{Erez10ascalable}
\citation{van_den_Berg_2012}
\citation{Needle_2014}
\@writefile{toc}{\contentsline {subsubsection}{Belief space road maps}{30}{section*.18}}
\@writefile{brf}{\backcite{BelRoadMap_2009}{{30}{2.3.3}{section*.18}}}
\@writefile{brf}{\backcite{Quadrator_2008}{{30}{2.3.3}{section*.18}}}
\@writefile{brf}{\backcite{FIRM_2011}{{30}{2.3.3}{section*.18}}}
\@writefile{brf}{\backcite{rob_online_bs_icra_2014}{{30}{2.3.3}{section*.18}}}
\@writefile{toc}{\contentsline {subsubsection}{Optimal control}{30}{section*.19}}
\@writefile{brf}{\backcite{bsp_rss_2010a}{{30}{2.3.3}{section*.19}}}
\citation{non_gauss_bel_plan_2012}
\citation{seq_traj_replan_iros_2013}
\@writefile{brf}{\backcite{LQG_MP_2011}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{Erez10ascalable}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{van_den_Berg_2012}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{Needle_2014}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{non_gauss_bel_plan_2012}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{seq_traj_replan_iros_2013}{{31}{2.3.3}{section*.19}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: planning}{31}{section*.20}}
\citation{un_water_inspection_icra_2012}
\citation{u_aware_grasp_ICRA_2015}
\citation{Li_2015}
\citation{Littman95}
\citation{RL_book_sa}
\citation{Thrun_2005}
\citation{acting_uncer_1996}
\citation{qmdp_ijcnn_2014}
\citation{where_look_2012}
\citation{Hauser_2011}
\citation{pomdp_iros_tous_2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Heuristics}{32}{subsection.2.3.4}}
\newlabel{lit:heuristics}{{2.3.4}{32}{Heuristics}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Myopic \& Q-MDP}{32}{section*.21}}
\@writefile{brf}{\backcite{un_water_inspection_icra_2012}{{32}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{u_aware_grasp_ICRA_2015}{{32}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{Li_2015}{{32}{2.3.4}{section*.21}}}
\citation{CostalNavigation1999}
\citation{stachniss05robotics}
\citation{dense_entropy_icra_2014}
\@writefile{brf}{\backcite{Littman95}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{RL_book_sa}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{Thrun_2005}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{acting_uncer_1996}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{qmdp_ijcnn_2014}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{where_look_2012}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{Hauser_2011}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{pomdp_iros_tous_2015}{{33}{2.3.4}{section*.21}}}
\@writefile{toc}{\contentsline {subsubsection}{Information gain}{33}{section*.22}}
\@writefile{brf}{\backcite{CostalNavigation1999}{{33}{2.3.4}{section*.22}}}
\citation{Hsiao_RSS_10}
\citation{Efficient_touch_2012}
\citation{next_best_touch}
\@writefile{brf}{\backcite{stachniss05robotics}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{dense_entropy_icra_2014}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Hsiao_RSS_10}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Efficient_touch_2012}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{next_best_touch}{{34}{2.3.4}{section*.22}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: heuristic}{34}{section*.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Summary: literature}{35}{subsection.2.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }}{36}{figure.caption.24}}
\newlabel{fig:mind_summary}{{2.8}{36}{Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }{figure.caption.24}{}}
\citation{Billard08chapter}
\citation{Billard_schol_2013}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Approach}{37}{section.2.4}}
\newlabel{sec:approach}{{2.4}{37}{Approach}{section.2.4}{}}
\@writefile{brf}{\backcite{Billard08chapter}{{37}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{Billard_schol_2013}{{37}{2.4}{section.2.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }}{38}{figure.caption.25}}
\newlabel{fig:belief-pipeline}{{2.9}{38}{Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }{figure.caption.25}{}}
\citation{Biomechanics_2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \textbf  {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf  {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf  {Learn policy:} A generative policy, $\policy  (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }}{39}{figure.caption.26}}
\newlabel{fig:human_search}{{2.10}{39}{\textbf {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf {Learn policy:} A generative policy, $\policy (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }}{40}{figure.caption.27}}
\newlabel{fig:control_architecture}{{2.11}{40}{Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }{figure.caption.27}{}}
\@writefile{brf}{\backcite{Biomechanics_2009}{{40}{2.4}{figure.caption.26}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning to reason with uncertainty as humans}{41}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf  {Blindfolded search task} \textit  {Left:} Search task, a human demonstrator searching for the green wooden block on the table given that both his hearing and vision senses have been impeded. He starts (hand) at the white spot near position (1). The the red and blue trajectories are examples of possible searches. \textit  {Middle:} Inferred belief the human might have with respect to his position. If the human always starts at (1) and his belief is known, all following beliefs (2) can be inferred from Bayes rule. \textit  {Right:} WAM Robot 7 DOF reproduces the search strategies demonstrated by humans to find the object.\relax }}{42}{figure.caption.28}}
\newlabel{fig:searching}{{3.1}{42}{\textbf {Blindfolded search task} \textit {Left:} Search task, a human demonstrator searching for the green wooden block on the table given that both his hearing and vision senses have been impeded. He starts (hand) at the white spot near position (1). The the red and blue trajectories are examples of possible searches. \textit {Middle:} Inferred belief the human might have with respect to his position. If the human always starts at (1) and his belief is known, all following beliefs (2) can be inferred from Bayes rule. \textit {Right:} WAM Robot 7 DOF reproduces the search strategies demonstrated by humans to find the object.\relax }{figure.caption.28}{}}
\citation{Wang_2007}
\citation{what_det_our_nav_ability_2010}
\citation{spatial_updating_2008}
\citation{spatial_memory_how_ego_allo_combine_2006}
\citation{updating_egocentric_human_navigation_2000}
\citation{Pasqualotto2013175}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Background}{43}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Spatial navigation}{43}{subsection.3.1.1}}
\@writefile{brf}{\backcite{Wang_2007}{{43}{3.1.1}{subsection.3.1.1}}}
\@writefile{brf}{\backcite{what_det_our_nav_ability_2010}{{43}{3.1.1}{subsection.3.1.1}}}
\@writefile{brf}{\backcite{spatial_updating_2008}{{43}{3.1.1}{subsection.3.1.1}}}
\@writefile{brf}{\backcite{spatial_memory_how_ego_allo_combine_2006}{{43}{3.1.1}{subsection.3.1.1}}}
\@writefile{brf}{\backcite{updating_egocentric_human_navigation_2000}{{43}{3.1.1}{subsection.3.1.1}}}
\@writefile{brf}{\backcite{Pasqualotto2013175}{{43}{3.1.1}{subsection.3.1.1}}}
\citation{spatial_memory_how_ego_allo_combine_2006}
\citation{cogprints730}
\citation{human_stsm_2015}
\citation{Iachini2014}
\@writefile{brf}{\backcite{spatial_memory_how_ego_allo_combine_2006}{{44}{3.1.1}{subsection.3.1.1}}}
\@writefile{toc}{\contentsline {subsubsection}{Spatial cognition and memory}{44}{section*.29}}
\@writefile{brf}{\backcite{cogprints730}{{44}{3.1.1}{section*.29}}}
\@writefile{brf}{\backcite{human_stsm_2015}{{44}{3.1.1}{section*.29}}}
\@writefile{brf}{\backcite{Iachini2014}{{44}{3.1.1}{section*.29}}}
\citation{stankiewicz2006lost}
\citation{stankiewicz2006lost}
\citation{Bake_Saxe_Tene_2011}
\citation{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}
\citation{Bake_Tene_Saxe_2006}
\citation{Bake_Saxe_Tene_2011}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{45}{3.1.1}{section*.29}}}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{45}{3.1.1}{section*.29}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Human beliefs}{45}{subsection.3.1.2}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{45}{3.1.2}{subsection.3.1.2}}}
\@writefile{brf}{\backcite{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}{{45}{3.1.2}{subsection.3.1.2}}}
\citation{Kasper2001153}
\citation{Hamner_2006_5810}
\citation{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}
\citation{Nicolescu01learningand}
\citation{GeorgiosLidoris}
\@writefile{brf}{\backcite{Bake_Tene_Saxe_2006}{{46}{3.1.2}{subsection.3.1.2}}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{46}{3.1.2}{subsection.3.1.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Programming by demonstration \& uncertainty}{46}{subsection.3.1.3}}
\@writefile{brf}{\backcite{Kasper2001153}{{46}{3.1.3}{subsection.3.1.3}}}
\@writefile{brf}{\backcite{Hamner_2006_5810}{{46}{3.1.3}{subsection.3.1.3}}}
\@writefile{brf}{\backcite{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}{{46}{3.1.3}{subsection.3.1.3}}}
\@writefile{brf}{\backcite{Nicolescu01learningand}{{46}{3.1.3}{subsection.3.1.3}}}
\@writefile{brf}{\backcite{GeorgiosLidoris}{{47}{3.1.3}{subsection.3.1.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Experimental setup}{47}{subsection.3.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \textit  {Top left}: A participant is trying to locate the green wooden block on the table given that both vision and hearing senses have been inhibited, the location of his hand is being tracked by the OptiTrack\textsuperscript  {\textregistered } system. \textit  {Top right:} Initial distribution of the uncertainty or belief we assume the human has with respect to his position. \textit  {Bottom right:} Set of recorded searches, the trajectories are with respect to the hand. \textit  {Bottom right:} Trajectories starting from same area but have different search patterns, the red trajectories all navigate to the goal via the top right corner as opposed to the blue which go by the bottom left and right corner. Among these two groups there are trajectories which seem to minimize the distance taken to reach the goal as opposed to some which seek to stay close to the edge and corners.\relax }}{48}{figure.caption.30}}
\newlabel{fig:experiment}{{3.2}{48}{\textit {Top left}: A participant is trying to locate the green wooden block on the table given that both vision and hearing senses have been inhibited, the location of his hand is being tracked by the OptiTrack\textsuperscript {\textregistered } system. \textit {Top right:} Initial distribution of the uncertainty or belief we assume the human has with respect to his position. \textit {Bottom right:} Set of recorded searches, the trajectories are with respect to the hand. \textit {Bottom right:} Trajectories starting from same area but have different search patterns, the red trajectories all navigate to the goal via the top right corner as opposed to the blue which go by the bottom left and right corner. Among these two groups there are trajectories which seem to minimize the distance taken to reach the goal as opposed to some which seek to stay close to the edge and corners.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Formulation}{48}{subsection.3.1.5}}
\citation{Arul_Mask_Clap_2002}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {subsubsection}{Belief model}{49}{section*.31}}
\@writefile{brf}{\backcite{Arul_Mask_Clap_2002}{{49}{3.1.5}{equation.3.1.1}}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{49}{3.1.5}{equation.3.1.1}}}
\@writefile{toc}{\contentsline {subsubsection}{Sensing \& Motion model}{50}{section*.32}}
\newlabel{eq:sensingfunction}{{3.3}{50}{Sensing \& Motion model}{equation.3.1.3}{}}
\citation{DiffEntropyHuber2008}
\@writefile{toc}{\contentsline {subsubsection}{Uncertainty}{51}{section*.33}}
\newlabel{eq:gmm1}{{3.4}{51}{Uncertainty}{equation.3.1.4}{}}
\@writefile{brf}{\backcite{DiffEntropyHuber2008}{{51}{3.1.5}{figure.caption.34}}}
\citation{BillardCDS08}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Representation of the estimated density function. \textit  {Top Left and Right:} Initial starting point, all Gaussian functions are uniformly distributed with uniform priors. The red cluster always has the highest likelihood which is taken to be the believed location of the robot's/human's end-effector. \textit  {Bottom Left:} Contact with the table has been established, the robot location differers from his belief. \textit  {Bottom Right:} Contact has been made with a corner, the clusters reflect that the robot could be at any corner (note that weights are not depicted, only cluster assignment).\relax }}{52}{figure.caption.34}}
\newlabel{fig:clustering}{{3.3}{52}{Representation of the estimated density function. \textit {Top Left and Right:} Initial starting point, all Gaussian functions are uniformly distributed with uniform priors. The red cluster always has the highest likelihood which is taken to be the believed location of the robot's/human's end-effector. \textit {Bottom Left:} Contact with the table has been established, the robot location differers from his belief. \textit {Bottom Right:} Contact has been made with a corner, the clusters reflect that the robot could be at any corner (note that weights are not depicted, only cluster assignment).\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Methods}{52}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Model of human search}{52}{subsection.3.2.1}}
\@writefile{brf}{\backcite{BillardCDS08}{{52}{3.2.1}{subsection.3.2.1}}}
\citation{CostalNavigation1999}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \textit  {Left: } Resulting search GMM, a total of 67 Gaussian mixture components are present. We note the many overlapping Gaussians: this results from the level of uncertainty over the different choices taken. For example humans follow along the edge of the table in different directions and might leave the edge once they are confident with respect to their location. \textit  {Right:} Information Gain map of the table environment, dark regions indicate high information gain as oppose to lighter ones. Not surprisingly, the highest are the corners, followed by the edges.\relax }}{53}{figure.caption.35}}
\newlabel{fig:gmm}{{3.4}{53}{\textit {Left: } Resulting search GMM, a total of 67 Gaussian mixture components are present. We note the many overlapping Gaussians: this results from the level of uncertainty over the different choices taken. For example humans follow along the edge of the table in different directions and might leave the edge once they are confident with respect to their location. \textit {Right:} Information Gain map of the table environment, dark regions indicate high information gain as oppose to lighter ones. Not surprisingly, the highest are the corners, followed by the edges.\relax }{figure.caption.35}{}}
\newlabel{eq:gmm2}{{3.7}{53}{Model of human search}{equation.3.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Coastal Navigation}{53}{subsection.3.2.2}}
\@writefile{brf}{\backcite{CostalNavigation1999}{{53}{3.2.2}{subsection.3.2.2}}}
\newlabel{eq:objective_function}{{3.8}{54}{Coastal Navigation}{equation.3.2.8}{}}
\newlabel{eq:IG}{{3.9}{54}{Coastal Navigation}{equation.3.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Control}{54}{subsection.3.2.3}}
\newlabel{eq:conditional}{{3.10}{54}{Control}{equation.3.2.10}{}}
\newlabel{eq:GMR}{{3.11}{55}{Control}{equation.3.2.11}{}}
\newlabel{eq:weight}{{3.12}{55}{Control}{equation.3.2.12}{}}
\newlabel{eq:w_expectation}{{3.13}{55}{Control}{equation.3.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Overview of the decision loop. At the top a strategy is chosen given an initial belief $p(x_{0}|z_{0})$ of the location of the end-effector (initially through sampling the conditional). A speed is applied to the given direction based on the believed distance to the goal. This velocity is passed onwards to a low level impedance controller which sends out the required torques. The resulting sensation, encoded through the Multinomial distribution over the environment features, and actual displacement are sent back to update the belief.\relax }}{56}{figure.caption.36}}
\newlabel{fig:flow_chart}{{3.5}{56}{Overview of the decision loop. At the top a strategy is chosen given an initial belief $p(x_{0}|z_{0})$ of the location of the end-effector (initially through sampling the conditional). A speed is applied to the given direction based on the believed distance to the goal. This velocity is passed onwards to a low level impedance controller which sends out the required torques. The resulting sensation, encoded through the Multinomial distribution over the environment features, and actual displacement are sent back to update the belief.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Results and discussion}{57}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Search \& behaviour analysis}{57}{subsection.3.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Expected sensation. Plots of the expected sensation of the edge and corner feature for all trajectories. The axes are associated with the sensor measurements, 0 means that the corresponding feature is not sensed and 1 the feature is fully sensed. A point in the plots summaries a whole trajectory by the mean and variance of the probability of sensing a corner or edge. The radius of the circles are proportional to the variance. The doted blue rectangle represents the decision boundary for classifying a trajectory as being either risk-prone or ris-averse. A point which lies inside the rectangle is risk-prone. \textit  {Left:} Human trajectories demonstrate a wide variety of behaviours ranging from those remaining close to features to those preferring more risk. \textit  {Right:} Red points show Greedy and blue points the GMM model. \textit  {Bottom:} Green circles are associated with the Hybrid method whilst orange are those of the Coastal navigation method. The Hybrid method is a skewed version of the GMM which tends towards risky behaviour and exhibits the same kind of behaviour as the Coastal algorithm.\relax }}{58}{figure.caption.37}}
\newlabel{fig:expectedfeatures}{{3.6}{58}{Expected sensation. Plots of the expected sensation of the edge and corner feature for all trajectories. The axes are associated with the sensor measurements, 0 means that the corresponding feature is not sensed and 1 the feature is fully sensed. A point in the plots summaries a whole trajectory by the mean and variance of the probability of sensing a corner or edge. The radius of the circles are proportional to the variance. The doted blue rectangle represents the decision boundary for classifying a trajectory as being either risk-prone or ris-averse. A point which lies inside the rectangle is risk-prone. \textit {Left:} Human trajectories demonstrate a wide variety of behaviours ranging from those remaining close to features to those preferring more risk. \textit {Right:} Red points show Greedy and blue points the GMM model. \textit {Bottom:} Green circles are associated with the Hybrid method whilst orange are those of the Coastal navigation method. The Hybrid method is a skewed version of the GMM which tends towards risky behaviour and exhibits the same kind of behaviour as the Coastal algorithm.\relax }{figure.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Percentage of risk-prone trajectories based on two decision criteria, the feature (f) and the risk (r) (information gain) metrics discussed above.\relax }}{59}{table.caption.39}}
\newlabel{tab:percentage-risk-prone}{{3.1}{59}{Percentage of risk-prone trajectories based on two decision criteria, the feature (f) and the risk (r) (information gain) metrics discussed above.\relax }{table.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Risk of searches. Illustration of risk-prone and risk-averse searches in terms of a Risk factor (\textit  {left}) and expected sensation (\textit  {right}). \textit  {Left:} Each trajectory was reduced to a single scalar, which we call the Risk factor, quantizing the risk of a trajectory. The Risk factor is inversely proportional to the sum of the information gain of a particular trajectory. The colour paired dotes (risk averse) and squares (risk prone) represent trajectories which are plotted in Figure \ref  {fig:risk_examples}, to illustrate that these correspond to risk averse and prone searches. \textit  {Right:} Corresponding trajectories chosen in the Risk factor space but represented in the feature space. As expected, trajectories with a high risk map to regions of low expected feature. However the transition from the Risk space to feature space is non-linear and will result in a different risk-level classification than the feature metric previously discussed.\relax }}{59}{figure.caption.38}}
\newlabel{fig:riskexamples}{{3.7}{59}{Risk of searches. Illustration of risk-prone and risk-averse searches in terms of a Risk factor (\textit {left}) and expected sensation (\textit {right}). \textit {Left:} Each trajectory was reduced to a single scalar, which we call the Risk factor, quantizing the risk of a trajectory. The Risk factor is inversely proportional to the sum of the information gain of a particular trajectory. The colour paired dotes (risk averse) and squares (risk prone) represent trajectories which are plotted in Figure \ref {fig:risk_examples}, to illustrate that these correspond to risk averse and prone searches. \textit {Right:} Corresponding trajectories chosen in the Risk factor space but represented in the feature space. As expected, trajectories with a high risk map to regions of low expected feature. However the transition from the Risk space to feature space is non-linear and will result in a different risk-level classification than the feature metric previously discussed.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Risk prone \& averse searches (red \& green trajectories). \textit  {Top left:} Two human trajectories taken from data shown in Figure \ref  {fig:riskexamples}. \textit  {Top right:} Two Greedy trajectories. \textit  {Bottom left:} GMM trajectories, all starting from the same location, the colour coding is to illustrate the different policies which were encoded and emerge given the same initial conditions. \textit  {Bottom right:} Corresponding expected features of each trajectory, the colour coding matches the trajectories to the ``GMM risk types'' sub-figure. All the searches which were generated by the GMM in for this initialisation produced risk-averse searches (based on the feature metric discussed previous).\relax }}{60}{figure.caption.40}}
\newlabel{fig:risk_examples}{{3.8}{60}{Risk prone \& averse searches (red \& green trajectories). \textit {Top left:} Two human trajectories taken from data shown in Figure \ref {fig:riskexamples}. \textit {Top right:} Two Greedy trajectories. \textit {Bottom left:} GMM trajectories, all starting from the same location, the colour coding is to illustrate the different policies which were encoded and emerge given the same initial conditions. \textit {Bottom right:} Corresponding expected features of each trajectory, the colour coding matches the trajectories to the ``GMM risk types'' sub-figure. All the searches which were generated by the GMM in for this initialisation produced risk-averse searches (based on the feature metric discussed previous).\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}GMM \& Coastal Navigation policy analysis}{60}{subsection.3.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Illustration of three different types of modes present during the execution of the task where the robot is being controlled by the learned GMM model. The white ball represents the actual position of the robot's end-effector. The blue ball represents the believed position of the robot's end-effector and the robot is acting according to it. The blue ball arrows represent modes. Colours encode the mode's weights given by the priors $\pi _{k}$ after conditioning ( but not re-weighted as previously described). The spectrum ranges from red (high weight) to blue (low weight). \textit  {Top left:} Three modes are present, but two agree with each other. \textit  {Top right:} Three modes are again present indicating appropriate ways to reduce the uncertainty. \textit  {Lower left:} Two modes are in opposing directions. No flipping behaviour between modes occurs since preference is given to the modes pointing in the same direction as the robot's current trajectory. \textit  {Lower right:} GMM modes when conditioned on the state represented in the lower left figure. The two modes represent the possible directions (un-normalised).\relax }}{61}{figure.caption.41}}
\newlabel{fig:modes}{{3.9}{61}{Illustration of three different types of modes present during the execution of the task where the robot is being controlled by the learned GMM model. The white ball represents the actual position of the robot's end-effector. The blue ball represents the believed position of the robot's end-effector and the robot is acting according to it. The blue ball arrows represent modes. Colours encode the mode's weights given by the priors $\pi _{k}$ after conditioning ( but not re-weighted as previously described). The spectrum ranges from red (high weight) to blue (low weight). \textit {Top left:} Three modes are present, but two agree with each other. \textit {Top right:} Three modes are again present indicating appropriate ways to reduce the uncertainty. \textit {Lower left:} Two modes are in opposing directions. No flipping behaviour between modes occurs since preference is given to the modes pointing in the same direction as the robot's current trajectory. \textit {Lower right:} GMM modes when conditioned on the state represented in the lower left figure. The two modes represent the possible directions (un-normalised).\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Illustration of the vector field for the Coastal and GMM policy. \textit  {Top Left} Coastal policy, there is only one possible direction for every state at any time, the values of $\lambda _2$ in the cost function were set experimentally. \textit  {Others:} The GMM policy for three different levels of uncertainty. For each point multiple possible actions are possible which are reflected by the number of arrows (only the first three most likely actions). As the uncertainty decreases the policy becomes less multi-model, but still is around the edges and corners. Note that once being certain if one is close to the edge there is a possibility to go either straight to the goal or stay close to the edge and corners.\relax }}{62}{figure.caption.42}}
\newlabel{fig:vectorfield}{{3.10}{62}{Illustration of the vector field for the Coastal and GMM policy. \textit {Top Left} Coastal policy, there is only one possible direction for every state at any time, the values of $\lambda _2$ in the cost function were set experimentally. \textit {Others:} The GMM policy for three different levels of uncertainty. For each point multiple possible actions are possible which are reflected by the number of arrows (only the first three most likely actions). As the uncertainty decreases the policy becomes less multi-model, but still is around the edges and corners. Note that once being certain if one is close to the edge there is a possibility to go either straight to the goal or stay close to the edge and corners.\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Time efficiency \& Uncertainty}{62}{subsection.3.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Four search initializations, from \textit  {top left} to \textit  {bottom right} we refer to them as \#1-4. The circle with a black dot at its centre indicates the true starting point of the end-effector (eof), whilst the triangle with the black dot is the initial believed location of the eof. The initialisation in \#1 was chosen such that the true and believed eof location were at opposite sides of the table. This setting was selected to high light the draw back in methods which do not take into account uncertainty. The second initialisation, \#2, reflects the situation where once again their is a large distance between true and believed location of the eof. However, this time both are right on top of the table. The starting points in \#3, are a variant on \#1 but with the difference that the believed eof position is above the table whilst the true eof location is not. The last experiment,\#4, was a setup which would be favourable to algorithms which are inclined to be greedy, both true and believed eof locations are close to one another.\relax }}{63}{figure.caption.43}}
\newlabel{fig:four-initialisations}{{3.11}{63}{Four search initializations, from \textit {top left} to \textit {bottom right} we refer to them as \#1-4. The circle with a black dot at its centre indicates the true starting point of the end-effector (eof), whilst the triangle with the black dot is the initial believed location of the eof. The initialisation in \#1 was chosen such that the true and believed eof location were at opposite sides of the table. This setting was selected to high light the draw back in methods which do not take into account uncertainty. The second initialisation, \#2, reflects the situation where once again their is a large distance between true and believed location of the eof. However, this time both are right on top of the table. The starting points in \#3, are a variant on \#1 but with the difference that the believed eof position is above the table whilst the true eof location is not. The last experiment,\#4, was a setup which would be favourable to algorithms which are inclined to be greedy, both true and believed eof locations are close to one another.\relax }{figure.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Mean distance and variance taken to reach the goal for 3 methods in 5 experiments. The grey shaded entries correspond to the results of the search algorithm which obtained the fastest time to reach the goal in each type of experiment/search.\relax }}{64}{table.caption.44}}
\newlabel{tab:mean-var-distance}{{3.2}{64}{Mean distance and variance taken to reach the goal for 3 methods in 5 experiments. The grey shaded entries correspond to the results of the search algorithm which obtained the fastest time to reach the goal in each type of experiment/search.\relax }{table.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces ANOVA tests the null hypothesis that all search experiments produced the same type of search with respect to the distance taken to reach the goal. All the p-values are extremely small which indicate that the null hypothesis can safely be rejected.\relax }}{64}{table.caption.45}}
\newlabel{tab:anova-1}{{3.3}{64}{ANOVA tests the null hypothesis that all search experiments produced the same type of search with respect to the distance taken to reach the goal. All the p-values are extremely small which indicate that the null hypothesis can safely be rejected.\relax }{table.caption.45}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces ANOVA between paired search methods. The first column gives an indication of the probability that both the Greedy and GMM searches are statistically the same (the null hypothesis). This was rejected with a tolerance of below \%1. In the second column, Greedy vs Coastal searches \#1 and \#4 are statistically closer than the rest with a p-value threshold of 10\% required to be able to reject the null hypothesis. In the third column the uniform and \#3 are not statistically different and would require a higher threshold on the p-value to be so.\relax }}{65}{table.caption.46}}
\newlabel{fig:anova-2}{{3.4}{65}{ANOVA between paired search methods. The first column gives an indication of the probability that both the Greedy and GMM searches are statistically the same (the null hypothesis). This was rejected with a tolerance of below \%1. In the second column, Greedy vs Coastal searches \#1 and \#4 are statistically closer than the rest with a p-value threshold of 10\% required to be able to reject the null hypothesis. In the third column the uniform and \#3 are not statistically different and would require a higher threshold on the p-value to be so.\relax }{table.caption.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Conclusions}{65}{section.3.4}}
\citation{Bake_Saxe_Tene_2011}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Reduction of the uncertainty for the Uniform, \#1, \#2 and \#4 experiment, the expected value is reported \textit  {Top left}: Uniform initialisation, expected uncertainty for the Greedy (red), GMM (blue), Hybrid (green) \& Coastal (orange) search strategies. \textit  {Top right:} Experiment \#1. \textit  {Bottom left:} Experiment \#2. \textit  {Bottom right:} Experiment \#4.\relax }}{66}{figure.caption.47}}
\newlabel{fig:uncertainty}{{3.12}{66}{Reduction of the uncertainty for the Uniform, \#1, \#2 and \#4 experiment, the expected value is reported \textit {Top left}: Uniform initialisation, expected uncertainty for the Greedy (red), GMM (blue), Hybrid (green) \& Coastal (orange) search strategies. \textit {Top right:} Experiment \#1. \textit {Bottom left:} Experiment \#2. \textit {Bottom right:} Experiment \#4.\relax }{figure.caption.47}{}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{67}{3.4}{section.3.4}}}
\bibstyle{plainnat}
\bibdata{bib/search_human_blind.bib,bib/RL.bib,bib/pomdp.bib,bib/cpomdp.bib,bib/citations.bib,bib/DT.bib,bib/ToM.bib,bib/spatial_navigation.bib,bib/ProspectTheory.bib,imitation_learning.bib,bib/ch3-citations.bib}
\bibcite{FIRM_2011}{{1}{2011}{{a.~Agha-mohammadi et~al.}}{{a.~Agha-mohammadi, Chakravorty, and Amato}}}
\bibcite{rob_online_bs_icra_2014}{{2}{2014}{{a.~Agha-mohammadi et~al.}}{{a.~Agha-mohammadi, Agarwal, Mahadevan, Chakravorty, Tomkins, Denny, and Amato}}}
\bibcite{sis_pomdp_2002}{{3}{2002}{{Aberdeen and Baxter}}{{}}}
\bibcite{Arul_Mask_Clap_2002}{{4}{2002}{{Arulampalam et~al.}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\bibcite{Bake_Saxe_Tene_2011}{{5}{2011}{{Bake et~al.}}{{Bake, Tenenbaum, and Saxe}}}
\bibcite{Bake_Tene_Saxe_2006}{{6}{2006}{{Baker et~al.}}{{Baker, Tenenbaum, and Saxe}}}
\bibcite{gpomdp_2000}{{7}{2000}{{Baxter and Bartlett}}{{}}}
\bibcite{Bernoulli1954}{{8}{1954}{{Bernoulli}}{{}}}
\bibcite{Billard_schol_2013}{{9}{2013}{{Billard and Grollman}}{{}}}
\bibcite{Billard08chapter}{{10}{2008{a}}{{Billard et~al.}}{{Billard, Calinon, Dillmann, and Schaal}}}
\bibcite{BillardCDS08}{{11}{2008{b}}{{Billard et~al.}}{{Billard, Calinon, Dillmann, and Schaal}}}
\@writefile{toc}{\contentsline {chapter}{References}{69}{chapter*.48}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{solving_continous_pomdps_2013}{{12}{2013}{{Brechtel et~al.}}{{Brechtel, Gindele, and Dillmann}}}
\bibcite{mc_update_ppomdps}{{13}{2011}{{Brooks and Williams}}{{}}}
\bibcite{spatial_memory_how_ego_allo_combine_2006}{{14}{2006}{{Burgess}}{{}}}
\bibcite{ActingUncertainty_1996}{{15}{1996{a}}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{acting_uncer_1996}{{16}{1996{b}}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{u_aware_grasp_ICRA_2015}{{17}{2015}{{Chen and von Wichert}}{{}}}
\bibcite{Chambrier2014}{{18}{2014}{{de~Chambrier and Billard}}{{}}}
\bibcite{p_search_surv_2011}{{19}{2011}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{POMDP_approach_2010}{{20}{2010}{{Du et~al.}}{{Du, Hsu, Kurniawati, Lee, Ong, and Png}}}
\bibcite{Erez10ascalable}{{21}{2010}{{Erez and Smart}}{{}}}
\bibcite{Tree_batch_2005}{{22}{2005}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{ac_survey_2012}{{23}{2012}{{Grondman et~al.}}{{Grondman, Busoniu, Lopes, and Babuska}}}
\bibcite{Hamner_2006_5810}{{24}{2006}{{Hamner et~al.}}{{Hamner, Singh, and Scherer}}}
\bibcite{Hauser_2011}{{25}{2011}{{Hauser}}{{}}}
\bibcite{DRQ_AAAI_2015}{{26}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{Quadrator_2008}{{27}{2008}{{He et~al.}}{{He, Prentice, and Roy}}}
\bibcite{next_best_touch}{{28}{2013}{{Hebert et~al.}}{{Hebert, Howard, Hudson, Ma, and Burdick}}}
\bibcite{un_water_inspection_icra_2012}{{29}{2012}{{Hollinger et~al.}}{{Hollinger, Englot, Hover, Mitra, and Sukhatme}}}
\bibcite{Hsiao_RSS_10}{{30}{2010}{{Hsiao et~al.}}{{Hsiao, Kaelbling, and Lozano-Perez}}}
\bibcite{DiffEntropyHuber2008}{{31}{2008}{{Huber et~al.}}{{Huber, Bailey, Durrant-Whyte, and Hanebeck}}}
\bibcite{Iachini2014}{{32}{2014}{{Iachini et~al.}}{{Iachini, Ruggiero, and Ruotolo}}}
\bibcite{Efficient_touch_2012}{{33}{2012}{{Javdani et~al.}}{{Javdani, Klingensmith, Bagnell, Pollard, and Srinivasa}}}
\bibcite{Kaelbling_1998}{{34}{1998}{{Kaelbling et~al.}}{{Kaelbling, Littman, and Cassandra}}}
\bibcite{Kasper2001153}{{35}{2001}{{Kasper et~al.}}{{Kasper, Fricke, Steuernagel, and von Puttkamer}}}
\bibcite{heli_2004}{{36}{2004}{{Kim et~al.}}{{Kim, Jordan, Sastry, and Ng}}}
\bibcite{PoWER_2009}{{37}{2009}{{Kober and Peters}}{{}}}
\bibcite{RL_robots_surv_2013}{{38}{2013}{{Kober et~al.}}{{Kober, Bagnell, and Peters}}}
\bibcite{pancake_2010}{{39}{2010{a}}{{Kormushev et~al.}}{{Kormushev, Calinon, and Caldwell}}}
\bibcite{archery_2010}{{40}{2010{b}}{{Kormushev et~al.}}{{Kormushev, Calinon, Saegusa, and Metta}}}
\bibcite{SARSOP}{{41}{2008}{{Kurniawati et~al.}}{{Kurniawati, Hsu, and Lee}}}
\bibcite{human_stsm_2015}{{42}{}{{Lavenexa et~al.}}{{Lavenexa, Boujonb, Ndarugendamwob, and Lavenexa}}}
\bibcite{Li_2015}{{43}{2016}{{Li et~al.}}{{Li, Hang, Kragic, and Billard}}}
\bibcite{bs_compression_2010}{{44}{2010}{{Li et~al.}}{{Li, Cheung, and Liu}}}
\bibcite{GeorgiosLidoris}{{45}{2011}{{Lidoris}}{{}}}
\bibcite{qmdp_ijcnn_2014}{{46}{2014}{{Lin et~al.}}{{Lin, Lu, and Makedon}}}
\bibcite{Littman95}{{47}{1995}{{Littman et~al.}}{{Littman, Cassandra, and Kaelbling}}}
\bibcite{cogprints730}{{48}{1956}{{Miller}}{{}}}
\bibcite{mnih-dqn-2015}{{49}{2015}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis}}}
\bibcite{Pegasus_2000}{{50}{2000}{{Ng and Jordan}}{{}}}
\bibcite{Nicolescu01learningand}{{51}{2001}{{Nicolescu and Mataric}}{{}}}
\bibcite{RL_book_sa}{{52}{2012}{{Now\'{e} et~al.}}{{Now\'{e}, Vrancx, and De~Hauwere}}}
\bibcite{where_look_2012}{{53}{2012}{{Nunez-Varela et~al.}}{{Nunez-Varela, Ravindran, and Wyatt}}}
\bibcite{Pasqualotto2013175}{{54}{2013}{{Pasqualotto et~al.}}{{Pasqualotto, Spiller, Jansari, and Proulx}}}
\bibcite{NAC_2008}{{55}{2008}{{Peters and Schaal}}{{}}}
\bibcite{PBVI}{{56}{2003}{{Pineau et~al.}}{{Pineau, Gordon, and Thrun}}}
\bibcite{non_gauss_bel_plan_2012}{{57}{2012}{{Platt et~al.}}{{Platt, Kaelbling, Lozano-Perez, and Tedrake}}}
\bibcite{bsp_rss_2010a}{{58}{2010}{{Platt et~al.}}{{Platt, Tedrake, Kaelbling, and Lozano-P\'{e}rez}}}
\bibcite{PBVI_C_2006}{{59}{2006}{{Porta et~al.}}{{Porta, Vlassis, Spaan, and Poupart}}}
\bibcite{BelRoadMap_2009}{{60}{2009}{{Prentice and Roy}}{{}}}
\bibcite{decision_un_2013}{{61}{2013}{{Preuschoff et~al.}}{{Preuschoff, Mohr, and Hsu}}}
\bibcite{rai2013learning}{{62}{2013}{{Rai et~al.}}{{Rai, De~Chambrier, and Billard}}}
\bibcite{Sondik_1973}{{63}{1973}{{Richard D.~Smallwood}}{{}}}
\bibcite{Richardson1_Baker1_Tenenbaum1_Saxe1_2012}{{64}{2012}{{Richardson et~al.}}{{Richardson, Bake, Tenenbaum, and Saxe}}}
\bibcite{neura_fqi_2005}{{65}{2005}{{Riedmiller}}{{}}}
\bibcite{Ross08onlineplanning}{{66}{2008}{{Ross et~al.}}{{Ross, Pineau, Paquet, and Chaib-draa}}}
\bibcite{CostalNavigation1999}{{67}{1999}{{Roy et~al.}}{{Roy, Burgard, Fox, and Thrun}}}
\bibcite{belief_compression_2005}{{68}{2005}{{Roy}}{{}}}
\bibcite{EPCA_2003}{{69}{2003}{{Roy and Gordon}}{{}}}
\bibcite{Roy99coastalnavigation}{{70}{1999}{{Roy and Thrun}}{{}}}
\bibcite{LfD_Autonomous_Navigation_in_Complex_Unstructured_Terrain}{{71}{2010}{{Silver et~al.}}{{Silver, Bagnell, and Stentz}}}
\bibcite{HSV}{{72}{2004}{{Smith and Simmons}}{{}}}
\bibcite{HSVI2}{{73}{2012}{{Smith and Simmons}}{{}}}
\bibcite{Spaan05icra}{{74}{2005}{{Spaan and Vlassis}}{{}}}
\bibcite{stachniss05robotics}{{75}{2005}{{Stachniss et~al.}}{{Stachniss, Grisetti, and Burgard}}}
\bibcite{stankiewicz2006lost}{{76}{2006}{{Stankiewicz et~al.}}{{Stankiewicz, Legge, Mansfield, and Schlicht}}}
\bibcite{dmp_iros_2011}{{77}{2011}{{Stulp et~al.}}{{Stulp, Theodorou, Kalakrishnan, Pastor, Righetti, and Schaal}}}
\bibcite{dmp_seq_2012}{{78}{2012}{{Stulp et~al.}}{{Stulp, Theodorou, and Schaal}}}
\bibcite{Needle_2014}{{79}{2014}{{Sun and Alterovitz}}{{}}}
\bibcite{MC-POMDP}{{80}{2000}{{Thrun}}{{}}}
\bibcite{Thrun_2005}{{81}{2005}{{Thrun et~al.}}{{Thrun, Burgard, and Fox}}}
\bibcite{dense_entropy_icra_2014}{{82}{2014}{{Vallve and Andrade{-}Cetto}}{{}}}
\bibcite{LQG_MP_2011}{{83}{2011}{{Van Den~Berg et~al.}}{{Van Den~Berg, Abbeel, and Goldberg}}}
\bibcite{van_den_Berg_2012}{{84}{2012}{{van~den Berg et~al.}}{{van~den Berg, Patil, and Alterovitz}}}
\bibcite{FSVI}{{85}{2007}{{Veloso}}{{}}}
\bibcite{pomdp_iros_tous_2015}{{86}{2015}{{Vien and Toussaint}}{{}}}
\bibcite{eNAC_2003}{{87}{2003}{{Vijayakumar et~al.}}{{Vijayakumar, Shibata, and Schaal}}}
\bibcite{VonNeumann1944}{{88}{1990}{{Von~Neumann and Morgenstern}}{{}}}
\bibcite{Wang2016}{{89}{2016}{{Wang et~al.}}{{Wang, Uchibe, and Doya}}}
\bibcite{Wang_2007}{{90}{2007}{{Wang}}{{}}}
\bibcite{updating_egocentric_human_navigation_2000}{{91}{2000}{{Wang and Spelke}}{{}}}
\bibcite{reinforce_1992}{{92}{1992}{{Williams}}{{}}}
\bibcite{Biomechanics_2009}{{93}{2009}{{Winter}}{{}}}
\bibcite{what_det_our_nav_ability_2010}{{94}{2010}{{Wolbers and Hegarty}}{{}}}
\bibcite{spatial_updating_2008}{{95}{2008}{{Wolbers et~al.}}{{Wolbers, Hegarty, Buchel, and Loomis}}}
\bibcite{seq_traj_replan_iros_2013}{{96}{2013}{{Zito et~al.}}{{Zito, Kopicki, Stolkin, Borst, Schmidt, Roa, and Wyatt}}}

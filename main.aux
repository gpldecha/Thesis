\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Bernoulli1954}
\citation{VonNeumann1944}
\citation{ActingUncertainty_1996}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}}
\@writefile{brf}{\backcite{Bernoulli1954}{{1}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{VonNeumann1944}{{1}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{ActingUncertainty_1996}{{1}{1.1}{section.1.1}}}
\citation{Billard08chapter}
\citation{stankiewicz2006lost}
\citation{Thrun_Burgard_Fox_2005}
\@writefile{brf}{\backcite{Billard08chapter}{{2}{1.1}{section.1.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contribution}{2}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Learning to reason with uncertainty as humans}{2}{subsection.1.2.1}}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Non-parametric Bayesian state space filter}{3}{subsection.1.2.2}}
\citation{rai2013learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Reinforcement learning in belief space}{4}{subsection.1.2.3}}
\@writefile{brf}{\backcite{rai2013learning}{{4}{1.2.3}{subsection.1.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Thesis outline}{4}{section.1.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Decisions under Uncertainty}{5}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Decision theory}{6}{subsection.2.1.1}}
\citation{Bernoulli1954}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces ad\relax }}{7}{figure.caption.1}}
\@writefile{brf}{\backcite{Bernoulli1954}{{7}{2.1.1}{figure.caption.1}}}
\citation{VonNeumann1944}
\newlabel{eq:exp_utility}{{2.1.1}{8}{Decision theory}{figure.caption.1}{}}
\@writefile{brf}{\backcite{VonNeumann1944}{{8}{2.1.1}{figure.caption.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Beliefs \& desires}{8}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sequential decision process}{8}{section.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Definition of common variables used.\relax }}{9}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:notation}{{2.1}{9}{Definition of common variables used.\relax }{table.caption.2}{}}
\newlabel{eq:joint_state_actions_util}{{2.1}{10}{Sequential decision process}{equation.2.2.1}{}}
\newlabel{fig:mdp_off}{{2.2(a)}{11}{Subfigure 2 2.2(a)}{subfigure.2.2.1}{}}
\newlabel{sub@fig:mdp_off}{{(a)}{11}{Subfigure 2 2.2(a)\relax }{subfigure.2.2.1}{}}
\newlabel{fig:mdp_on}{{2.2(b)}{11}{Subfigure 2 2.2(b)}{subfigure.2.2.2}{}}
\newlabel{sub@fig:mdp_on}{{(b)}{11}{Subfigure 2 2.2(b)\relax }{subfigure.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf  {(a)} the decision nodes are not considered random variables whilst in \textbf  {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }}{11}{figure.caption.3}}
\newlabel{fig:mdp}{{2.2}{11}{Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf {(a)} the decision nodes are not considered random variables whilst in \textbf {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {off-policy}}}{11}{figure.caption.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {on-policy}}}{11}{figure.caption.3}}
\newlabel{eq:joint_state_actions}{{2.2}{11}{Sequential decision process}{equation.2.2.2}{}}
\newlabel{eq:temporal_expected_utility}{{2.4}{11}{Sequential decision process}{equation.2.2.4}{}}
\newlabel{eq:max_util}{{2.2}{11}{Sequential decision process}{equation.2.2.4}{}}
\newlabel{eq:expansion}{{2.5}{11}{Sequential decision process}{equation.2.2.5}{}}
\newlabel{eq:bellman}{{2.6}{12}{Sequential decision process}{equation.2.2.6}{}}
\newlabel{eq:on_policy_bellman}{{2.7}{12}{Sequential decision process}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}POMDP}{13}{subsection.2.2.1}}
\newlabel{eq:sensor}{{2.8}{13}{POMDP}{equation.2.2.8}{}}
\newlabel{eq:likelihood}{{2.9}{13}{POMDP}{equation.2.2.9}{}}
\newlabel{fig:motion_update}{{2.3(a)}{14}{Subfigure 2 2.3(a)}{subfigure.2.3.1}{}}
\newlabel{sub@fig:motion_update}{{(a)}{14}{Subfigure 2 2.3(a)\relax }{subfigure.2.3.1}{}}
\newlabel{fig:measurement}{{2.3(b)}{14}{Subfigure 2 2.3(b)}{subfigure.2.3.2}{}}
\newlabel{sub@fig:measurement}{{(b)}{14}{Subfigure 2 2.3(b)\relax }{subfigure.2.3.2}{}}
\newlabel{fig:likelihood}{{2.3(c)}{14}{Subfigure 2 2.3(c)}{subfigure.2.3.3}{}}
\newlabel{sub@fig:likelihood}{{(c)}{14}{Subfigure 2 2.3(c)\relax }{subfigure.2.3.3}{}}
\newlabel{fig:measurement_update}{{2.3(d)}{14}{Subfigure 2 2.3(d)}{subfigure.2.3.4}{}}
\newlabel{sub@fig:measurement_update}{{(d)}{14}{Subfigure 2 2.3(d)\relax }{subfigure.2.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \textbf  {(a)} An agent is located to the south west of a brick wall, it is equipped with a range sensor. The agent takes a forward action, but skids which results in a high increase of the uncertainty.\textbf  {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is off. \textbf  {(c)} The agent uses with his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf  {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }}{14}{figure.caption.4}}
\newlabel{fig:belief_update_example}{{2.3}{14}{\textbf {(a)} An agent is located to the south west of a brick wall, it is equipped with a range sensor. The agent takes a forward action, but skids which results in a high increase of the uncertainty.\textbf {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is off. \textbf {(c)} The agent uses with his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{14}{figure.caption.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{14}{figure.caption.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{14}{figure.caption.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{14}{figure.caption.4}}
\newlabel{eq:motion_update}{{2.10}{15}{POMDP}{equation.2.2.10}{}}
\newlabel{eq:measurement_update}{{2.11}{15}{POMDP}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Bayesian state space filter.\relax }}{15}{figure.caption.5}}
\newlabel{fig:sub_pomdp}{{2.5(a)}{16}{Subfigure 2 2.5(a)}{subfigure.2.5.1}{}}
\newlabel{sub@fig:sub_pomdp}{{(a)}{16}{Subfigure 2 2.5(a)\relax }{subfigure.2.5.1}{}}
\newlabel{fig:sub_bmdp}{{2.5(b)}{16}{Subfigure 2 2.5(b)}{subfigure.2.5.2}{}}
\newlabel{sub@fig:sub_bmdp}{{(b)}{16}{Subfigure 2 2.5(b)\relax }{subfigure.2.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces \textbf  {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf  {(b)} belief-MDP, the POMDP is cast into a belief Markov Decision Process. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }}{16}{figure.caption.6}}
\newlabel{fig:pomdp}{{2.5}{16}{\textbf {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf {(b)} belief-MDP, the POMDP is cast into a belief Markov Decision Process. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{16}{figure.caption.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{16}{figure.caption.6}}
\newlabel{eq:belief_bellman}{{2.15}{16}{POMDP}{equation.2.2.15}{}}
\citation{Sondik_1973}
\citation{Thrun_2005}
\citation{Kaelbling_1998}
\newlabel{eq:belief_state_transformation}{{2.16}{17}{POMDP}{equation.2.2.16}{}}
\newlabel{eq:max_component}{{2.17}{17}{POMDP}{equation.2.2.17}{}}
\newlabel{eq:final_belief_bellman}{{2.18}{17}{POMDP}{equation.2.2.18}{}}
\@writefile{brf}{\backcite{Sondik_1973}{{17}{2.2.1}{equation.2.2.18}}}
\@writefile{brf}{\backcite{Thrun_2005}{{17}{2.2.1}{equation.2.2.18}}}
\citation{Sol_POMDP_Policy_space_1998}
\citation{Quadrator_2008}
\citation{BelRoadMap_2009}
\citation{Erez10ascalable}
\citation{mc_update_ppomdps}
\citation{Platt-RSS-10}
\citation{Bayesian_explor_exploit_2009}
\citation{Spaan05icra}
\citation{Thrun_2005}
\citation{Rand_belief_space_replanning}
\citation{Ross08onlineplanning}
\citation{Macro_uncertainty_2011}
\@writefile{brf}{\backcite{Kaelbling_1998}{{18}{2.2.1}{equation.2.2.18}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}State of the art}{18}{section.2.3}}
\@writefile{brf}{\backcite{Sol_POMDP_Policy_space_1998}{{18}{2.3}{section.2.3}}}
\@writefile{toc}{\contentsline {subsubsection}{Tree search}{18}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{Planning}{18}{section*.8}}
\@writefile{brf}{\backcite{Quadrator_2008}{{18}{2.3}{section*.8}}}
\@writefile{brf}{\backcite{BelRoadMap_2009}{{18}{2.3}{section*.8}}}
\@writefile{toc}{\contentsline {subsubsection}{Optimal control}{18}{section*.9}}
\@writefile{brf}{\backcite{Platt-RSS-10}{{18}{2.3}{section*.9}}}
\@writefile{brf}{\backcite{Bayesian_explor_exploit_2009}{{18}{2.3}{section*.9}}}
\@writefile{brf}{\backcite{Thrun_2005}{{18}{2.3}{section*.9}}}
\@writefile{brf}{\backcite{Rand_belief_space_replanning}{{18}{2.3}{section*.9}}}
\@writefile{brf}{\backcite{Ross08onlineplanning}{{18}{2.3}{section*.9}}}
\@writefile{brf}{\backcite{Macro_uncertainty_2011}{{18}{2.3}{section*.9}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Summary}{18}{section.2.4}}
\bibstyle{plainnat}
\bibdata{bib/pomdp.bib,bib/ProspectTheory.bib,bib/DT.bib,bib/ToM.bib,bib/citations.bib,bib/cpomdp.bib}
\bibcite{Bernoulli1954}{{1}{1954}{{Bernoulli}}{{}}}
\bibcite{Billard08chapter}{{2}{2008}{{Billard et~al.}}{{Billard, Calinon, Dillmann, and Schaal}}}
\bibcite{ActingUncertainty_1996}{{3}{1996}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{Sol_POMDP_Policy_space_1998}{{4}{1998}{{Hansen}}{{}}}
\bibcite{Rand_belief_space_replanning}{{5}{2010}{{Hauser}}{{}}}
\bibcite{Quadrator_2008}{{6}{2008}{{He et~al.}}{{He, Prentice, and Roy}}}
\bibcite{Macro_uncertainty_2011}{{7}{2011}{{He et~al.}}{{He, Brunskill, and Roy}}}
\bibcite{Kaelbling_1998}{{8}{1998}{{Kaelbling et~al.}}{{Kaelbling, Littman, and Cassandra}}}
\bibcite{Bayesian_explor_exploit_2009}{{9}{2009}{{Martinez-Cantin et~al.}}{{Martinez-Cantin, de~Freitas, Brochu, Castellanos, and Doucet}}}
\bibcite{Platt-RSS-10}{{10}{2010}{{Platt et~al.}}{{Platt, Tedrake, Kaelbling, and Lozano-Perez}}}
\@writefile{toc}{\contentsline {chapter}{References}{19}{chapter*.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{BelRoadMap_2009}{{11}{2009}{{Prentice and Roy}}{{}}}
\bibcite{rai2013learning}{{12}{2013}{{Rai et~al.}}{{Rai, De~Chambrier, and Billard}}}
\bibcite{Sondik_1973}{{13}{1973}{{Richard D.~Smallwood}}{{}}}
\bibcite{Ross08onlineplanning}{{14}{2008}{{Ross et~al.}}{{Ross, Pineau, Paquet, and Chaib-draa}}}
\bibcite{Thrun_2005}{{15}{2005}{{Thrun et~al.}}{{Thrun, Burgard, and Fox}}}
\bibcite{VonNeumann1944}{{16}{1990}{{Von~Neumann and Morgenstern}}{{}}}

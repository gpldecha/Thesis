\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{decision_un_2013}
\citation{Bernoulli1954}
\citation{VonNeumann1944}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}}
\@writefile{brf}{\backcite{decision_un_2013}{{1}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{Bernoulli1954}{{1}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{VonNeumann1944}{{1}{1.1}{section.1.1}}}
\citation{ActingUncertainty_1996}
\citation{stankiewicz2006lost}
\citation{Billard08chapter}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of the decision making under uncertainty in both robotics and everyday life situations. Images taken from the public domain.\relax }}{2}{figure.caption.1}}
\@writefile{brf}{\backcite{ActingUncertainty_1996}{{2}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{stankiewicz2006lost}{{2}{1.1}{figure.caption.1}}}
\@writefile{brf}{\backcite{Billard08chapter}{{2}{1.1}{figure.caption.1}}}
\citation{Bake_Saxe_Tene_2011}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contribution}{3}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Learning to reason with uncertainty as humans}{3}{subsection.1.2.1}}
\@writefile{brf}{\backcite{Bake_Saxe_Tene_2011}{{3}{1.2.1}{subsection.1.2.1}}}
\citation{rai2013learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Non-parametric Bayesian state space filter}{4}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Reinforcement learning in belief space}{4}{subsection.1.2.3}}
\@writefile{brf}{\backcite{rai2013learning}{{4}{1.2.3}{subsection.1.2.3}}}
\citation{Chambrier2014}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Thesis outline}{5}{section.1.3}}
\@writefile{brf}{\backcite{Chambrier2014}{{5}{1.3}{section.1.3}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Chapter outline.\relax }}{7}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ch2_outline}{{2.1}{7}{Chapter outline.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Decisions under uncertainty}{8}{section.2.1}}
\newlabel{sec:deci_un}{{2.1}{8}{Decisions under uncertainty}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Relation between beliefs, desires and actions and are all considered to be rational.\relax }}{9}{figure.caption.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Decision theory}{9}{subsection.2.1.1}}
\citation{Bernoulli1954}
\citation{VonNeumann1944}
\@writefile{brf}{\backcite{Bernoulli1954}{{10}{2.1.1}{figure.caption.3}}}
\newlabel{eq:exp_utility}{{2.1.1}{10}{Decision theory}{figure.caption.3}{}}
\@writefile{brf}{\backcite{VonNeumann1944}{{10}{2.1.1}{figure.caption.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sequential decision making}{11}{section.2.2}}
\newlabel{sec:sqp}{{2.2}{11}{Sequential decision making}{section.2.2}{}}
\newlabel{eq:joint_state_actions_util}{{2.1}{11}{Sequential decision making}{equation.2.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Definition of common variables used.\relax }}{12}{table.caption.4}}
\newlabel{tab:notation}{{2.1}{12}{Definition of common variables used.\relax }{table.caption.4}{}}
\newlabel{fig:mdp_off}{{2.3(a)}{13}{Subfigure 2 2.3(a)}{subfigure.2.3.1}{}}
\newlabel{sub@fig:mdp_off}{{(a)}{13}{Subfigure 2 2.3(a)\relax }{subfigure.2.3.1}{}}
\newlabel{fig:mdp_on}{{2.3(b)}{13}{Subfigure 2 2.3(b)}{subfigure.2.3.2}{}}
\newlabel{sub@fig:mdp_on}{{(b)}{13}{Subfigure 2 2.3(b)\relax }{subfigure.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf  {(a)} the decision nodes are not considered random variables whilst in \textbf  {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }}{13}{figure.caption.5}}
\newlabel{fig:mdp}{{2.3}{13}{Dynamical Bayesian Network of a Markov Decision Process; it encodes the temporal relation between the random variables (circles), utilities (diamond) and decisions (squares). The arrows specify conditional distributions. In \textbf {(a)} the decision nodes are not considered random variables whilst in \textbf {(b)} they are. From these two DBN we can read off two conditional distributions, the state transition distribution (in red) and the action distribution (in purple). \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {off-policy}}}{13}{figure.caption.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {on-policy}}}{13}{figure.caption.5}}
\newlabel{eq:joint_state_actions}{{2.2}{14}{Sequential decision making}{equation.2.2.2}{}}
\newlabel{eq:temporal_expected_utility}{{2.4}{14}{Sequential decision making}{equation.2.2.4}{}}
\newlabel{eq:expansion}{{2.5}{14}{Sequential decision making}{equation.2.2.5}{}}
\newlabel{eq:bellman}{{2.6}{14}{Sequential decision making}{equation.2.2.6}{}}
\newlabel{eq:on_policy_bellman}{{2.7}{15}{Sequential decision making}{equation.2.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}POMDP}{15}{subsection.2.2.1}}
\newlabel{eq:sensor}{{2.8}{16}{POMDP}{equation.2.2.8}{}}
\newlabel{eq:likelihood}{{2.9}{16}{POMDP}{equation.2.2.9}{}}
\newlabel{fig:motion_update}{{2.4(a)}{17}{Subfigure 2 2.4(a)}{subfigure.2.4.1}{}}
\newlabel{sub@fig:motion_update}{{(a)}{17}{Subfigure 2 2.4(a)\relax }{subfigure.2.4.1}{}}
\newlabel{fig:measurement}{{2.4(b)}{17}{Subfigure 2 2.4(b)}{subfigure.2.4.2}{}}
\newlabel{sub@fig:measurement}{{(b)}{17}{Subfigure 2 2.4(b)\relax }{subfigure.2.4.2}{}}
\newlabel{fig:likelihood}{{2.4(c)}{17}{Subfigure 2 2.4(c)}{subfigure.2.4.3}{}}
\newlabel{sub@fig:likelihood}{{(c)}{17}{Subfigure 2 2.4(c)\relax }{subfigure.2.4.3}{}}
\newlabel{fig:measurement_update}{{2.4(d)}{17}{Subfigure 2 2.4(d)}{subfigure.2.4.4}{}}
\newlabel{sub@fig:measurement_update}{{(d)}{17}{Subfigure 2 2.4(d)\relax }{subfigure.2.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \textbf  {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf  {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf  {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf  {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }}{17}{figure.caption.6}}
\newlabel{fig:belief_update_example}{{2.4}{17}{\textbf {(a)} An agent is located to the south west of a brick wall. It is equipped with a range sensor. The agent takes a forward action but skids, which results in a high increase of the uncertainty.\textbf {(b)} The agent takes a measurement, $y_0$, of this distance to the wall; because his sensor is noisy his estimate is inaccurate. \textbf {(c)} The agent uses his measurement model to evaluate the plausibility of all locations in the world which would result in a similar measurement; illustrated by the likelihood function $p(y_0|x_0)$. \textbf {(d)} The likelihood is integrated into the probability density function; $p(x_0|y_0) \propto p(y_0|x)p(x_0)$.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{17}{figure.caption.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{17}{figure.caption.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{17}{figure.caption.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{17}{figure.caption.6}}
\newlabel{eq:motion_update}{{2.10}{18}{POMDP}{equation.2.2.10}{}}
\newlabel{eq:measurement_update}{{2.11}{18}{POMDP}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Bayesian state space filter.\relax }}{18}{figure.caption.7}}
\newlabel{fig:sub_pomdp}{{2.6(a)}{19}{Subfigure 2 2.6(a)}{subfigure.2.6.1}{}}
\newlabel{sub@fig:sub_pomdp}{{(a)}{19}{Subfigure 2 2.6(a)\relax }{subfigure.2.6.1}{}}
\newlabel{fig:sub_bmdp}{{2.6(b)}{19}{Subfigure 2 2.6(b)}{subfigure.2.6.2}{}}
\newlabel{sub@fig:sub_bmdp}{{(b)}{19}{Subfigure 2 2.6(b)\relax }{subfigure.2.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \textbf  {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf  {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }}{19}{figure.caption.8}}
\newlabel{fig:pomdp}{{2.6}{19}{\textbf {(a)} POMDP graphical model. The state space, $X$, is hidden, but is still partially observable through a measurement, $Y$. \textbf {(b)} The POMDP is cast into a belief Markov Decision Process, belief-MDP. The state space is a probability distribution, $b(x_t) = p(x_t)$, (known as a belief state) and is no longer considered a latent state. The original state transition function $p(x_{t+1}|x_t,a_t)$ is replaced by a belief state transition, $p(b_{t+1}|b_t,a_t)$. The reward is now a function of the belief.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{19}{figure.caption.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{19}{figure.caption.8}}
\newlabel{eq:belief_bellman}{{2.15}{19}{POMDP}{equation.2.2.15}{}}
\citation{Sondik_1973}
\citation{Thrun_2005}
\citation{Kaelbling_1998}
\newlabel{eq:belief_state_transformation}{{2.16}{20}{POMDP}{equation.2.2.16}{}}
\newlabel{eq:max_component}{{2.17}{20}{POMDP}{equation.2.2.17}{}}
\newlabel{eq:final_belief_bellman}{{2.18}{20}{POMDP}{equation.2.2.18}{}}
\@writefile{brf}{\backcite{Sondik_1973}{{20}{2.2.1}{equation.2.2.18}}}
\@writefile{brf}{\backcite{Thrun_2005}{{20}{2.2.1}{equation.2.2.18}}}
\@writefile{brf}{\backcite{Kaelbling_1998}{{20}{2.2.1}{equation.2.2.18}}}
\citation{POMDP_approach_2010}
\citation{Thrun_2005}
\citation{PBVI}
\citation{HSV}
\citation{HSVI2}
\citation{FSVI}
\citation{SARSOP}
\citation{POMDP_approach_2010}
\@writefile{brf}{\backcite{POMDP_approach_2010}{{21}{2.2.1}{equation.2.2.18}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Literature review}{21}{section.2.3}}
\newlabel{sec:lit_rev}{{2.3}{21}{Literature review}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Value Iteration}{21}{subsection.2.3.1}}
\newlabel{lit:VI}{{2.3.1}{21}{Value Iteration}{subsection.2.3.1}{}}
\@writefile{brf}{\backcite{Thrun_2005}{{21}{2.3.1}{subsection.2.3.1}}}
\@writefile{toc}{\contentsline {subsubsection}{Point-base Value Iteration}{21}{section*.10}}
\@writefile{brf}{\backcite{PBVI}{{21}{2.3.1}{section*.10}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Mind-map of AI and robotic methods for acting under uncertainty.\relax }}{22}{figure.caption.9}}
\newlabel{fig:mindmap}{{2.7}{22}{Mind-map of AI and robotic methods for acting under uncertainty.\relax }{figure.caption.9}{}}
\citation{Spaan05icra}
\citation{PBVI_C_2006}
\citation{solving_continous_pomdps_2013}
\@writefile{brf}{\backcite{HSV}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{HSVI2}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{FSVI}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{SARSOP}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{POMDP_approach_2010}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{Spaan05icra}{{23}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{PBVI_C_2006}{{23}{2.3.1}{section*.10}}}
\citation{Ross08onlineplanning}
\citation{MC-POMDP}
\citation{Tree_batch_2005}
\@writefile{brf}{\backcite{solving_continous_pomdps_2013}{{24}{2.3.1}{section*.10}}}
\@writefile{brf}{\backcite{Ross08onlineplanning}{{24}{2.3.1}{section*.10}}}
\@writefile{toc}{\contentsline {subsubsection}{Approximate Value Iteration}{24}{section*.11}}
\@writefile{brf}{\backcite{MC-POMDP}{{24}{2.3.1}{section*.11}}}
\citation{mc_update_ppomdps}
\citation{neura_fqi_2005}
\citation{DRQ_AAAI_2015}
\citation{mnih-dqn-2015}
\citation{Roy99coastalnavigation}
\@writefile{brf}{\backcite{Tree_batch_2005}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{mc_update_ppomdps}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{neura_fqi_2005}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{DRQ_AAAI_2015}{{25}{2.3.1}{section*.11}}}
\@writefile{brf}{\backcite{mnih-dqn-2015}{{25}{2.3.1}{section*.11}}}
\@writefile{toc}{\contentsline {subsubsection}{Latent Value Iteration}{25}{section*.12}}
\citation{belief_compression_2005}
\citation{EPCA_2003}
\citation{bs_compression_2010}
\@writefile{brf}{\backcite{Roy99coastalnavigation}{{26}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{belief_compression_2005}{{26}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{EPCA_2003}{{26}{2.3.1}{section*.12}}}
\@writefile{brf}{\backcite{bs_compression_2010}{{26}{2.3.1}{section*.12}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: Value Iteration}{26}{section*.13}}
\citation{gpomdp_2000}
\citation{reinforce_1992}
\citation{gpomdp_2000}
\citation{sis_pomdp_2002}
\citation{Pegasus_2000}
\citation{heli_2004}
\citation{dmp_iros_2011}
\citation{dmp_seq_2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Policy search}{27}{subsection.2.3.2}}
\newlabel{lit:policy_search}{{2.3.2}{27}{Policy search}{subsection.2.3.2}{}}
\@writefile{brf}{\backcite{gpomdp_2000}{{27}{2.3.2}{subsection.2.3.2}}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient: policy search}{27}{section*.14}}
\@writefile{brf}{\backcite{reinforce_1992}{{27}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{gpomdp_2000}{{27}{2.3.2}{section*.14}}}
\citation{PoWER_2009}
\citation{archery_2010}
\citation{pancake_2010}
\citation{Wang2016}
\citation{p_search_surv_2011}
\citation{RL_robots_surv_2013}
\citation{ac_survey_2012}
\citation{eNAC_2003}
\citation{NAC_2008}
\@writefile{brf}{\backcite{sis_pomdp_2002}{{28}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{Pegasus_2000}{{28}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{heli_2004}{{28}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{dmp_iros_2011}{{28}{2.3.2}{section*.14}}}
\@writefile{brf}{\backcite{dmp_seq_2012}{{28}{2.3.2}{section*.14}}}
\@writefile{toc}{\contentsline {subsubsection}{Expectation-Maximisation: policy search}{28}{section*.15}}
\@writefile{brf}{\backcite{PoWER_2009}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{archery_2010}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{pancake_2010}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{Wang2016}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{p_search_surv_2011}{{28}{2.3.2}{section*.15}}}
\@writefile{brf}{\backcite{RL_robots_surv_2013}{{28}{2.3.2}{section*.15}}}
\@writefile{toc}{\contentsline {subsubsection}{Actor-critic: policy search}{28}{section*.16}}
\@writefile{brf}{\backcite{ac_survey_2012}{{29}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{eNAC_2003}{{29}{2.3.2}{section*.16}}}
\@writefile{brf}{\backcite{NAC_2008}{{29}{2.3.2}{section*.16}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: policy search}{29}{section*.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Planning}{29}{subsection.2.3.3}}
\newlabel{lit:Planning}{{2.3.3}{29}{Planning}{subsection.2.3.3}{}}
\citation{BelRoadMap_2009}
\citation{Quadrator_2008}
\citation{FIRM_2011}
\citation{rob_online_bs_icra_2014}
\citation{bsp_rss_2010a}
\citation{LQG_MP_2011}
\citation{Erez10ascalable}
\citation{van_den_Berg_2012}
\citation{Needle_2014}
\@writefile{toc}{\contentsline {subsubsection}{Belief space road maps}{30}{section*.18}}
\@writefile{brf}{\backcite{BelRoadMap_2009}{{30}{2.3.3}{section*.18}}}
\@writefile{brf}{\backcite{Quadrator_2008}{{30}{2.3.3}{section*.18}}}
\@writefile{brf}{\backcite{FIRM_2011}{{30}{2.3.3}{section*.18}}}
\@writefile{brf}{\backcite{rob_online_bs_icra_2014}{{30}{2.3.3}{section*.18}}}
\@writefile{toc}{\contentsline {subsubsection}{Optimal control}{30}{section*.19}}
\@writefile{brf}{\backcite{bsp_rss_2010a}{{30}{2.3.3}{section*.19}}}
\citation{non_gauss_bel_plan_2012}
\citation{seq_traj_replan_iros_2013}
\@writefile{brf}{\backcite{LQG_MP_2011}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{Erez10ascalable}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{van_den_Berg_2012}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{Needle_2014}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{non_gauss_bel_plan_2012}{{31}{2.3.3}{section*.19}}}
\@writefile{brf}{\backcite{seq_traj_replan_iros_2013}{{31}{2.3.3}{section*.19}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: planning}{31}{section*.20}}
\citation{un_water_inspection_icra_2012}
\citation{u_aware_grasp_ICRA_2015}
\citation{Li_2015}
\citation{Littman95}
\citation{RL_book_sa}
\citation{Thrun_2005}
\citation{acting_uncer_1996}
\citation{qmdp_ijcnn_2014}
\citation{where_look_2012}
\citation{Hauser_2011}
\citation{pomdp_iros_tous_2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Heuristics}{32}{subsection.2.3.4}}
\newlabel{lit:heuristics}{{2.3.4}{32}{Heuristics}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Myopic \& Q-MDP}{32}{section*.21}}
\@writefile{brf}{\backcite{un_water_inspection_icra_2012}{{32}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{u_aware_grasp_ICRA_2015}{{32}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{Li_2015}{{32}{2.3.4}{section*.21}}}
\citation{CostalNavigation1999}
\citation{stachniss05robotics}
\citation{dense_entropy_icra_2014}
\@writefile{brf}{\backcite{Littman95}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{RL_book_sa}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{Thrun_2005}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{acting_uncer_1996}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{qmdp_ijcnn_2014}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{where_look_2012}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{Hauser_2011}{{33}{2.3.4}{section*.21}}}
\@writefile{brf}{\backcite{pomdp_iros_tous_2015}{{33}{2.3.4}{section*.21}}}
\@writefile{toc}{\contentsline {subsubsection}{Information gain}{33}{section*.22}}
\@writefile{brf}{\backcite{CostalNavigation1999}{{33}{2.3.4}{section*.22}}}
\citation{Hsiao_RSS_10}
\citation{Efficient_touch_2012}
\citation{next_best_touch}
\@writefile{brf}{\backcite{stachniss05robotics}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{dense_entropy_icra_2014}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Hsiao_RSS_10}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{Efficient_touch_2012}{{34}{2.3.4}{section*.22}}}
\@writefile{brf}{\backcite{next_best_touch}{{34}{2.3.4}{section*.22}}}
\@writefile{toc}{\contentsline {subsubsection}{Summary: heuristic}{34}{section*.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Summary: literature}{35}{subsection.2.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }}{36}{figure.caption.24}}
\newlabel{fig:mind_summary}{{2.8}{36}{Summary of the aspects of the reviewed methods. Local refers to the optimality of the solution, on/off-line refers to if the solution is computed on the stop (on-line) or many simulations are required to obtain the solution (off-line).\relax }{figure.caption.24}{}}
\citation{Billard08chapter}
\citation{Billard_schol_2013}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Approach}{37}{section.2.4}}
\newlabel{sec:approach}{{2.4}{37}{Approach}{section.2.4}{}}
\@writefile{brf}{\backcite{Billard08chapter}{{37}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{Billard_schol_2013}{{37}{2.4}{section.2.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }}{38}{figure.caption.25}}
\newlabel{fig:belief-pipeline}{{2.9}{38}{Three steps in learning a POMDP policy from human demonstrations: First gather the belief-action dataset, second compress the beliefs and third learn a generative policy.\relax }{figure.caption.25}{}}
\citation{Biomechanics_2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \textbf  {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf  {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf  {Learn policy:} A generative policy, $\policy  (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }}{39}{figure.caption.26}}
\newlabel{fig:human_search}{{2.10}{39}{\textbf {Demonstrations:} An apprentice is looking at a human teacher who is searching for the alarm clock's button and his pair of socks. The apprentice assumes the structure of the original beliefs the human teacher has with respect to his position and that of the alarm clock and socks, these are represented by the red, yellow and blue density functions. \textbf {Compression:} Given the data set of beliefs and actions obtained from the demonstrations, the beliefs is compressed to a fixed parametrisation. \textbf {Learn policy:} A generative policy, $\policy (g(b),a)$ is learned from the actions and compressed beliefs and can be executed according the schematic on the right. SE represents any Bayesian state space estimator, which takes as input, the current observation, belief and action and outputs the next belief state.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }}{40}{figure.caption.27}}
\newlabel{fig:control_architecture}{{2.11}{40}{Control architecture of the apprentice robot. The control loop should run between 10-100Hz. Given an applied action, the world returns an observation which is integrated by the State Estimator (SE) to give the current belief. The belief is the compressed and given as input to the policy.\relax }{figure.caption.27}{}}
\@writefile{brf}{\backcite{Biomechanics_2009}{{40}{2.4}{figure.caption.26}}}
\bibstyle{plainnat}
\bibdata{bib/RL.bib,bib/pomdp.bib,bib/cpomdp.bib,bib/citations.bib,bib/DT.bib,bib/ToM.bib,bib/spatial_navigation.bib,bib/ProspectTheory.bib,imitation_learning.bib}
\bibcite{FIRM_2011}{{1}{2011}{{a.~Agha-mohammadi et~al.}}{{a.~Agha-mohammadi, Chakravorty, and Amato}}}
\bibcite{rob_online_bs_icra_2014}{{2}{2014}{{a.~Agha-mohammadi et~al.}}{{a.~Agha-mohammadi, Agarwal, Mahadevan, Chakravorty, Tomkins, Denny, and Amato}}}
\bibcite{sis_pomdp_2002}{{3}{2002}{{Aberdeen and Baxter}}{{}}}
\bibcite{Bake_Saxe_Tene_2011}{{4}{2011}{{Bake et~al.}}{{Bake, Tenenbaum, and Saxe}}}
\bibcite{gpomdp_2000}{{5}{2000}{{Baxter and Bartlett}}{{}}}
\bibcite{Bernoulli1954}{{6}{1954}{{Bernoulli}}{{}}}
\bibcite{Billard_schol_2013}{{7}{2013}{{Billard and Grollman}}{{}}}
\bibcite{Billard08chapter}{{8}{2008}{{Billard et~al.}}{{Billard, Calinon, Dillmann, and Schaal}}}
\bibcite{solving_continous_pomdps_2013}{{9}{2013}{{Brechtel et~al.}}{{Brechtel, Gindele, and Dillmann}}}
\bibcite{mc_update_ppomdps}{{10}{2011}{{Brooks and Williams}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{41}{chapter*.28}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{ActingUncertainty_1996}{{11}{1996{a}}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{acting_uncer_1996}{{12}{1996{b}}{{Cassandra et~al.}}{{Cassandra, Kaelbling, and Kurien}}}
\bibcite{u_aware_grasp_ICRA_2015}{{13}{2015}{{Chen and von Wichert}}{{}}}
\bibcite{Chambrier2014}{{14}{2014}{{de~Chambrier and Billard}}{{}}}
\bibcite{p_search_surv_2011}{{15}{2011}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{POMDP_approach_2010}{{16}{2010}{{Du et~al.}}{{Du, Hsu, Kurniawati, Lee, Ong, and Png}}}
\bibcite{Erez10ascalable}{{17}{2010}{{Erez and Smart}}{{}}}
\bibcite{Tree_batch_2005}{{18}{2005}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{ac_survey_2012}{{19}{2012}{{Grondman et~al.}}{{Grondman, Busoniu, Lopes, and Babuska}}}
\bibcite{Hauser_2011}{{20}{2011}{{Hauser}}{{}}}
\bibcite{DRQ_AAAI_2015}{{21}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{Quadrator_2008}{{22}{2008}{{He et~al.}}{{He, Prentice, and Roy}}}
\bibcite{next_best_touch}{{23}{2013}{{Hebert et~al.}}{{Hebert, Howard, Hudson, Ma, and Burdick}}}
\bibcite{un_water_inspection_icra_2012}{{24}{2012}{{Hollinger et~al.}}{{Hollinger, Englot, Hover, Mitra, and Sukhatme}}}
\bibcite{Hsiao_RSS_10}{{25}{2010}{{Hsiao et~al.}}{{Hsiao, Kaelbling, and Lozano-Perez}}}
\bibcite{Efficient_touch_2012}{{26}{2012}{{Javdani et~al.}}{{Javdani, Klingensmith, Bagnell, Pollard, and Srinivasa}}}
\bibcite{Kaelbling_1998}{{27}{1998}{{Kaelbling et~al.}}{{Kaelbling, Littman, and Cassandra}}}
\bibcite{heli_2004}{{28}{2004}{{Kim et~al.}}{{Kim, Jordan, Sastry, and Ng}}}
\bibcite{PoWER_2009}{{29}{2009}{{Kober and Peters}}{{}}}
\bibcite{RL_robots_surv_2013}{{30}{2013}{{Kober et~al.}}{{Kober, Bagnell, and Peters}}}
\bibcite{pancake_2010}{{31}{2010{a}}{{Kormushev et~al.}}{{Kormushev, Calinon, and Caldwell}}}
\bibcite{archery_2010}{{32}{2010{b}}{{Kormushev et~al.}}{{Kormushev, Calinon, Saegusa, and Metta}}}
\bibcite{SARSOP}{{33}{2008}{{Kurniawati et~al.}}{{Kurniawati, Hsu, and Lee}}}
\bibcite{Li_2015}{{34}{2016}{{Li et~al.}}{{Li, Hang, Kragic, and Billard}}}
\bibcite{bs_compression_2010}{{35}{2010}{{Li et~al.}}{{Li, Cheung, and Liu}}}
\bibcite{qmdp_ijcnn_2014}{{36}{2014}{{Lin et~al.}}{{Lin, Lu, and Makedon}}}
\bibcite{Littman95}{{37}{1995}{{Littman et~al.}}{{Littman, Cassandra, and Kaelbling}}}
\bibcite{mnih-dqn-2015}{{38}{2015}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis}}}
\bibcite{Pegasus_2000}{{39}{2000}{{Ng and Jordan}}{{}}}
\bibcite{RL_book_sa}{{40}{2012}{{Now\'{e} et~al.}}{{Now\'{e}, Vrancx, and De~Hauwere}}}
\bibcite{where_look_2012}{{41}{2012}{{Nunez-Varela et~al.}}{{Nunez-Varela, Ravindran, and Wyatt}}}
\bibcite{NAC_2008}{{42}{2008}{{Peters and Schaal}}{{}}}
\bibcite{PBVI}{{43}{2003}{{Pineau et~al.}}{{Pineau, Gordon, and Thrun}}}
\bibcite{non_gauss_bel_plan_2012}{{44}{2012}{{Platt et~al.}}{{Platt, Kaelbling, Lozano-Perez, and Tedrake}}}
\bibcite{bsp_rss_2010a}{{45}{2010}{{Platt et~al.}}{{Platt, Tedrake, Kaelbling, and Lozano-P\'{e}rez}}}
\bibcite{PBVI_C_2006}{{46}{2006}{{Porta et~al.}}{{Porta, Vlassis, Spaan, and Poupart}}}
\bibcite{BelRoadMap_2009}{{47}{2009}{{Prentice and Roy}}{{}}}
\bibcite{decision_un_2013}{{48}{2013}{{Preuschoff et~al.}}{{Preuschoff, Mohr, and Hsu}}}
\bibcite{rai2013learning}{{49}{2013}{{Rai et~al.}}{{Rai, De~Chambrier, and Billard}}}
\bibcite{Sondik_1973}{{50}{1973}{{Richard D.~Smallwood}}{{}}}
\bibcite{neura_fqi_2005}{{51}{2005}{{Riedmiller}}{{}}}
\bibcite{Ross08onlineplanning}{{52}{2008}{{Ross et~al.}}{{Ross, Pineau, Paquet, and Chaib-draa}}}
\bibcite{CostalNavigation1999}{{53}{1999}{{Roy et~al.}}{{Roy, Burgard, Fox, and Thrun}}}
\bibcite{belief_compression_2005}{{54}{2005}{{Roy}}{{}}}
\bibcite{EPCA_2003}{{55}{2003}{{Roy and Gordon}}{{}}}
\bibcite{Roy99coastalnavigation}{{56}{1999}{{Roy and Thrun}}{{}}}
\bibcite{HSV}{{57}{2004}{{Smith and Simmons}}{{}}}
\bibcite{HSVI2}{{58}{2012}{{Smith and Simmons}}{{}}}
\bibcite{Spaan05icra}{{59}{2005}{{Spaan and Vlassis}}{{}}}
\bibcite{stachniss05robotics}{{60}{2005}{{Stachniss et~al.}}{{Stachniss, Grisetti, and Burgard}}}
\bibcite{stankiewicz2006lost}{{61}{2006}{{Stankiewicz et~al.}}{{Stankiewicz, Legge, Mansfield, and Schlicht}}}
\bibcite{dmp_iros_2011}{{62}{2011}{{Stulp et~al.}}{{Stulp, Theodorou, Kalakrishnan, Pastor, Righetti, and Schaal}}}
\bibcite{dmp_seq_2012}{{63}{2012}{{Stulp et~al.}}{{Stulp, Theodorou, and Schaal}}}
\bibcite{Needle_2014}{{64}{2014}{{Sun and Alterovitz}}{{}}}
\bibcite{MC-POMDP}{{65}{2000}{{Thrun}}{{}}}
\bibcite{Thrun_2005}{{66}{2005}{{Thrun et~al.}}{{Thrun, Burgard, and Fox}}}
\bibcite{dense_entropy_icra_2014}{{67}{2014}{{Vallve and Andrade{-}Cetto}}{{}}}
\bibcite{LQG_MP_2011}{{68}{2011}{{Van Den~Berg et~al.}}{{Van Den~Berg, Abbeel, and Goldberg}}}
\bibcite{van_den_Berg_2012}{{69}{2012}{{van~den Berg et~al.}}{{van~den Berg, Patil, and Alterovitz}}}
\bibcite{FSVI}{{70}{2007}{{Veloso}}{{}}}
\bibcite{pomdp_iros_tous_2015}{{71}{2015}{{Vien and Toussaint}}{{}}}
\bibcite{eNAC_2003}{{72}{2003}{{Vijayakumar et~al.}}{{Vijayakumar, Shibata, and Schaal}}}
\bibcite{VonNeumann1944}{{73}{1990}{{Von~Neumann and Morgenstern}}{{}}}
\bibcite{Wang2016}{{74}{2016}{{Wang et~al.}}{{Wang, Uchibe, and Doya}}}
\bibcite{reinforce_1992}{{75}{1992}{{Williams}}{{}}}
\bibcite{Biomechanics_2009}{{76}{2009}{{Winter}}{{}}}
\bibcite{seq_traj_replan_iros_2013}{{77}{2013}{{Zito et~al.}}{{Zito, Kopicki, Stolkin, Borst, Schmidt, Roa, and Wyatt}}}

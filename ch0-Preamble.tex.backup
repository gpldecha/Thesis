%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter 0: Preamble
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{figures/ch0/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
\lettrine[lines=2]{D}{ecision} making and planning when state information is partially
available is a problem faced by all forms of intelligent entities being either virtual,
synthetic or biological. The formulation of decision problem under partial state information leads to an exorbitant 
set of choices with associated probabilistic outcomes making the resolution of the problem 
to a concise set of decisions difficult when using traditional planning methods. Human beings have the abilities 
have handling such state uncertainty which we acquired through teaching one another and self learning.  

A large body of scientific work in PbD has focused on learning how to imitate human behaviour. Tasks such as \textit{pick and place}, hitting motions, and
bipedal locomotion have been encoded through either symbolic, statistical or dynamical system rep-
resentations. In contrast there has been less focus on transferring higher cognitive behaviour such as
problem solving skills and search strategies from humans to robots.

Our aim is to model how humans reason with respect to their beliefs and the role uncertainty plays during spatial navigation search tasks. 
We consider for instance tasks such as localising  an object in a room or connection a plug to a power socket in the dark, 
or any such situation with total suppression of visual information, and transfer this reasoning mechanisms to a robot apprentice. 
There are many robotic application domains in which uncertainty resulting from a lack of visual perception is common, such as in underwater 
maintenance, planetary exploration and occluded manipulation tasks. Learning human search models and transferring them to robots is useful
in such domains and learning a search strategy from scratch would prove intractable.

A difficulty in learning humans reasoning mechanisms, in the search scenarios we consider, is that
the humans beliefs and sensations (haptic and tactile) are unobservable and they vary within and
across subjects. We infer the human sensations from either the kinematic relationship between them
and a known geometric description of the environment or the human subjects use a tool equipped with
a sensor (force-torque sensor) whose measurements are used to infer the human sensations. The actual
sensations, which are a function of either the sensor tool or kinematic-environment measurements, are
transformed to a binary feature vector which encodes whether contact are present between features
such as surfaces, edges and corners of the environment.

We model the human's beliefs by a probability density function which we update through recursive Bayesian 
state space estimation using motion estimates, acquired through a tracking system (the human subjects wore markers), 
and the sensation estimates were obtained as described above. We make the assumption that the probability 
density function, representing the human's belief, is updated by a Bayesian recursion and that this process is similar to the way in 
which humans integrate information.

To model the reasoning processes of human subjects performing the search tasks we learn a generative joint distribution over beliefs
and actions (end-effector velocities) which were recorded during the executions of the task. 
The high dimensionality of the belief and its varying complexity  during the searches required that we compress the 
belief to its most likely state and entropy. 

We evaluate this methodology of learning search strategies in a task consisting of finding an object on a table. 
We demonstrate that multiple search strategies are encoded in the joint belief-action distribution and 
we compare this approach with greedy myopic and coastal navigation search algorithms. The results show that the human learned 
search model is the fastest of all methods.

We consider in a second setting a task in which human subjects have to demonstrate how 
to search for and connect a plug to a power socket to a robot apprentice deprived of visual information. 
We take the same approach but incorporate the learning of the policy into a reinforcement learning framework 
and demonstrate that by defining a simple cost function the quality of the final learned policy can be significantly
improved without the need of performing exploratory rollouts which are costly and typically necessary in RL.

Both search tasks above can be considered as active localisation in the sense that uncertainty
originates from the position of the human or robot in the world. We now consider search setting
in which both the position of the robot and and aspects of environment are uncertain. Given the
unstructured nature of the belief a histogram parametrisation of the joint distribution over the
robot and the environment is necessary. However, naively doing so becomes quickly intractable as
the computational cost is exponential in terms of the parametrisation. We demonstrate that by
only parametrising the marginals and by memorising the parameters of the measurement likelihood
functions we can recover the exact same solution as the naive parametrisations at a cost which is
linear in space and time complexity as oppose to exponential.

\mylineskip

\noindent\textsc{\textbf{Keywords:}} Programming by Demonstration, POMDP, Reinforcement Learning, State Space Estimation (SSE)
\end{abstract}

\begin{resume}

Resume here

\mylineskip

\noindent\textsc{\textbf{Mots Clé:}} Keywords
\end{resume}
%


\begin{dedication}
\emph{Dedication\\ here}
\end{dedication}
\forcenewpage
\chapter*{Acknowledgments}
Acknowledgements here